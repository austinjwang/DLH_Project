{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ0sNuMePBXx"
   },
   "source": [
    "# Github Repo with this notebook and related files\n",
    "https://github.com/austinjwang/DLH_Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ0sNuMePBXx"
   },
   "source": [
    "# Background [[1](#References)]\n",
    "Modern machine learning algorithms can extract relevant features from medical data and make predictions for previously unseen patients. Current deep learning architectures used for risk prediction, based on Electronic Medical Records (EMR) data generally employ attention layers on top of recurrent layers. The models allow for state-of-the-art predictions and have enhanced interpretability by virtue of using Attention models. However, they fall short on processing time-series data (diagnosis codes and procedure codes in EMR) that was sampled at irregular time intervals. Predictive accuracy was comparable across neural network architectures. Groups of patients suffering from infectious complications, with chronic or progressive conditions, and for whom standard medical care was not suitable. Attention-based networks may be preferable to recurrent networks if an interpretable model is required, at only marginal cost in predictive accuracy.\n",
    "\n",
    "# Paper explanation\n",
    "The paper our project was based on proposed evaluating different deep learning architectures for predicting ICU readmission and describing patients at risk. The innovation of the method lies in its comparison of various deep learning architectures. This approach goes beyond traditional machine learning methods and explores the potential of state-of-the-art deep learning techniques in healthcare analytics. Model performance was gauged using metrics such as accuracy, precision, recall, and AUC. The effectiveness of the method would depend on how well it performed compared to the baseline models or existing clinical scoring systems in predicting ICU readmissions and providing interpretable insights into patient risk factors. The contribution of the research lies in its exploration of advanced deep learning techniques in a critical healthcare domain. By benchmarking and comparing different architectures, the paper provides valuable insights into the potential of deep learning for improving predictive accuracy and interpretability in ICU readmission prediction, thereby contributing to the ongoing research in healthcare analytics and patient care optimization.  The paper also concluded that several different models were viable with no one approach being by far the best, and choosing a model could depend on factors such as the need for interpretability of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uygL9tTPSVHB"
   },
   "source": [
    "# Scope of Reproducibility:\n",
    "\n",
    "The hypothesis of the paper is below:\n",
    "*   The utilization of various deep learning architectures, including LSTM, CNN, and Transformer models, will lead to improved accuracy in predicting readmission to the Intensive Care Unit (ICU) and provide valuable insights into identifying patients-at-risk compared to traditional machine learning methods or clinical scoring systems.\n",
    "\n",
    "The goal of the project is to explore different approaches to time embeddings (MCE with time aware attention, embedding layers with concatenated elapsed times and embedding layers + neural ODE’s), across different deep learning architecture combinations. Our project is based on an existing study that concluded Neural ODEs applied to code embeddings did generally resulted in improved performance, suggesting that they may constitute a building block of interest for neural networks processing not only continuous time series, but also timestamped codes. \n",
    "\n",
    "We would assume that the conclusion in the study holds true and through our implementation of the models attempt to prove that out.  From a computational perspective, the size of the MIMIC III tables involved and runtime of the scripts lead us to believe that these results can be reproduced using personal computers without the need for external memory or computational power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzVUQS0CHry0"
   },
   "source": [
    "# Environment\n",
    "\n",
    "* Python Version: Python 3.11+\n",
    "* Required Packages (no specific version requirements):\n",
    "  * pandas\n",
    "  * numpy\n",
    "  * tqdm\n",
    "  * torch\n",
    "  * torchdiffeq\n",
    "  * scipy\n",
    "  * sklearn\n",
    "  * tabulate (displaying results at the end of this notebook only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWAHJ_1CdtaA"
   },
   "source": [
    "# Methodology\n",
    "\n",
    "The methodology section of this notebook contains subsections to describe the appraoch for data, models, training, and testing/evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "yu61Jp1xrnKk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchdiffeq import odeint, odeint_adjoint\n",
    "import os\n",
    "from time import time\n",
    "import scipy.stats as st\n",
    "from sklearn.metrics import *\n",
    "import torch.utils.data as utils\n",
    "import pickle\n",
    "from tabulate import tabulate\n",
    "\n",
    "from torch.utils.data.dataset import (\n",
    "    ChainDataset,\n",
    "    ConcatDataset,\n",
    "    Dataset,\n",
    "    IterableDataset,\n",
    "    StackDataset,\n",
    "    Subset,\n",
    "    TensorDataset,\n",
    "    random_split,\n",
    ")\n",
    "\n",
    "from torch.utils.data.dataloader import (\n",
    "    DataLoader,\n",
    "    _DatasetKind,\n",
    "    get_worker_info,\n",
    "    default_collate,\n",
    "    default_convert,\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NbPHUTMbkD3"
   },
   "source": [
    "## Data\n",
    "The data for this project comes from the MIMIC III Clinical Database. Specifically, we use the following tables which can be downloaded directly from physionet (https://physionet.org/content/mimiciii/1.4/) once becoming a credentialed user via training course:\n",
    "* ADMISSIONS\n",
    "* CHARTEVENTS\n",
    "* D_ITEMS\n",
    "* DIAGNOSES_ICD\n",
    "* ICUSTAYS\n",
    "* OUTPUTEVENTS\n",
    "* PATIENTS\n",
    "* PRESCRIPTIONS\n",
    "* PROCEDURES_ICD\n",
    "* SERVICES\n",
    "\n",
    "The following ER diagram helps illustrate how these tables are linked together: https://pi.cs.oswego.edu/~jmiles3/mimic/assets/MIMIC-ER-DIAGRAM.jpg\n",
    "  \n",
    "After downloading the compressed tables from physionet, we unzip the files locally and run a series of preprocessing steps that will ultimately lead to data arrays that we will use to train and evaluate the models. The preprocessing functions are shown in the Preprocessing Code section below. In addition to the MIMIC III tables listed above, the preprocessing functions also generate intermediate data files that can be used in later preprocessing steps that will ultimately lead to constructing the training/testing data arrays.  The implementation for the preprocessing code largely reuses the preprocessing code provided in the GitHub repo of the original paper (https://github.com/sebbarb/time_aware_attention), with some additional comments added and small fixes due to package and python versioning.\n",
    "\n",
    "All intermediate files that could be compressed to a small enough size to be uploaded to Github are in the data folder of the parent repo.  The original MIMIC III tables which are required for running the preprocessing steps cannot be uploaded to the parent repo, so the preprocessing code shown below represents what works for our team when we have downloaded the files locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzVUQS0CHry0"
   },
   "source": [
    "### Descriptive Statistics\n",
    "\n",
    "Due to the limitations of the pyhealth library on the MIMIC3Dataset, we were not able to get full descriptive statistics of every table used in this project.  However, for tables that could be parsed using the library, the results are shown in the cells below.  The can run assuming the user downloads the appropriate MIMIC III datasets and uncompresses them locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStatistics of base dataset (dev=False):\\n\\t- Dataset: MIMIC3Dataset\\n\\t- Number of patients: 46520\\n\\t- Number of visits: 58976\\n\\t- Number of visits per patient: 1.2678\\n\\t- Number of events per visit in DIAGNOSES_ICD: 11.0384\\n\\t- Number of events per visit in PRESCRIPTIONS: 70.4013\\n\\t- Number of events per visit in PROCEDURES_ICD: 4.0711\\n'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pyhealth.datasets import MIMIC3Dataset\n",
    "\n",
    "# ADMISSIONS and PATIENTS are part of our dataset but don't need to be included in this code since they are parsed by default\n",
    "# CHARTEVENTS, D_ITEMS, ICUSTAYS, OUTPUTEVENTS, PATIENTS, and SERVICES do not have a parser in this library as indicated\n",
    "# in the documentation: https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC3Dataset.html\n",
    "\n",
    "# The code below can be uncommented to generate statistics about a subset of tables used in this project.  The results are pasted in the comment at the bottom.\n",
    "# dataset = MIMIC3Dataset(\n",
    "#     root=\"../MIMIC-III Clinical Database/uncompressed/\",\n",
    "#     tables=[\"DIAGNOSES_ICD\", \"PRESCRIPTIONS\", \"PROCEDURES_ICD\"],\n",
    "# )\n",
    "# dataset.stat()\n",
    "# dataset.info()\n",
    "\n",
    "'''\n",
    "Statistics of base dataset (dev=False):\n",
    "\t- Dataset: MIMIC3Dataset\n",
    "\t- Number of patients: 46520\n",
    "\t- Number of visits: 58976\n",
    "\t- Number of visits per patient: 1.2678\n",
    "\t- Number of events per visit in DIAGNOSES_ICD: 11.0384\n",
    "\t- Number of events per visit in PRESCRIPTIONS: 70.4013\n",
    "\t- Number of events per visit in PROCEDURES_ICD: 4.0711\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzVUQS0CHry0"
   },
   "source": [
    "### Preprocessing Code\n",
    "\n",
    "The following 7 preprocessing steps need to be run in the same order as presented in this notebook.  Besides running each cell as such, there are no other requirements or steps in the prepprocessing stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: MIMIC-III ICUSTAYS, PATIENTS, ADMISSIONS, and SERVICES tables to combine into an intermediate dataset that joins these tables together.\n",
    "mimic_dir = '../MIMIC-III Clinical Database/uncompressed/'\n",
    "data_dir = './data/'\n",
    "\n",
    "min_count = 100 # words whose occurred less than min_cnt are encoded as OTHER\n",
    "\n",
    "def create_icu_pat_admit():\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    # Load icustays table\n",
    "    # Table purpose: Defines each ICUSTAY_ID in the database, i.e. defines a single ICU stay\n",
    "    print('Load ICU stays...')\n",
    "    dtype = {'SUBJECT_ID': 'int32',\n",
    "           'HADM_ID': 'int32',\n",
    "           'ICUSTAY_ID': 'int32',\n",
    "           'INTIME': 'str',\n",
    "           'OUTTIME': 'str',\n",
    "           'LOS': 'float32'}\n",
    "    parse_dates = ['INTIME', 'OUTTIME']\n",
    "    icustays = pd.read_csv(mimic_dir + 'ICUSTAYS.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    # Load patients table\n",
    "    # Table purpose: Contains all charted data for all patients.\n",
    "    print('Load patients...')\n",
    "    dtype = {'SUBJECT_ID': 'int32',\n",
    "           'GENDER': 'str',\n",
    "           'DOB': 'str',\n",
    "           'DOD': 'str'}\n",
    "    parse_dates = ['DOB', 'DOD']\n",
    "    patients = pd.read_csv(mimic_dir + 'PATIENTS.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)  \n",
    "    \n",
    "    # Adjust shifted DOBs for older patients (median imputation)\n",
    "    old_patient = patients['DOB'].dt.year < 2000\n",
    "    date_offset = pd.DateOffset(years=(300-91), days=(-0.4*365))\n",
    "    patients['DOB'][old_patient] = patients['DOB'][old_patient].apply(lambda x: x + date_offset)\n",
    "    \n",
    "    # Replace GENDER by dummy binary column \n",
    "    patients = pd.get_dummies(patients, columns = ['GENDER'], drop_first=True)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    print('Load admissions...')\n",
    "    # Load admissions table\n",
    "    # Table purpose: Define a patients hospital admission, HADM_ID.\n",
    "    dtype = {'SUBJECT_ID': 'int32', \n",
    "           'HADM_ID': 'int32',\n",
    "           'ADMISSION_LOCATION': 'str',\n",
    "           'INSURANCE': 'str',\n",
    "           'MARITAL_STATUS': 'str',\n",
    "           'ETHNICITY': 'str',\n",
    "           'ADMITTIME': 'str',\n",
    "           'ADMISSION_TYPE': 'str'}\n",
    "    parse_dates = ['ADMITTIME']\n",
    "    admissions = pd.read_csv(mimic_dir + 'ADMISSIONS.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    print('Load services...')\n",
    "    # Load services table\n",
    "    # Table purpose: Lists services that a patient was admitted/transferred under.\n",
    "    dtype = {'SUBJECT_ID': 'int32', \n",
    "           'HADM_ID': 'int32',\n",
    "           'TRANSFERTIME': 'str',\n",
    "           'CURR_SERVICE': 'str'}\n",
    "    parse_dates = ['TRANSFERTIME']\n",
    "    services = pd.read_csv(mimic_dir + 'SERVICES.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    # Link icustays and patients tables\n",
    "    print('Link icustays and patients tables...')\n",
    "    icu_pat = pd.merge(icustays, patients, how='inner', on='SUBJECT_ID')\n",
    "    icu_pat.sort_values(by=['SUBJECT_ID', 'OUTTIME'], ascending=[True, False], inplace=True)\n",
    "    assert len(icu_pat['SUBJECT_ID'].unique()) == 46476\n",
    "    assert len(icu_pat['ICUSTAY_ID'].unique()) == 61532\n",
    "    \n",
    "    # Exclude icu stays during which patient died\n",
    "    icu_pat = icu_pat[~(icu_pat['DOD'] <= icu_pat['OUTTIME'])]\n",
    "    assert len(icu_pat['SUBJECT_ID'].unique()) == 43126\n",
    "    assert len(icu_pat['ICUSTAY_ID'].unique()) == 56745\n",
    "    \n",
    "    # Determine number of icu discharges in the last 365 days\n",
    "    print('Compute number of recent admissions...')\n",
    "    icu_pat['NUM_RECENT_ADMISSIONS'] = 0\n",
    "    for name, group in tqdm(icu_pat.groupby(['SUBJECT_ID'])):\n",
    "        for index, row in group.iterrows():\n",
    "            days_diff = (row['OUTTIME']-group['OUTTIME']).dt.days\n",
    "            icu_pat.at[index, 'NUM_RECENT_ADMISSIONS'] = len(group[(days_diff > 0) & (days_diff <=365)])\n",
    "    \n",
    "    # Create age variable and exclude patients < 18 y.o.\n",
    "    icu_pat['AGE'] = (icu_pat['OUTTIME'] - icu_pat['DOB']).dt.days/365.\n",
    "    icu_pat = icu_pat[icu_pat['AGE'] >= 18]\n",
    "    assert len(icu_pat['SUBJECT_ID'].unique()) == 35233\n",
    "    assert len(icu_pat['ICUSTAY_ID'].unique()) == 48616\n",
    "    \n",
    "    # Time to next admission (discharge to admission!)\n",
    "    icu_pat['DAYS_TO_NEXT'] = (icu_pat.groupby(['SUBJECT_ID']).shift(1)['INTIME'] - icu_pat['OUTTIME']).dt.days\n",
    "    \n",
    "    # Add early readmission flag (less than 30 days after discharge)\n",
    "    icu_pat['POSITIVE'] = (icu_pat['DAYS_TO_NEXT'] <= 30)\n",
    "    assert icu_pat['POSITIVE'].sum() == 5495\n",
    "    \n",
    "    # Add early death flag (less than 30 days after discharge)\n",
    "    early_death = ((icu_pat['DOD'] - icu_pat['OUTTIME']).dt.days <= 30)\n",
    "    assert early_death.sum() == 3795\n",
    "    \n",
    "    # Censor negative patients who died within less than 30 days after discharge (no chance of readmission)\n",
    "    icu_pat = icu_pat[icu_pat['POSITIVE'] | ~early_death]\n",
    "    assert len(icu_pat['SUBJECT_ID'].unique()) == 33150\n",
    "    assert len(icu_pat['ICUSTAY_ID'].unique()) == 45298\n",
    "    \n",
    "    # Clean up\n",
    "    icu_pat.drop(columns=['DOB', 'DOD', 'DAYS_TO_NEXT'], inplace=True)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    # Link icu_pat and admissions tables\n",
    "    print('Link icu_pat and admissions tables...')\n",
    "    icu_pat_admit = pd.merge(icu_pat, admissions, how='left', on=['SUBJECT_ID', 'HADM_ID'])\n",
    "    print(icu_pat_admit.isnull().sum())\n",
    "    \n",
    "    print('Some data cleaning on admissions...')\n",
    "    icu_pat_admit.loc[icu_pat_admit['ETHNICITY'].str.contains('WHITE'), 'ETHNICITY']    = 'WHITE'\n",
    "    icu_pat_admit.loc[icu_pat_admit['ETHNICITY'].str.contains('BLACK'), 'ETHNICITY']    = 'BLACK/AFRICAN AMERICAN'\n",
    "    icu_pat_admit.loc[icu_pat_admit['ETHNICITY'].str.contains('ASIAN'), 'ETHNICITY']    = 'ASIAN'\n",
    "    icu_pat_admit.loc[icu_pat_admit['ETHNICITY'].str.contains('HISPANIC'), 'ETHNICITY'] = 'HISPANIC/LATINO'\n",
    "    icu_pat_admit.loc[icu_pat_admit['ETHNICITY'].str.contains('DECLINED'), 'ETHNICITY'] = 'OTHER/UNKNOWN'\n",
    "    icu_pat_admit.loc[icu_pat_admit['ETHNICITY'].str.contains('MULTI'), 'ETHNICITY']    = 'OTHER/UNKNOWN'\n",
    "    icu_pat_admit.loc[icu_pat_admit['ETHNICITY'].str.contains('UNKNOWN'), 'ETHNICITY']  = 'OTHER/UNKNOWN'\n",
    "    icu_pat_admit.loc[icu_pat_admit['ETHNICITY'].str.contains('OTHER'), 'ETHNICITY']  = 'OTHER/UNKNOWN'\n",
    "    \n",
    "    icu_pat_admit['MARITAL_STATUS'].fillna('UNKNOWN', inplace=True)\n",
    "    icu_pat_admit.loc[icu_pat_admit['MARITAL_STATUS'].str.contains('MARRIED'), 'MARITAL_STATUS']      = 'MARRIED/LIFE PARTNER'\n",
    "    icu_pat_admit.loc[icu_pat_admit['MARITAL_STATUS'].str.contains('LIFE PARTNER'), 'MARITAL_STATUS'] = 'MARRIED/LIFE PARTNER'\n",
    "    icu_pat_admit.loc[icu_pat_admit['MARITAL_STATUS'].str.contains('WIDOWED'), 'MARITAL_STATUS']      = 'WIDOWED/DIVORCED/SEPARATED'\n",
    "    icu_pat_admit.loc[icu_pat_admit['MARITAL_STATUS'].str.contains('DIVORCED'), 'MARITAL_STATUS']     = 'WIDOWED/DIVORCED/SEPARATED'\n",
    "    icu_pat_admit.loc[icu_pat_admit['MARITAL_STATUS'].str.contains('SEPARATED'), 'MARITAL_STATUS']    = 'WIDOWED/DIVORCED/SEPARATED'\n",
    "    icu_pat_admit.loc[icu_pat_admit['MARITAL_STATUS'].str.contains('UNKNOWN'), 'MARITAL_STATUS']      = 'OTHER/UNKNOWN'\n",
    "    \n",
    "    columns_to_mask = ['ADMISSION_LOCATION',\n",
    "                     'INSURANCE',\n",
    "                     'MARITAL_STATUS',\n",
    "                     'ETHNICITY']\n",
    "    icu_pat_admit = icu_pat_admit.apply(lambda x: x.mask(x.map(x.value_counts()) < min_count, 'OTHER/UNKNOWN') if x.name in columns_to_mask else x)                   \n",
    "    icu_pat_admit = icu_pat_admit.apply(lambda x: x.str.title() if x.name in columns_to_mask else x)\n",
    "    \n",
    "    # Compute pre-ICU length of stay in fractional days\n",
    "    icu_pat_admit['PRE_ICU_LOS'] = (icu_pat_admit['INTIME'] - icu_pat_admit['ADMITTIME']) / np.timedelta64(1, 'D')\n",
    "    icu_pat_admit.loc[icu_pat_admit['PRE_ICU_LOS']<0, 'PRE_ICU_LOS'] = 0\n",
    "    \n",
    "    # Clean up\n",
    "    icu_pat_admit.drop(columns=['ADMITTIME'], inplace=True)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    # Link services table\n",
    "    # Keep first service only\n",
    "    services.sort_values(by=['HADM_ID', 'TRANSFERTIME'], ascending=True, inplace=True)\n",
    "    services = services.groupby(['HADM_ID']).nth(0).reset_index()\n",
    "    \n",
    "    # Check if first service is a surgery\n",
    "    services['SURGERY'] = services['CURR_SERVICE'].str.contains('SURG') | (services['CURR_SERVICE'] == 'ORTHO')\n",
    "    \n",
    "    print('Link services table...')  \n",
    "    icu_pat_admit = pd.merge(icu_pat_admit, services, how='left', on=['SUBJECT_ID', 'HADM_ID'])\n",
    "    \n",
    "    # Get elective surgery admissions\n",
    "    icu_pat_admit['ELECTIVE_SURGERY'] = ((icu_pat_admit['ADMISSION_TYPE'] == 'ELECTIVE') & icu_pat_admit['SURGERY']).astype(int)\n",
    "    \n",
    "    # Clean up\n",
    "    icu_pat_admit.drop(columns=['TRANSFERTIME', 'CURR_SERVICE', 'ADMISSION_TYPE', 'SURGERY'], inplace=True)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    # Baseline characteristics table\n",
    "    pos = icu_pat_admit[icu_pat_admit['POSITIVE']==1]\n",
    "    neg = icu_pat_admit[icu_pat_admit['POSITIVE']==0]\n",
    "    print('Total pos {}'.format(len(pos)))\n",
    "    print('Total neg {}'.format(len(neg)))\n",
    "    print(pos['LOS'].describe())\n",
    "    print(neg['LOS'].describe())\n",
    "    print((pos['PRE_ICU_LOS']).describe())\n",
    "    print((neg['PRE_ICU_LOS']).describe())\n",
    "    pd.set_option('display.precision', 1)\n",
    "    print(pos['AGE'].describe())\n",
    "    print(neg['AGE'].describe())\n",
    "    print(pos['NUM_RECENT_ADMISSIONS'].describe())\n",
    "    print(neg['NUM_RECENT_ADMISSIONS'].describe())\n",
    "    print(pd.DataFrame({'COUNTS': pos['GENDER_M'].value_counts(), 'PERC': pos['GENDER_M'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': neg['GENDER_M'].value_counts(), 'PERC': neg['GENDER_M'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': pos['ADMISSION_LOCATION'].value_counts(), 'PERC': pos['ADMISSION_LOCATION'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': neg['ADMISSION_LOCATION'].value_counts(), 'PERC': neg['ADMISSION_LOCATION'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': pos['INSURANCE'].value_counts(), 'PERC': pos['INSURANCE'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': neg['INSURANCE'].value_counts(), 'PERC': neg['INSURANCE'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': pos['MARITAL_STATUS'].value_counts(), 'PERC': pos['MARITAL_STATUS'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': neg['MARITAL_STATUS'].value_counts(), 'PERC': neg['MARITAL_STATUS'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': pos['ETHNICITY'].value_counts(), 'PERC': pos['ETHNICITY'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': neg['ETHNICITY'].value_counts(), 'PERC': neg['ETHNICITY'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': pos['ELECTIVE_SURGERY'].value_counts(), 'PERC': pos['ELECTIVE_SURGERY'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': neg['ELECTIVE_SURGERY'].value_counts(), 'PERC': neg['ELECTIVE_SURGERY'].value_counts(normalize=True)*100}))  \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Save...')\n",
    "    assert len(icu_pat_admit) == 45298\n",
    "    icu_pat_admit.sort_values(by='ICUSTAY_ID', ascending=True, inplace=True)\n",
    "    icu_pat_admit.to_pickle(data_dir + 'icu_pat_admit.pkl')\n",
    "    icu_pat_admit.to_csv(data_dir + 'icu_pat_admit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: reduce chartevents size by removing implausible measurements, but also add human readable labels to indicate what the chart data represents\n",
    "# for each entry using data in D_ITEMS\n",
    "def reduce_chart_events():\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    \n",
    "    # Relevant ITEMIDs\n",
    "    gcs_eye_opening          = [184, 220739, 226756, 227011]\n",
    "    gcs_verbal_response      = [723, 223900, 226758, 227014]\n",
    "    gcs_motor_response       = [454, 223901, 226757, 227012]\n",
    "    gcs_total                = [198, 226755]\n",
    "    diastolic_blood_pressure = [8364, 8368, 8440, 8441, 8502, 8503, 8506, 8555, 220051, 220180, 224643, 225310, 227242]\n",
    "    systolic_blood_pressure  = [   6,   51,  442,  455, 3313, 3315, 3321, 6701, 220050, 220179, 224167, 225309, 227243]\n",
    "    mean_blood_pressure      = [52, 443, 456, 2293, 2294, 2647, 3312, 3314, 3320, 6590, 6702, 6927, 7620, 220052, 220181, 225312]\n",
    "    heart_rate               = [211, 220045, 227018]\n",
    "    fraction_inspired_oxygen = [189, 190, 727, 1040, 1206, 1863, 2518, 2981, 3420, 3422, 7018, 7041, 7570, 223835, 226754, 227009, 227010]\n",
    "    respiratory_rate         = [614, 615, 618, 619, 651, 653, 1884, 3603, 6749, 7884, 8113, 220210, 224422, 224688, 224689, 224690, 226774, 227050]\n",
    "    body_temperature         = [676, 677, 678, 679, 3652, 3654, 6643, 223761, 223762, 226778, 227054]\n",
    "    weight                   = [763, 3580, 3581, 3582, 3693, 224639, 226512, 226531]\n",
    "    height                   = [1394, 226707, 226730]\n",
    "    \n",
    "    def inch_to_cm(value):\n",
    "        return value*2.54\n",
    "    \n",
    "    def lb_to_kg(value):\n",
    "        return value/2.205\n",
    "    \n",
    "    def oz_to_kg(value):\n",
    "        return value/35.274\n",
    "    \n",
    "    def f_to_c(value):\n",
    "        return (value-32)*5/9\n",
    "    \n",
    "    def frac_to_perc(value):\n",
    "        return value*100\n",
    "    \n",
    "    # Relevant ITEMIDs\n",
    "    body_temperature_F       = [678, 679, 3652, 3654, 6643, 223761, 226778, 227054]\n",
    "    weight_lb                = [3581, 226531]\n",
    "    weight_oz                = [3582]\n",
    "    height_inch              = [1394, 226707]\n",
    "    \n",
    "    relevant_ids = (gcs_eye_opening + gcs_verbal_response + gcs_motor_response + gcs_total + mean_blood_pressure + \n",
    "                  heart_rate + fraction_inspired_oxygen + respiratory_rate + body_temperature + weight + height)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    print('Load item definitions')\n",
    "    dtype = {'ITEMID': 'int32',\n",
    "           'LABEL': 'str',\n",
    "           'UNITNAME': 'str'}\n",
    "    defs = pd.read_csv(mimic_dir + 'D_ITEMS.csv', usecols=dtype.keys(), dtype=dtype)\n",
    "    print('GCS_EYE_OPENING')\n",
    "    print(defs[defs['ITEMID'].isin(gcs_eye_opening)])\n",
    "    print('GCS_VERBAL_RESPONSE')\n",
    "    print(defs[defs['ITEMID'].isin(gcs_verbal_response)])\n",
    "    print('GCS_MOTOR_RESPONSE')\n",
    "    print(defs[defs['ITEMID'].isin(gcs_motor_response)])\n",
    "    print('GCS_TOTAL')\n",
    "    print(defs[defs['ITEMID'].isin(gcs_total)])\n",
    "    print('DIASTOLIC_BP')\n",
    "    print(defs[defs['ITEMID'].isin(diastolic_blood_pressure)])\n",
    "    print('SYSTOLIC_BP')\n",
    "    print(defs[defs['ITEMID'].isin(systolic_blood_pressure)])\n",
    "    print('MEAN_BP')\n",
    "    print(defs[defs['ITEMID'].isin(mean_blood_pressure)])\n",
    "    print('HEART_RATE')\n",
    "    print(defs[defs['ITEMID'].isin(heart_rate)])\n",
    "    print('FRACTION_INSPIRED_OXYGEN')\n",
    "    print(defs[defs['ITEMID'].isin(fraction_inspired_oxygen)])\n",
    "    print('RESPIRATORY_RATE')\n",
    "    print(defs[defs['ITEMID'].isin(respiratory_rate)])\n",
    "    print('BODY_TEMPERATURE')\n",
    "    print(defs[defs['ITEMID'].isin(body_temperature)])\n",
    "    print('WEIGHT')\n",
    "    print(defs[defs['ITEMID'].isin(weight)])\n",
    "    print('HEIGHT')\n",
    "    print(defs[defs['ITEMID'].isin(height)])\n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Loading Chart Events')\n",
    "    dtype = {'SUBJECT_ID': 'int32',\n",
    "           'HADM_ID': 'int32',\n",
    "           'ICUSTAY_ID': 'str',\n",
    "           'ITEMID': 'int32',\n",
    "           'CHARTTIME': 'str',\n",
    "           'VALUENUM': 'float32'}\n",
    "    parse_dates = ['CHARTTIME']\n",
    "    # Load chartevents table\n",
    "    # Table purpose: Contains all charted data for all patients.\n",
    "    chunksize = 1000000\n",
    "    i = 0\n",
    "    # Not parsing dates\n",
    "    for df in tqdm(pd.read_csv(mimic_dir + 'CHARTEVENTS.csv', usecols=dtype.keys(), dtype=dtype, chunksize=chunksize)):\n",
    "        df = df[df['ICUSTAY_ID'].notna() & df['VALUENUM'].notna() & (df['ITEMID'].isin(relevant_ids)) & (df['VALUENUM'] > 0)]\n",
    "        # convert units\n",
    "        df.loc[df['ITEMID'].isin(body_temperature_F), 'VALUENUM'] = f_to_c(df[df['ITEMID'].isin(body_temperature_F)].VALUENUM)\n",
    "        df.loc[df['ITEMID'].isin(weight_lb), 'VALUENUM'] = lb_to_kg(df[df['ITEMID'].isin(weight_lb)].VALUENUM)\n",
    "        df.loc[df['ITEMID'].isin(weight_oz), 'VALUENUM'] = oz_to_kg(df[df['ITEMID'].isin(weight_oz)].VALUENUM)\n",
    "        df.loc[df['ITEMID'].isin(height_inch), 'VALUENUM'] = inch_to_cm(df[df['ITEMID'].isin(height_inch)].VALUENUM)\n",
    "        df.loc[(df['ITEMID'].isin(fraction_inspired_oxygen)) & (df['VALUENUM']<=1), 'VALUENUM'] = frac_to_perc(df[(df['ITEMID'].isin(fraction_inspired_oxygen)) & (df['VALUENUM']<=1)].VALUENUM)\n",
    "        # remove implausible measurements\n",
    "        df = df[~(df['ITEMID'].isin(gcs_total) & (df.VALUENUM < 3))]\n",
    "        df = df[~(df['ITEMID'].isin(diastolic_blood_pressure + systolic_blood_pressure + mean_blood_pressure) & (df.VALUENUM > 250))]\n",
    "        df = df[~(df['ITEMID'].isin(heart_rate) & ((df.VALUENUM < 1) | (df.VALUENUM > 250)))]\n",
    "        df = df[~(df['ITEMID'].isin(fraction_inspired_oxygen) & (df.VALUENUM > 100))]\n",
    "        df = df[~(df['ITEMID'].isin(respiratory_rate) & ((df.VALUENUM < 1) | (df.VALUENUM > 100)))]\n",
    "        df = df[~(df['ITEMID'].isin(body_temperature) & (df.VALUENUM > 50))]\n",
    "        df = df[~(df['ITEMID'].isin(weight) & (df.VALUENUM > 700))]\n",
    "        df = df[~(df['ITEMID'].isin(height) & (df.VALUENUM > 300))]\n",
    "        df = df[df['VALUENUM'] > 0]\n",
    "        # label\n",
    "        df['CE_TYPE'] = ''\n",
    "        df.loc[df['ITEMID'].isin(gcs_eye_opening), 'CE_TYPE'] = 'GCS_EYE_OPENING'\n",
    "        df.loc[df['ITEMID'].isin(gcs_verbal_response), 'CE_TYPE'] = 'GCS_VERBAL_RESPONSE'\n",
    "        df.loc[df['ITEMID'].isin(gcs_motor_response), 'CE_TYPE'] = 'GCS_MOTOR_RESPONSE'\n",
    "        df.loc[df['ITEMID'].isin(gcs_total), 'CE_TYPE'] = 'GCS_TOTAL'\n",
    "        df.loc[df['ITEMID'].isin(diastolic_blood_pressure), 'CE_TYPE'] = 'DIASTOLIC_BP'\n",
    "        df.loc[df['ITEMID'].isin(systolic_blood_pressure), 'CE_TYPE'] = 'SYSTOLIC_BP'\n",
    "        df.loc[df['ITEMID'].isin(mean_blood_pressure), 'CE_TYPE'] = 'MEAN_BP'\n",
    "        df.loc[df['ITEMID'].isin(heart_rate), 'CE_TYPE'] = 'HEART_RATE'\n",
    "        df.loc[df['ITEMID'].isin(fraction_inspired_oxygen), 'CE_TYPE'] = 'FRACTION_INSPIRED_OXYGEN'\n",
    "        df.loc[df['ITEMID'].isin(respiratory_rate), 'CE_TYPE'] = 'RESPIRATORY_RATE'\n",
    "        df.loc[df['ITEMID'].isin(body_temperature), 'CE_TYPE'] = 'BODY_TEMPERATURE'\n",
    "        df.loc[df['ITEMID'].isin(weight), 'CE_TYPE'] = 'WEIGHT'\n",
    "        df.loc[df['ITEMID'].isin(height), 'CE_TYPE'] = 'HEIGHT'    \n",
    "        df.drop(columns=['ITEMID'], inplace=True)\n",
    "        \n",
    "        # save\n",
    "        if i == 0:\n",
    "            df.to_csv(data_dir + 'chartevents_reduced.csv', index=False)\n",
    "        else:\n",
    "            df.to_csv(data_dir + 'chartevents_reduced.csv', mode='a', header=False, index=False)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: reduce outputs events step by filtering on events specific to urine output, determined by data in D_ITEMS\n",
    "def reduce_output_events():\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    \n",
    "    # Relevant ITEMIDs, from https://github.com/vincentmajor/mimicfilters/blob/master/lists/OASIS_components/preprocess_urine_awk_str.txt\n",
    "    urine_output = [42810, 43171, 43173, 43175, 43348, 43355, 43365, 43372, 43373, 43374, 43379, 43380, 43431, 43462, 43522, 40405, 40428, 40534, \n",
    "    40288, 42042, 42068, 42111, 42119, 42209, 41857, 40715, 40056, 40061, 40085, 40094, 40096, 42001, 42676, 42556, 43093, 44325, 44706,\n",
    "    44506, 42859, 44237, 44313, 44752, 44824, 44837, 43576, 43589, 43633, 44911, 44925, 42362, 42463, 42507, 42510, 40055, 40057, 40065,\n",
    "    40069, 45804, 45841, 43811, 43812, 43856, 43897, 43931, 43966, 44080, 44103, 44132, 45304, 46177, 46532, 46578, 46658, 46748, 40651,\n",
    "    43053, 43057, 40473, 42130, 41922, 44253, 44278, 46180, 44684, 43333, 43347, 42592, 42666, 42765, 42892, 45927, 44834, 43638, 43654,\n",
    "    43519, 43537, 42366, 45991, 46727, 46804, 43987, 44051, 227489, 226566, 226627, 226631, 45415, 42111, 41510, 40055, 226559, 40428,\n",
    "    40580, 40612, 40094, 40848, 43685, 42362, 42463, 42510, 46748, 40972, 40973, 46456, 226561, 226567, 226632, 40096, 40651, 226557,\n",
    "    226558, 40715, 226563]\n",
    "    \n",
    "    # Relevant ITEMIDs\n",
    "    print('-----------------------------------------')\n",
    "    print('Load item definitions')\n",
    "    dtype = {'ITEMID': 'int32',\n",
    "           'LABEL': 'str',\n",
    "           'UNITNAME': 'str',\n",
    "           'LINKSTO': 'str'}\n",
    "    defs = pd.read_csv(mimic_dir + 'D_ITEMS.csv', usecols=dtype.keys(), dtype=dtype)\n",
    "    print('URINE_OUTPUT')\n",
    "    defs = defs[defs['ITEMID'].isin(urine_output)]\n",
    "    defs['LABEL'] = defs['LABEL'].str.lower()\n",
    "    # Remove measurements in /kg/hr\n",
    "    defs = defs[~(defs['LABEL'].str.contains('hr') | defs['LABEL'].str.contains('kg')) | defs['LABEL'].str.contains('nephro')]\n",
    "    print(defs['LABEL'])\n",
    "    urine_output = defs['ITEMID'].tolist()\n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Loading Output Events')\n",
    "    dtype = {'ICUSTAY_ID': 'str',\n",
    "           'ITEMID': 'int32',\n",
    "           'CHARTTIME': 'str',\n",
    "           'VALUE': 'float32'}\n",
    "    parse_dates = ['CHARTTIME']\n",
    "    \n",
    "    # Load outputevents table\n",
    "    # Table purpose: Output data for patients.\n",
    "    df = pd.read_csv(mimic_dir + 'OUTPUTEVENTS.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)\n",
    "    df = df.rename(columns={'VALUE': 'VALUENUM'})\n",
    "    df = df[df['ICUSTAY_ID'].notna() & df['VALUENUM'].notna() & (df['ITEMID'].isin(urine_output)) & (df['VALUENUM'] > 0)]\n",
    "    df['ICUSTAY_ID'] = df['ICUSTAY_ID'].astype('int32')\n",
    "    \n",
    "    # remove implausible measurements\n",
    "    df = df[~(df.VALUENUM > 10000)]\n",
    "    \n",
    "    # sum all outputs in one day\n",
    "    df.drop(columns=['ITEMID'], inplace=True)\n",
    "    df['CHARTTIME'] = df['CHARTTIME'].dt.date\n",
    "    df = df.groupby(['ICUSTAY_ID', 'CHARTTIME']).sum()\n",
    "    df['CE_TYPE'] = 'URINE_OUTPUT'\n",
    "    df = df[~(df.VALUENUM > 10000)]\n",
    "    \n",
    "    print('Remove admission and discharge days (since data on urine output is incomplete)')\n",
    "    # Load icustays table\n",
    "    # Table purpose: Defines each ICUSTAY_ID in the database, i.e. defines a single ICU stay\n",
    "    print('Load ICU stays...')\n",
    "    dtype = {'ICUSTAY_ID': 'int32',\n",
    "           'INTIME': 'str',\n",
    "           'OUTTIME': 'str'}\n",
    "    parse_dates = ['INTIME', 'OUTTIME']\n",
    "    icustays = pd.read_csv(mimic_dir + 'ICUSTAYS.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)\n",
    "    icustays['INTIME'] = icustays['INTIME'].dt.date\n",
    "    icustays['OUTTIME'] = icustays['OUTTIME'].dt.date\n",
    "    \n",
    "    # Merge\n",
    "    tmp = icustays[['ICUSTAY_ID', 'INTIME']].drop_duplicates()\n",
    "    tmp = tmp.rename(columns={'INTIME': 'CHARTTIME'})\n",
    "    tmp['ID_IN'] = 1\n",
    "    df = pd.merge(df, tmp, how='left', on=['ICUSTAY_ID', 'CHARTTIME'])\n",
    "    tmp = icustays[['ICUSTAY_ID', 'OUTTIME']].drop_duplicates()\n",
    "    tmp = tmp.rename(columns={'OUTTIME': 'CHARTTIME'})\n",
    "    tmp['ID_OUT'] = 1\n",
    "    df = pd.merge(df, tmp, how='left', on=['ICUSTAY_ID', 'CHARTTIME'])\n",
    "    \n",
    "    # Remove admission and discharge days\n",
    "    df = df[df['ID_IN'].isnull() & df['ID_OUT'].isnull()]\n",
    "    df.drop(columns=['ID_IN', 'ID_OUT'], inplace=True)\n",
    "    \n",
    "    # Add SUBJECT_ID and HADM_ID\n",
    "    icustays.drop(columns=['INTIME', 'OUTTIME'], inplace=True)  \n",
    "    df['CHARTTIME'] = pd.to_datetime(df['CHARTTIME']) + pd.DateOffset(hours=12)\n",
    "    \n",
    "    # Save\n",
    "    df.to_pickle(data_dir + 'outputevents_reduced.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Create a merged intermediate output file which contains content from the reduced chartevents and reduced output events generated\n",
    "# in the previous two steps\n",
    "def merge_chart_outputs():\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    # Load (reduced) chartevents table\n",
    "    print('Loading chart events...')\n",
    "    dtype = {'SUBJECT_ID': 'int32',\n",
    "           'ICUSTAY_ID': 'int32',\n",
    "           'CE_TYPE': 'str',\n",
    "           'CHARTTIME': 'str',\n",
    "           'VALUENUM': 'float32'}\n",
    "    parse_dates = ['CHARTTIME']\n",
    "    charts = pd.read_csv(data_dir + 'chartevents_reduced.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Compute BMI and GCS total...')\n",
    "    charts.sort_values(by=['SUBJECT_ID', 'ICUSTAY_ID', 'CHARTTIME'], ascending=[True, True, False], inplace=True)\n",
    "    \n",
    "    # Compute BMI\n",
    "    rows_bmi = (charts['CE_TYPE']=='WEIGHT') | (charts['CE_TYPE']=='HEIGHT')\n",
    "    charts_bmi = charts[rows_bmi]\n",
    "    charts_bmi = charts_bmi.pivot_table(index=['SUBJECT_ID', 'ICUSTAY_ID', 'CHARTTIME'], columns='CE_TYPE', values='VALUENUM')\n",
    "    charts_bmi = charts_bmi.rename_axis(None, axis=1).reset_index()\n",
    "    charts_bmi['HEIGHT'] = charts_bmi.groupby('SUBJECT_ID')['HEIGHT'].ffill()\n",
    "    charts_bmi['HEIGHT'] = charts_bmi.groupby('SUBJECT_ID')['HEIGHT'].bfill()\n",
    "    charts_bmi =  charts_bmi[~pd.isnull(charts_bmi).any(axis=1)]\n",
    "    charts_bmi['VALUENUM'] = charts_bmi['WEIGHT']/charts_bmi['HEIGHT']/charts_bmi['HEIGHT']*10000\n",
    "    charts_bmi['CE_TYPE'] = 'BMI'\n",
    "    charts_bmi.drop(columns=['HEIGHT', 'WEIGHT'], inplace=True)\n",
    "    \n",
    "    # Compute GCS total if not available\n",
    "    rows_gcs = (charts['CE_TYPE']=='GCS_EYE_OPENING') | (charts['CE_TYPE']=='GCS_VERBAL_RESPONSE') | (charts['CE_TYPE']=='GCS_MOTOR_RESPONSE') | (charts['CE_TYPE']=='GCS_TOTAL')\n",
    "    charts_gcs = charts[rows_gcs]\n",
    "    charts_gcs = charts_gcs.pivot_table(index=['SUBJECT_ID', 'ICUSTAY_ID', 'CHARTTIME'], columns='CE_TYPE', values='VALUENUM')\n",
    "    charts_gcs = charts_gcs.rename_axis(None, axis=1).reset_index()\n",
    "    null_gcs_total = charts_gcs['GCS_TOTAL'].isnull()\n",
    "    charts_gcs.loc[null_gcs_total, 'GCS_TOTAL'] = charts_gcs[null_gcs_total].GCS_EYE_OPENING + charts_gcs[null_gcs_total].GCS_VERBAL_RESPONSE + charts_gcs[null_gcs_total].GCS_MOTOR_RESPONSE\n",
    "    charts_gcs =  charts_gcs[~charts_gcs['GCS_TOTAL'].isnull()]\n",
    "    charts_gcs = charts_gcs.rename(columns={'GCS_TOTAL': 'VALUENUM'})\n",
    "    charts_gcs['CE_TYPE'] = 'GCS_TOTAL'\n",
    "    charts_gcs.drop(columns=['GCS_EYE_OPENING', 'GCS_VERBAL_RESPONSE', 'GCS_MOTOR_RESPONSE'], inplace=True)\n",
    "    \n",
    "    # Merge back with rest of the table\n",
    "    rows_others = ~rows_bmi & ~rows_gcs\n",
    "    charts = pd.concat([charts_bmi, charts_gcs, charts[rows_others]], ignore_index=True, sort=False)\n",
    "    charts.drop(columns=['SUBJECT_ID'], inplace=True)\n",
    "    charts.sort_values(by=['ICUSTAY_ID', 'CHARTTIME'], ascending=[True, False], inplace=True)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    # Load (reduced) outputevents table\n",
    "    print('Loading output events...')\n",
    "    outputs = pd.read_pickle(data_dir + 'outputevents_reduced.pkl')\n",
    "    df = pd.concat([charts, outputs], ignore_index=True, sort=False)\n",
    "    df.sort_values(by=['ICUSTAY_ID', 'CHARTTIME'], ascending=[True, False], inplace=True)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Create categorical variable...')\n",
    "    # Bin according to OASIS severity score\n",
    "    heart_rate_bins               = np.array([-1, 32.99, 88.5, 106.5, 125.5, np.Inf])\n",
    "    respiratory_rate_bins         = np.array([-1, 5.99, 12.5, 22.5, 30.5, 44.5, np.Inf])\n",
    "    body_temperature_bins         = np.array([-1, 33.21, 35.93, 36.39, 36.88, 39.88, np.Inf])\n",
    "    mean_bp_bins                  = np.array([-1, 20.64, 50.99, 61.32, 143.44, np.Inf])\n",
    "    fraction_inspired_oxygen_bins = np.array([-1, np.Inf])\n",
    "    gcs_total_bins                = np.array([-1, 7, 13, 14, 15])\n",
    "    bmi_bins                      = np.array([-1, 15, 16, 18.5, 25, 30, 35, 40, 45, 50, 60, np.Inf])\n",
    "    urine_output_bins             = np.array([-1, 670.99, 1426.99, 2543.99, 6896, np.Inf])\n",
    "    bins = [heart_rate_bins, respiratory_rate_bins, body_temperature_bins, mean_bp_bins, fraction_inspired_oxygen_bins, gcs_total_bins, urine_output_bins]\n",
    "\n",
    "    # Labels \n",
    "    heart_rate_labels               = ['CHART_HR_m1', 'CHART_HR_n', 'CHART_HR_p1', 'CHART_HR_p2', 'CHART_HR_p3']\n",
    "    respiratory_rate_labels         = ['CHART_RR_m2', 'CHART_RR_m1', 'CHART_RR_n', 'CHART_RR_p1', 'CHART_RR_p2', 'CHART_RR_p3']\n",
    "    body_temperature_labels         = ['CHART_BT_m3', 'CHART_BT_m2', 'CHART_BT_m1', 'CHART_BT_n', 'CHART_BT_p1', 'CHART_BT_p2']\n",
    "    mean_bp_labels                  = ['CHART_BP_m3', 'CHART_BP_m2', 'CHART_BP_m1', 'CHART_BP_n', 'CHART_BP_p1']\n",
    "    fraction_inspired_oxygen_labels = ['CHART_VENT']\n",
    "    gcs_total_labels                = ['CHART_GC_m3', 'CHART_GC_m2', 'CHART_GC_m1', 'CHART_GC_n']\n",
    "    bmi_labels                      = ['CHART_BM_m3', 'CHART_BM_m2', 'CHART_BM_m1', 'CHART_BM_n', 'CHART_BM_p1', 'CHART_BM_p2', 'CHART_BM_p3', 'CHART_BM_p4', 'CHART_BM_p5', 'CHART_BM_p6', 'CHART_BM_p7']\n",
    "    urine_output_labels             = ['CHART_UO_m3', 'CHART_UO_m2', 'CHART_UO_m1', 'CHART_UO_n', 'CHART_UO_p1']\n",
    "    labels = [heart_rate_labels, respiratory_rate_labels, body_temperature_labels, mean_bp_labels, fraction_inspired_oxygen_labels, gcs_total_labels, urine_output_labels]\n",
    "\n",
    "    # Chart event types\n",
    "    ce_types = ['HEART_RATE', 'RESPIRATORY_RATE', 'BODY_TEMPERATURE', 'MEAN_BP', 'FRACTION_INSPIRED_OXYGEN', 'GCS_TOTAL', 'URINE_OUTPUT']\n",
    "    \n",
    "    df_list = []\n",
    "    df_list_last_only = [] # for logistic regression\n",
    "    for type, label, bin in zip(ce_types, labels, bins):\n",
    "        # get chart events of a specific type\n",
    "        tmp = df[df['CE_TYPE'] == type]\n",
    "\n",
    "        # bin them and sort\n",
    "        tmp['VALUECAT'] = pd.cut(tmp['VALUENUM'], bins=bin, labels=label)\n",
    "        tmp.drop(columns=['CE_TYPE', 'VALUENUM'], inplace=True)\n",
    "        tmp.sort_values(by=['ICUSTAY_ID', 'CHARTTIME'], ascending=[True, False], inplace=True)\n",
    "\n",
    "        # remove consecutive duplicates\n",
    "        tmp = tmp[(tmp[['ICUSTAY_ID', 'VALUECAT']] != tmp[['ICUSTAY_ID', 'VALUECAT']].shift()).any(axis=1)]\n",
    "        df_list.append(tmp)\n",
    "\n",
    "        # for logistic regression, keep only the last measurement\n",
    "        tmp = tmp.drop_duplicates(subset='ICUSTAY_ID')\n",
    "        df_list_last_only.append(tmp)\n",
    "    \n",
    "    df = pd.concat(df_list, ignore_index=True, sort=False)\n",
    "    df.sort_values(by=['ICUSTAY_ID', 'CHARTTIME'], ascending=[True, False], inplace=True)\n",
    "    \n",
    "    # drop duplicates to keep size manageable\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Save...')\n",
    "    df.to_pickle(data_dir + 'charts_outputs_reduced.pkl')\n",
    "    df.to_csv(data_dir + 'charts_outputs_reduced.csv', index=False)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Save data for logistic regression...')\n",
    "    \n",
    "    # for logistic regression\n",
    "    df_last_only = pd.concat(df_list_last_only, ignore_index=True, sort=False)\n",
    "    df_last_only.sort_values(by=['ICUSTAY_ID', 'CHARTTIME'], ascending=[True, False], inplace=True)\n",
    "    df_last_only.to_pickle(data_dir + 'charts_outputs_last_only.pkl')\n",
    "    df_last_only.to_csv(data_dir + 'charts_outputs_last_only.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Links diagnoses with their associated procedures\n",
    "def link_diagnoses_procedures():\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    # Load icu_pat table\n",
    "    print('Loading icu_pat...')\n",
    "    icu_pat = pd.read_pickle(data_dir + 'icu_pat_admit.pkl')\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    print('Load admissions...')\n",
    "    # Load admissions table\n",
    "    # Table purpose: Define a patients hospital admission, HADM_ID.\n",
    "    dtype = {'HADM_ID': 'int32',\n",
    "           'ADMITTIME': 'str',\n",
    "           'DISCHTIME': 'str'}\n",
    "    parse_dates = ['ADMITTIME', 'DISCHTIME']\n",
    "    admissions = pd.read_csv(mimic_dir + 'ADMISSIONS.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    print('Load diagnoses and procedures...')\n",
    "    dtype = {'SUBJECT_ID': 'int32',\n",
    "           'HADM_ID': 'int32',\n",
    "           'ICD9_CODE': 'str'}\n",
    "\n",
    "    # Load diagnosis_icd table\n",
    "    # Table purpose: Contains ICD diagnoses for patients, most notably ICD-9 diagnoses.\n",
    "    diagnoses = pd.read_csv(mimic_dir + 'DIAGNOSES_ICD.csv', usecols=dtype.keys(), dtype=dtype)\n",
    "    diagnoses = diagnoses.dropna()\n",
    "\n",
    "    # Load procedures_icd table\n",
    "    # Table purpose: Contains ICD procedures for patients, most notably ICD-9 procedures.\n",
    "    procedures = pd.read_csv(mimic_dir + 'PROCEDURES_ICD.csv', usecols=dtype.keys(), dtype=dtype)\n",
    "    procedures = procedures.dropna()\n",
    "    \n",
    "    # Merge diagnoses and procedures\n",
    "    diagnoses['ICD9_CODE'] = 'DIAGN_' + diagnoses['ICD9_CODE'].str.lower().str.strip()\n",
    "    procedures['ICD9_CODE'] = 'PROCE_' + procedures['ICD9_CODE'].str.lower().str.strip()\n",
    "    diag_proc = pd.concat([diagnoses, procedures], ignore_index=True, sort=False)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    # Link diagnoses/procedures and admissions tables\n",
    "    print('Link diagnoses/procedures and admissions tables...')\n",
    "    diag_proc = pd.merge(diag_proc, admissions, how='inner', on='HADM_ID').drop(columns=['HADM_ID'])\n",
    "    \n",
    "    # Link diagnoses/procedures and icu_pat tables\n",
    "    print('Link diagnoses/procedures and icu_pat tables...')\n",
    "    diag_proc = pd.merge(icu_pat[['SUBJECT_ID', 'ICUSTAY_ID', 'OUTTIME']], diag_proc, how='left', on=['SUBJECT_ID'])\n",
    "    \n",
    "    # Remove codes related to future admissions using time difference to ADMITTIME\n",
    "    diag_proc['DAYS_TO_OUT'] = (diag_proc['OUTTIME']-diag_proc['ADMITTIME']) / np.timedelta64(1, 'D')\n",
    "    diag_proc = diag_proc[(diag_proc['DAYS_TO_OUT'] >= 0) | diag_proc['DAYS_TO_OUT'].isna()]\n",
    "\n",
    "    # Reset time value using time difference to DISCHTIME (0 if negative)\n",
    "    diag_proc['DAYS_TO_OUT'] = (diag_proc['OUTTIME']-diag_proc['DISCHTIME']) / np.timedelta64(1, 'D')\n",
    "    diag_proc.loc[diag_proc['DAYS_TO_OUT'] < 0, 'DAYS_TO_OUT'] = 0\n",
    "    diag_proc = diag_proc.drop(columns=['SUBJECT_ID', 'OUTTIME', 'ADMITTIME', 'DISCHTIME'])\n",
    "\n",
    "    # Lost some ICUSTAY_IDs with only negative DAYS_TO_OUT, merge back\n",
    "    diag_proc = pd.merge(icu_pat[['ICUSTAY_ID']], diag_proc, how='left', on=['ICUSTAY_ID'])\n",
    "    \n",
    "    print('Drop duplicates...')\n",
    "    diag_proc = diag_proc.drop_duplicates()\n",
    "    \n",
    "    print('Map rare codes to OTHER...')\n",
    "    diag_proc = diag_proc.apply(lambda x: x.mask(x.map(x.value_counts()) < min_count, 'other') if x.name in ['ICD9_CODE'] else x)                   \n",
    "    \n",
    "    print('-----------------------------------------')  \n",
    "    print('Save...')\n",
    "    assert len(diag_proc['ICUSTAY_ID'].unique()) == 45298\n",
    "    diag_proc.sort_values(by=['ICUSTAY_ID', 'DAYS_TO_OUT'], ascending=[True, True], inplace=True)\n",
    "    diag_proc.to_pickle(data_dir + 'diag_proc.pkl')\n",
    "    diag_proc.to_csv(data_dir + 'diag_proc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Links chartevents and outputevents data with relevant prescriptions\n",
    "def link_charts_prescriptions():\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    # Load icu_pat table\n",
    "    print('Loading icu_pat...')\n",
    "    icu_pat = pd.read_pickle(data_dir + 'icu_pat_admit.pkl')\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    print('Load charts and outputs...')\n",
    "    charts_outputs = pd.read_pickle(data_dir + 'charts_outputs_reduced.pkl')\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    print('Load prescriptions...')\n",
    "    dtype = {'ICUSTAY_ID': 'str',\n",
    "           'DRUG': 'str',\n",
    "           'STARTDATE': 'str'}\n",
    "    parse_dates = ['STARTDATE']\n",
    "    # Load prescriptions table\n",
    "    # Table purpose: Contains medication related order entries, i.e. prescriptions\n",
    "    prescriptions = pd.read_csv(mimic_dir + 'PRESCRIPTIONS.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)\n",
    "    prescriptions = prescriptions.dropna()\n",
    "    prescriptions['ICUSTAY_ID'] = prescriptions['ICUSTAY_ID'].astype('int32')\n",
    "    prescriptions['DRUG'] = 'PRESC_' + prescriptions['DRUG'].str.lower().replace('\\s+', '', regex=True)\n",
    "    prescriptions = prescriptions.rename(columns={'DRUG': 'VALUECAT', 'STARTDATE': 'CHARTTIME'})\n",
    "    df = pd.concat([charts_outputs, prescriptions], ignore_index=True, sort=False)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    # Link charts/outputs and icu_pat tables\n",
    "    print('Link charts/outputs and icu_pat tables...')\n",
    "    df = pd.merge(icu_pat[['ICUSTAY_ID', 'OUTTIME']], df, how='left', on=['ICUSTAY_ID'])\n",
    "    \n",
    "    # Reset time value using time difference to DISCHTIME (0 if negative)\n",
    "    df['HOURS_TO_OUT'] = (df['OUTTIME']-df['CHARTTIME']) / np.timedelta64(1, 'h')\n",
    "    df.loc[df['HOURS_TO_OUT'] < 0, 'HOURS_TO_OUT'] = 0\n",
    "    df = df.drop(columns=['OUTTIME', 'CHARTTIME'])\n",
    "    \n",
    "    print('Drop duplicates...')\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    print('Map rare codes to OTHER...')\n",
    "    df = df.apply(lambda x: x.mask(x.map(x.value_counts()) < min_count, 'other') if x.name in ['VALUECAT'] else x)                   \n",
    "    \n",
    "    print('-----------------------------------------')  \n",
    "    print('Save...')\n",
    "    assert len(df['ICUSTAY_ID'].unique()) == 45298\n",
    "    df.sort_values(by=['ICUSTAY_ID', 'HOURS_TO_OUT'], ascending=[True, True], inplace=True)\n",
    "    df.to_pickle(data_dir + 'charts_prescriptions.pkl')\n",
    "    df.to_csv(data_dir + 'charts_prescriptions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: creates data arrays step used for training/validating/testing all models\n",
    "def create_data_arrays():\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    def get_arrays(df, code_column, time_column, quantile=1):\n",
    "      df['COUNT'] = df.groupby(['ICUSTAY_ID']).cumcount()\n",
    "      df = df[df['COUNT'] < df.groupby(['ICUSTAY_ID']).size().quantile(q=quantile)]\n",
    "      max_count_df = df['COUNT'].max()+1\n",
    "      print('max_count {}'.format(max_count_df))\n",
    "      multiindex_df = pd.MultiIndex.from_product([icu_pat['ICUSTAY_ID'], range(max_count_df)], names = ['ICUSTAY_ID', 'COUNT'])\n",
    "      df = df.set_index(['ICUSTAY_ID', 'COUNT'])\n",
    "    \n",
    "      print('Reindex df...')\n",
    "      df = df.reindex(multiindex_df).fillna(0)\n",
    "      print('done')\n",
    "      df_times = df[time_column].values.reshape((num_icu_stays, max_count_df))\n",
    "      df[code_column] = df[code_column].astype('category')\n",
    "      dict_df = dict(enumerate(df[code_column].cat.categories))\n",
    "      df[code_column] = df[code_column].cat.codes\n",
    "      df = df[code_column].values.reshape((num_icu_stays, max_count_df))\n",
    "    \n",
    "      return df, df_times, dict_df\n",
    "  \n",
    "    # Load icu_pat table\n",
    "    print('Loading icu_pat...')\n",
    "    icu_pat = pd.read_pickle(data_dir + 'icu_pat_admit.pkl')\n",
    "    \n",
    "    print('Loading diagnoses/procedures...')\n",
    "    dp = pd.read_pickle(data_dir + 'diag_proc.pkl')\n",
    "    \n",
    "    print('Loading charts/prescriptions...')\n",
    "    cp = pd.read_pickle(data_dir + 'charts_prescriptions.pkl')\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    num_icu_stays = len(icu_pat['ICUSTAY_ID'])\n",
    "    \n",
    "    # static variables\n",
    "    print('Create static array...')\n",
    "    icu_pat = pd.get_dummies(icu_pat, columns = ['ADMISSION_LOCATION', 'INSURANCE', 'MARITAL_STATUS', 'ETHNICITY'])\n",
    "    icu_pat.drop(columns=['ADMISSION_LOCATION_Emergency Room Admit', 'INSURANCE_Medicare', 'MARITAL_STATUS_Married/Life Partner', 'ETHNICITY_White'], inplace=True) # drop reference columns\n",
    "    static_columns = icu_pat.columns.str.contains('AGE|GENDER_M|LOS|NUM_RECENT_ADMISSIONS|ADMISSION_LOCATION|INSURANCE|MARITAL_STATUS|ETHNICITY|PRE_ICU_LOS|ELECTIVE_SURGERY')\n",
    "    static = icu_pat.loc[:, static_columns].values\n",
    "    static_vars = icu_pat.loc[:, static_columns].columns.values.tolist()\n",
    "    \n",
    "    # classification label\n",
    "    print('Create label array...')\n",
    "    label = icu_pat.loc[:, 'POSITIVE'].values\n",
    "    \n",
    "    # diagnoses/procedures and charts/prescriptions\n",
    "    print('Create diagnoses/procedures and charts/prescriptions array...')\n",
    "    dp, dp_times, dict_dp = get_arrays(dp, 'ICD9_CODE', 'DAYS_TO_OUT', 1)\n",
    "    cp, cp_times, dict_cp = get_arrays(cp, 'VALUECAT', 'HOURS_TO_OUT', 0.95)\n",
    "    \n",
    "    # Normalize times\n",
    "    dp_times = dp_times/dp_times.max()\n",
    "    cp_times = cp_times/cp_times.max()\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Split data into train/validate/test...')\n",
    "    # Split patients to avoid data leaks\n",
    "    patients = icu_pat['SUBJECT_ID'].drop_duplicates()\n",
    "    train, validate, test = np.split(patients.sample(frac=1, random_state=123), [int(.9*len(patients)), int(.9*len(patients))])\n",
    "    train_ids = icu_pat['SUBJECT_ID'].isin(train).values\n",
    "    validate_ids = icu_pat['SUBJECT_ID'].isin(validate).values\n",
    "    test_ids = icu_pat['SUBJECT_ID'].isin(test).values\n",
    "    \n",
    "    print('Get patients corresponding to test ids')\n",
    "    test_ids_patients = icu_pat['SUBJECT_ID'].iloc[test_ids].reset_index(drop=True)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Save...')\n",
    "    np.savez(data_dir + 'data_arrays.npz', static=static, static_vars=static_vars, label=label,\n",
    "           dp=dp, cp=cp, dp_times=dp_times, cp_times=cp_times, dict_dp=dict_dp, dict_cp=dict_cp,\n",
    "           train_ids=train_ids, validate_ids=validate_ids, test_ids=test_ids)\n",
    "    # np.savez(data_dir + 'data_dictionaries.npz', dict_dp=dict_dp, dict_cp=dict_cp)\n",
    "    test_ids_patients.to_pickle(data_dir + 'test_ids_patients.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "BZScZNbROw-N"
   },
   "outputs": [],
   "source": [
    "# Order in which the preprocessing steps must be run in order to produce the final data_arrays.npz and test_ids_patients.pkl files needed for training/validating/testing\n",
    "\n",
    "# create_icu_pat_admit()\n",
    "# reduce_chart_events()\n",
    "# reduce_output_events()\n",
    "# merge_chart_outputs()\n",
    "# link_diagnoses_procedures()\n",
    "# link_charts_prescriptions()\n",
    "# create_data_arrays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "  batch_size = 128\n",
    "  num_epochs = 80\n",
    "  dropout_rate = 0.5\n",
    "  patience = 10 # early stopping\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data, type):\n",
    "  # Data\n",
    "  static       = data['static'].astype('float32')\n",
    "  label        = data['label'].astype('float32')\n",
    "  dp           = data['dp'].astype('int64') # diagnoses/procedures\n",
    "  cp           = data['cp'].astype('int64') # charts/prescriptions\n",
    "  dp_times     = data['dp_times'].astype('float32')\n",
    "  cp_times     = data['cp_times'].astype('float32')\n",
    "  train_ids    = data['train_ids']\n",
    "  validate_ids = data['validate_ids']\n",
    "  test_ids     = data['test_ids']  \n",
    "\n",
    "  if (type == 'TRAIN'):\n",
    "    ids = train_ids\n",
    "  elif (type == 'VALIDATE'):\n",
    "    ids = validate_ids\n",
    "  elif (type == 'TEST'):\n",
    "    ids = test_ids\n",
    "  elif (type == 'ALL'):\n",
    "    ids = np.full_like(label, True, dtype=bool)\n",
    "\n",
    "  static   = static[ids, :]\n",
    "  label    = label[ids]\n",
    "  dp       = dp[ids, :]\n",
    "  cp       = cp[ids, :]\n",
    "  dp_times = dp_times[ids, :]\n",
    "  cp_times = cp_times[ids, :]\n",
    "  \n",
    "  return static, dp, cp, dp_times, cp_times, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_trainloader(data, type, shuffle=True, idx=None):\n",
    "  # Data\n",
    "  static, dp, cp, dp_times, cp_times, label = get_data(data, type)\n",
    "\n",
    "  # Bootstrap\n",
    "  if idx is not None:\n",
    "    static, dp, cp, dp_times, cp_times, label = static[idx], dp[idx], cp[idx], dp_times[idx], cp_times[idx], label[idx]\n",
    "\n",
    "  # Compute total batch count\n",
    "  num_batches = len(label) // batch_size\n",
    "  \n",
    "  # Create dataset\n",
    "  dataset = utils.TensorDataset(torch.from_numpy(static), \n",
    "                                torch.from_numpy(dp),\n",
    "                                torch.from_numpy(cp),\n",
    "                                torch.from_numpy(dp_times),\n",
    "                                torch.from_numpy(cp_times),\n",
    "                                torch.from_numpy(label))\n",
    "\n",
    "  # # Create batch queues\n",
    "  trainloader = utils.DataLoader(dataset,\n",
    "                                 batch_size = batch_size, \n",
    "                                 shuffle = shuffle,\n",
    "                                 sampler = None,\n",
    "                                 num_workers = 2,\n",
    "                                 drop_last = True)\n",
    "                                 \n",
    "  # # Weight of positive samples for training\n",
    "  pos_weight = torch.tensor((len(label) - np.sum(label))/np.sum(label))\n",
    "  \n",
    "  return trainloader, num_batches, pos_weight\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3muyDPFPbozY"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Model\n",
    "\n",
    "The combination of the different approaches that we plan to experiment and evaluate the outcomes are listed below:\n",
    "  * RNN (concatenated Δtime)\n",
    "  * RNN (concatenated Δtime) + Attention\n",
    "  * Attention (concatenated time)\n",
    "  * ODE + RNN + Attention\n",
    "  * ODE + Attention\n",
    "  * ODE + RNN    \n",
    "  * RNN (exp time decay) + Attention\n",
    "  * RNN (exp time decay)    \n",
    "  * RNN (ODE time decay)   \n",
    "  * RNN (ODE time decay) + Attention    \n",
    "  * MCE + RNN + Attention\n",
    "  * MCE + RNN\n",
    "  * MCE + Attention\n",
    "  * Logistic Regression\n",
    "\n",
    "All the above models have been implemented - trained and tested.\n",
    "Each model is defined in its own class, with the comments in the class outlining each step. \n",
    "\n",
    "Training is done on the output of the preprocessing step - data_arrays.npz.\n",
    "\n",
    "Loss function : BCEWithLogitsLoss\n",
    "\n",
    "Optimizer : Adam for stochastic gradient descent\n",
    "\n",
    "The trained model definitions are stored under \"logdir\". A separate folder by model captures the \"final_model.pt\" file.\n",
    "Model need not be trained every time, it does take on an average of 6-8 hours for training due to the data volumes and the devidce being \"cpu\".\n",
    "The pretrained models that are uploaded can be used for evaluation of results. However, if desired the trauining can also be done, the only prerequisities would be ensuring the required data files are available in data folder and the preprocessing steps to generate the \"data_arrays.npz\" have successfully completed.\n",
    "\n",
    "Subsequent functions for testing the model and capturing the results in the \"results\" folder are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class birnn_concat_time_delta(nn.Module):\n",
    "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
    "      super(birnn_concat_time_delta, self).__init__()\n",
    "      \n",
    "      # Embedding dimensions\n",
    "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))\n",
    "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))\n",
    "\n",
    "      # Embedding layers\n",
    "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
    "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
    "\n",
    "      # GRU layers\n",
    "      self.gru_dp_fw = nn.GRU(input_size=self.embed_dp_dim+1, hidden_size=self.embed_dp_dim+1, num_layers=1, batch_first=True)\n",
    "      self.gru_cp_fw = nn.GRU(input_size=self.embed_cp_dim+1, hidden_size=self.embed_cp_dim+1, num_layers=1, batch_first=True)\n",
    "      self.gru_dp_bw = nn.GRU(input_size=self.embed_dp_dim+1, hidden_size=self.embed_dp_dim+1, num_layers=1, batch_first=True)\n",
    "      self.gru_cp_bw = nn.GRU(input_size=self.embed_cp_dim+1, hidden_size=self.embed_cp_dim+1, num_layers=1, batch_first=True)\n",
    "      \n",
    "      # Fully connected output\n",
    "      self.fc_dp  = nn.Linear(2*(self.embed_dp_dim+1), 1)\n",
    "      self.fc_cp  = nn.Linear(2*(self.embed_cp_dim+1), 1)\n",
    "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
    "      \n",
    "      # Others\n",
    "      self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
    "      # Compute time delta\n",
    "      ## output dim: batch_size x seq_len\n",
    "      dp_t_delta_fw = abs_time_to_delta(dp_t)\n",
    "      cp_t_delta_fw = abs_time_to_delta(cp_t)\n",
    "      dp_t_delta_bw = abs_time_to_delta(torch.flip(dp_t, [1]))\n",
    "      cp_t_delta_bw = abs_time_to_delta(torch.flip(cp_t, [1]))    \n",
    "    \n",
    "      # Embedding\n",
    "      ## output dim: batch_size x seq_len x embedding_dim\n",
    "      embedded_dp_fw = self.embed_dp(dp)\n",
    "      embedded_cp_fw = self.embed_cp(cp)\n",
    "      embedded_dp_bw = torch.flip(embedded_dp_fw, [1])\n",
    "      embedded_cp_bw = torch.flip(embedded_cp_fw, [1])\n",
    "      \n",
    "      # Concatate with time\n",
    "      ## output dim: batch_size x seq_len x (embedding_dim+1)\n",
    "      concat_dp_fw = torch.cat((embedded_dp_fw, torch.unsqueeze(dp_t_delta_fw, dim=-1)), dim=-1)\n",
    "      concat_cp_fw = torch.cat((embedded_cp_fw, torch.unsqueeze(cp_t_delta_fw, dim=-1)), dim=-1)\n",
    "      concat_dp_bw = torch.cat((embedded_dp_bw, torch.unsqueeze(dp_t_delta_bw, dim=-1)), dim=-1)\n",
    "      concat_cp_bw = torch.cat((embedded_cp_bw, torch.unsqueeze(cp_t_delta_bw, dim=-1)), dim=-1)\n",
    "      ## Dropout\n",
    "      concat_dp_fw = self.dropout(concat_dp_fw)\n",
    "      concat_cp_fw = self.dropout(concat_cp_fw)\n",
    "      concat_dp_bw = self.dropout(concat_dp_bw)\n",
    "      concat_cp_bw = self.dropout(concat_cp_bw)\n",
    "      \n",
    "      # GRU\n",
    "      ## output dim rnn:        batch_size x seq_len x (embedding_dim+1)\n",
    "      ## output dim rnn_hidden: batch_size x 1 x (embedding_dim+1)\n",
    "      rnn_dp_fw, rnn_hidden_dp_fw = self.gru_dp_fw(concat_dp_fw)\n",
    "      rnn_cp_fw, rnn_hidden_cp_fw = self.gru_cp_fw(concat_cp_fw)\n",
    "      rnn_dp_bw, rnn_hidden_dp_bw = self.gru_dp_bw(concat_dp_bw)\n",
    "      rnn_cp_bw, rnn_hidden_cp_bw = self.gru_cp_bw(concat_cp_bw)      \n",
    "      ## output dim rnn_hidden: batch_size x (embedding_dim+1)\n",
    "      rnn_hidden_dp_fw = rnn_hidden_dp_fw.view(-1, self.embed_dp_dim+1)\n",
    "      rnn_hidden_cp_fw = rnn_hidden_cp_fw.view(-1, self.embed_cp_dim+1)\n",
    "      rnn_hidden_dp_bw = rnn_hidden_dp_bw.view(-1, self.embed_dp_dim+1)\n",
    "      rnn_hidden_cp_bw = rnn_hidden_cp_bw.view(-1, self.embed_cp_dim+1)\n",
    "      ## concatenate forward and backward: batch_size x 2*(embedding_dim+1)\n",
    "      rnn_hidden_dp = torch.cat((rnn_hidden_dp_fw, rnn_hidden_dp_bw), dim=-1)\n",
    "      rnn_hidden_cp = torch.cat((rnn_hidden_cp_fw, rnn_hidden_cp_bw), dim=-1)\n",
    "      \n",
    "      # Scores\n",
    "      score_dp = self.fc_dp(self.dropout(rnn_hidden_dp))\n",
    "      score_cp = self.fc_cp(self.dropout(rnn_hidden_cp))\n",
    "\n",
    "      # Concatenate to variable collection\n",
    "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
    "      \n",
    "      # Final linear projection\n",
    "      out = self.fc_all(self.dropout(all)).squeeze()\n",
    "\n",
    "      return out, []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUOdeDecay(nn.Module):\n",
    "  \"\"\"\n",
    "  GRU RNN module where the hidden state decays according to an ODE.\n",
    "  (see Rubanova et al. 2019, Latent ODEs for Irregularly-Sampled Time Series)\n",
    "  \n",
    "  Args:\n",
    "    inputs: A `Tensor` with embeddings in the last dimension.\n",
    "    times: A `Tensor` with the same shape as inputs containing the recorded times (but no embedding dimension).\n",
    "\n",
    "  Returns:\n",
    "    outs: Hidden states of the RNN.\n",
    "  \"\"\"\n",
    "  def __init__(self, input_size, hidden_size, bias=True):\n",
    "    super(GRUOdeDecay, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.gru_cell = nn.GRUCell(input_size, hidden_size)\n",
    "    self.decays = nn.Parameter(torch.Tensor(hidden_size)) # exponential decays vector\n",
    "    \n",
    "    # ODE\n",
    "    self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    self.ode_net = ODENet(self.device, self.input_size, self.input_size, output_dim=self.input_size, augment_dim=0, time_dependent=False, non_linearity='softplus', tol=1e-3, adjoint=True)\n",
    "  \n",
    "  def forward(self, inputs, times):\n",
    "    # initializing and then calling cuda() later isn't working for some reason\n",
    "    if torch.cuda.is_available():\n",
    "      hn = torch.zeros(inputs.size(0), self.hidden_size).cuda() # batch_size x hidden_size\n",
    "      outs = torch.zeros(inputs.size(0), inputs.size(1), self.hidden_size).cuda() # batch_size x seq_len x hidden_size\n",
    "    else:\n",
    "      hn = torch.zeros(inputs.size(0), self.hidden_size) # batch_size x hidden_size\n",
    "      outs = torch.zeros(inputs.size(0), inputs.size(1), self.hidden_size) # batch_size x seq_len x hidden_size\n",
    "\n",
    "    # this is slow\n",
    "    for seq in range(inputs.size(1)):\n",
    "      hn = self.gru_cell(inputs[:,seq,:], hn)\n",
    "      outs[:,seq,:] = hn\n",
    "      \n",
    "      times_unique, inverse_indices = torch.unique(times[:,seq], sorted=True, return_inverse=True)\n",
    "      if times_unique.size(0) > 1:\n",
    "        hn = self.ode_net(hn, times_unique)\n",
    "        hn = hn[inverse_indices, torch.arange(0, inverse_indices.size(0)), :]\n",
    "    return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(torch.nn.Module):\n",
    "  \"\"\"\n",
    "  Dot-product attention module.\n",
    "  \n",
    "  Args:\n",
    "    inputs: A `Tensor` with embeddings in the last dimension.\n",
    "    mask: A `Tensor`. Dimensions are the same as inputs but without the embedding dimension.\n",
    "      Values are 0 for 0-padding in the input and 1 elsewhere.\n",
    "\n",
    "  Returns:\n",
    "    outputs: The input `Tensor` whose embeddings in the last dimension have undergone a weighted average.\n",
    "      The second-last dimension of the `Tensor` is removed.\n",
    "    attention_weights: weights given to each embedding.\n",
    "  \"\"\"\n",
    "  def __init__(self, embedding_dim):\n",
    "    super(Attention, self).__init__()\n",
    "    self.context = nn.Parameter(torch.Tensor(embedding_dim)) # context vector\n",
    "    self.linear_hidden = nn.Linear(embedding_dim, embedding_dim)\n",
    "    self.reset_parameters()\n",
    "    \n",
    "  def reset_parameters(self):\n",
    "    nn.init.normal_(self.context)\n",
    "\n",
    "  def forward(self, inputs, mask):\n",
    "    # Hidden representation of embeddings (no change in dimensions)\n",
    "    hidden = torch.tanh(self.linear_hidden(inputs))\n",
    "    # Compute weight of each embedding\n",
    "    importance = torch.sum(hidden * self.context, dim=-1)\n",
    "    importance = importance.masked_fill(mask == 0, -1e9)\n",
    "    # Softmax so that weights sum up to one\n",
    "    attention_weights = F.softmax(importance, dim=-1)\n",
    "    # Weighted sum of embeddings\n",
    "    weighted_projection = inputs * torch.unsqueeze(attention_weights, dim=-1)\n",
    "    # Output\n",
    "    outputs = torch.sum(weighted_projection, dim=-2)\n",
    "    return outputs, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_STEPS = 1000 \n",
    "\n",
    "class ODEFunc(nn.Module):\n",
    "    \"\"\"MLP modeling the derivative of ODE system.\n",
    "    Parameters\n",
    "    ----------\n",
    "    device : torch.device\n",
    "    data_dim : int\n",
    "        Dimension of data.\n",
    "    hidden_dim : int\n",
    "        Dimension of hidden layers.\n",
    "    augment_dim: int\n",
    "        Dimension of augmentation. If 0 does not augment ODE, otherwise augments\n",
    "        it with augment_dim dimensions.\n",
    "    time_dependent : bool\n",
    "        If True adds time as input, making ODE time dependent.\n",
    "    non_linearity : string\n",
    "        One of 'relu' and 'softplus'\n",
    "    \"\"\"\n",
    "    def __init__(self, device, data_dim, hidden_dim, augment_dim=0,\n",
    "                 time_dependent=False, non_linearity='relu'):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        self.device = device\n",
    "        self.augment_dim = augment_dim\n",
    "        self.data_dim = data_dim\n",
    "        self.input_dim = data_dim + augment_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.nfe = 0  # Number of function evaluations\n",
    "        self.time_dependent = time_dependent\n",
    "\n",
    "        if time_dependent:\n",
    "            self.fc1 = nn.Linear(self.input_dim + 1, hidden_dim)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(self.input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, self.input_dim)\n",
    "\n",
    "        if non_linearity == 'relu':\n",
    "            self.non_linearity = nn.ReLU(inplace=True)\n",
    "        elif non_linearity == 'softplus':\n",
    "            self.non_linearity = nn.Softplus()\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        t : torch.Tensor\n",
    "            Current time. Shape (1,).\n",
    "        x : torch.Tensor\n",
    "            Shape (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        # Forward pass of model corresponds to one function evaluation, so\n",
    "        # increment counter\n",
    "        self.nfe += 1\n",
    "        if self.time_dependent:\n",
    "            # Shape (batch_size, 1)\n",
    "            t_vec = torch.ones(x.shape[0], 1).to(self.device) * t\n",
    "            # Shape (batch_size, data_dim + 1)\n",
    "            t_and_x = torch.cat([t_vec, x], 1)\n",
    "            # Shape (batch_size, hidden_dim)\n",
    "            out = self.fc1(t_and_x)\n",
    "        else:\n",
    "            out = self.fc1(x)\n",
    "        out = self.non_linearity(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.non_linearity(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ODEBlock(nn.Module):\n",
    "    \"\"\"Solves ODE defined by odefunc.\n",
    "    Parameters\n",
    "    ----------\n",
    "    device : torch.device\n",
    "    odefunc : ODEFunc instance or anode.conv_models.ConvODEFunc instance\n",
    "        Function defining dynamics of system.\n",
    "    is_conv : bool\n",
    "        If True, treats odefunc as a convolutional model.\n",
    "    tol : float\n",
    "        Error tolerance.\n",
    "    adjoint : bool\n",
    "        If True calculates gradient with adjoint method, otherwise\n",
    "        backpropagates directly through operations of ODE solver.\n",
    "    \"\"\"\n",
    "    def __init__(self, device, odefunc, is_conv=False, tol=1e-3, adjoint=False):\n",
    "        super(ODEBlock, self).__init__()\n",
    "        self.adjoint = adjoint\n",
    "        self.device = device\n",
    "        self.is_conv = is_conv\n",
    "        self.odefunc = odefunc\n",
    "        self.tol = tol\n",
    "\n",
    "    def forward(self, x, eval_times=None):\n",
    "        \"\"\"Solves ODE starting from x.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape (batch_size, self.odefunc.data_dim)\n",
    "        eval_times : None or torch.Tensor\n",
    "            If None, returns solution of ODE at final time t=1. If torch.Tensor\n",
    "            then returns full ODE trajectory evaluated at points in eval_times.\n",
    "        \"\"\"\n",
    "        # Forward pass corresponds to solving ODE, so reset number of function\n",
    "        # evaluations counter\n",
    "        self.odefunc.nfe = 0\n",
    "        \n",
    "        if eval_times is None:\n",
    "            integration_time = torch.tensor([0, 1]).float().type_as(x)\n",
    "        else:\n",
    "            integration_time = eval_times.type_as(x)\n",
    "\n",
    "\n",
    "        if self.odefunc.augment_dim > 0:\n",
    "            if self.is_conv:\n",
    "                # Add augmentation\n",
    "                batch_size, channels, height, width = x.shape\n",
    "                aug = torch.zeros(batch_size, self.odefunc.augment_dim,\n",
    "                                  height, width).to(self.device)\n",
    "                # Shape (batch_size, channels + augment_dim, height, width)\n",
    "                x_aug = torch.cat([x, aug], 1)\n",
    "            else:\n",
    "                # Add augmentation\n",
    "                aug = torch.zeros(x.shape[0], self.odefunc.augment_dim).to(self.device)\n",
    "                # Shape (batch_size, data_dim + augment_dim)\n",
    "                x_aug = torch.cat([x, aug], 1)\n",
    "        else:\n",
    "            x_aug = x\n",
    "\n",
    "        if self.adjoint:\n",
    "            out = odeint_adjoint(self.odefunc, x_aug, integration_time,\n",
    "                                 rtol=self.tol, atol=self.tol, method='euler',\n",
    "                                 options={'max_num_steps': MAX_NUM_STEPS})\n",
    "        else:\n",
    "            out = odeint(self.odefunc, x_aug, integration_time,\n",
    "                         rtol=self.tol, atol=self.tol, method='euler',\n",
    "                         options={'max_num_steps': MAX_NUM_STEPS})\n",
    "\n",
    "        if eval_times is None:\n",
    "            return out[1]  # Return only final time\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class ODENet(nn.Module):\n",
    "    \"\"\"An ODEBlock followed by a Linear layer.\n",
    "    Parameters\n",
    "    ----------\n",
    "    device : torch.device\n",
    "    data_dim : int\n",
    "        Dimension of data.\n",
    "    hidden_dim : int\n",
    "        Dimension of hidden layers.\n",
    "    output_dim : int\n",
    "        Dimension of output after hidden layer. Should be 1 for regression or\n",
    "        num_classes for classification.\n",
    "    augment_dim: int\n",
    "        Dimension of augmentation. If 0 does not augment ODE, otherwise augments\n",
    "        it with augment_dim dimensions.\n",
    "    time_dependent : bool\n",
    "        If True adds time as input, making ODE time dependent.\n",
    "    non_linearity : string\n",
    "        One of 'relu' and 'softplus'\n",
    "    tol : float\n",
    "        Error tolerance.\n",
    "    adjoint : bool\n",
    "        If True calculates gradient with adjoint method, otherwise\n",
    "        backpropagates directly through operations of ODE solver.\n",
    "    \"\"\"\n",
    "    def __init__(self, device, data_dim, hidden_dim, output_dim=1,\n",
    "                 augment_dim=0, time_dependent=False, non_linearity='relu',\n",
    "                 tol=1e-3, adjoint=False):\n",
    "        super(ODENet, self).__init__()\n",
    "        self.device = device\n",
    "        self.data_dim = data_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.augment_dim = augment_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.time_dependent = time_dependent\n",
    "        self.tol = tol\n",
    "\n",
    "        odefunc = ODEFunc(device, data_dim, hidden_dim, augment_dim,\n",
    "                          time_dependent, non_linearity)\n",
    "\n",
    "        self.odeblock = ODEBlock(device, odefunc, tol=tol, adjoint=adjoint)\n",
    "\n",
    "    def forward(self, x, eval_times=None):\n",
    "        features = self.odeblock(x, eval_times)\n",
    "        return features\n",
    "        \n",
    "class GRUExponentialDecay(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU RNN module where the hidden state decays exponentially\n",
    "    (see e.g. Che et al. 2018, Recurrent Neural Networks for Multivariate Time Series\n",
    "    with Missing Values).\n",
    "    \n",
    "    Args:\n",
    "    inputs: A `Tensor` with embeddings in the last dimension.\n",
    "    times: A `Tensor` with the same shape as inputs containing the recorded times (but no embedding dimension).\n",
    "    \n",
    "    Returns:\n",
    "    outs: Hidden states of the RNN.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super(GRUExponentialDecay, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru_cell = nn.GRUCell(input_size, hidden_size)\n",
    "        self.decays = nn.Parameter(torch.Tensor(hidden_size)) # exponential decays vector\n",
    "    \n",
    "    def forward(self, inputs, times):\n",
    "    # initializing and then calling cuda() later isn't working for some reason\n",
    "        if torch.cuda.is_available():\n",
    "          hn = torch.zeros(inputs.size(0), self.hidden_size).cuda() # batch_size x hidden_size\n",
    "          outs = torch.zeros(inputs.size(0), inputs.size(1), self.hidden_size).cuda() # batch_size x seq_len x hidden_size\n",
    "        else:\n",
    "          hn = torch.zeros(inputs.size(0), self.hidden_size) # batch_size x hidden_size\n",
    "          outs = torch.zeros(inputs.size(0), inputs.size(1), self.hidden_size) # batch_size x seq_len x hidden_size\n",
    "    \n",
    "    # this is slow\n",
    "        for seq in range(inputs.size(1)):\n",
    "          hn = self.gru_cell(inputs[:,seq,:], hn)\n",
    "          outs[:,seq,:] = hn\n",
    "          hn = hn*torch.exp(-torch.clamp(torch.unsqueeze(times[:,seq], dim=-1)*self.decays, min=0))\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class birnn_concat_time_delta_attention(nn.Module):\n",
    "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
    "      super(birnn_concat_time_delta_attention, self).__init__()\n",
    "      \n",
    "      # Embedding dimensions\n",
    "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))\n",
    "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))\n",
    "\n",
    "      # Embedding layers\n",
    "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
    "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
    "\n",
    "      # GRU layers\n",
    "      self.gru_dp_fw = nn.GRU(input_size=self.embed_dp_dim+1, hidden_size=self.embed_dp_dim+1, num_layers=1, batch_first=True)\n",
    "      self.gru_cp_fw = nn.GRU(input_size=self.embed_cp_dim+1, hidden_size=self.embed_cp_dim+1, num_layers=1, batch_first=True)\n",
    "      self.gru_dp_bw = nn.GRU(input_size=self.embed_dp_dim+1, hidden_size=self.embed_dp_dim+1, num_layers=1, batch_first=True)\n",
    "      self.gru_cp_bw = nn.GRU(input_size=self.embed_cp_dim+1, hidden_size=self.embed_cp_dim+1, num_layers=1, batch_first=True)\n",
    "\n",
    "      # Attention layers\n",
    "      self.attention_dp = Attention(embedding_dim=2*(self.embed_dp_dim+1)) #+1 for the concatenated time\n",
    "      self.attention_cp = Attention(embedding_dim=2*(self.embed_cp_dim+1))\n",
    "            \n",
    "      # Fully connected output\n",
    "      self.fc_dp  = nn.Linear(2*(self.embed_dp_dim+1), 1)\n",
    "      self.fc_cp  = nn.Linear(2*(self.embed_cp_dim+1), 1)\n",
    "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
    "      \n",
    "      # Others\n",
    "      self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
    "      # Compute time delta\n",
    "      ## output dim: batch_size x seq_len\n",
    "      dp_t_delta_fw = abs_time_to_delta(dp_t)\n",
    "      cp_t_delta_fw = abs_time_to_delta(cp_t)\n",
    "      dp_t_delta_bw = abs_time_to_delta(torch.flip(dp_t, [1]))\n",
    "      cp_t_delta_bw = abs_time_to_delta(torch.flip(cp_t, [1]))    \n",
    "    \n",
    "      # Embedding\n",
    "      ## output dim: batch_size x seq_len x embedding_dim\n",
    "      embedded_dp_fw = self.embed_dp(dp)\n",
    "      embedded_cp_fw = self.embed_cp(cp)\n",
    "      embedded_dp_bw = torch.flip(embedded_dp_fw, [1])\n",
    "      embedded_cp_bw = torch.flip(embedded_cp_fw, [1])\n",
    "      \n",
    "      # Concatate with time\n",
    "      ## output dim: batch_size x seq_len x (embedding_dim+1)\n",
    "      concat_dp_fw = torch.cat((embedded_dp_fw, torch.unsqueeze(dp_t_delta_fw, dim=-1)), dim=-1)\n",
    "      concat_cp_fw = torch.cat((embedded_cp_fw, torch.unsqueeze(cp_t_delta_fw, dim=-1)), dim=-1)\n",
    "      concat_dp_bw = torch.cat((embedded_dp_bw, torch.unsqueeze(dp_t_delta_bw, dim=-1)), dim=-1)\n",
    "      concat_cp_bw = torch.cat((embedded_cp_bw, torch.unsqueeze(cp_t_delta_bw, dim=-1)), dim=-1)\n",
    "      ## Dropout\n",
    "      concat_dp_fw = self.dropout(concat_dp_fw)\n",
    "      concat_cp_fw = self.dropout(concat_cp_fw)\n",
    "      concat_dp_bw = self.dropout(concat_dp_bw)\n",
    "      concat_cp_bw = self.dropout(concat_cp_bw)\n",
    "      \n",
    "      # GRU\n",
    "      ## output dim rnn:        batch_size x seq_len x (embedding_dim+1)\n",
    "      ## output dim rnn_hidden: batch_size x 1 x (embedding_dim+1)\n",
    "      rnn_dp_fw, rnn_hidden_dp_fw = self.gru_dp_fw(concat_dp_fw)\n",
    "      rnn_cp_fw, rnn_hidden_cp_fw = self.gru_cp_fw(concat_cp_fw)\n",
    "      rnn_dp_bw, rnn_hidden_dp_bw = self.gru_dp_bw(concat_dp_bw)\n",
    "      rnn_cp_bw, rnn_hidden_cp_bw = self.gru_cp_bw(concat_cp_bw)      \n",
    "      # concatenate forward and backward\n",
    "      ## output dim: batch_size x seq_len x 2*(embedding_dim+1)\n",
    "      rnn_dp = torch.cat((rnn_dp_fw, torch.flip(rnn_dp_bw, [1])), dim=-1)\n",
    "      rnn_cp = torch.cat((rnn_cp_fw, torch.flip(rnn_cp_bw, [1])), dim=-1)\n",
    "\n",
    "      # Attention\n",
    "      ## output dim: batch_size x 2*(embedding_dim+1)\n",
    "      attended_dp, weights_dp = self.attention_dp(rnn_dp, (dp > 0).float())\n",
    "      attended_cp, weights_cp = self.attention_cp(rnn_cp, (cp > 0).float())\n",
    "      \n",
    "      # Scores\n",
    "      score_dp = self.fc_dp(self.dropout(attended_dp))\n",
    "      score_cp = self.fc_cp(self.dropout(attended_cp))\n",
    "\n",
    "      # Concatenate to variable collection\n",
    "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
    "      \n",
    "      # Final linear projection\n",
    "      out = self.fc_all(self.dropout(all)).squeeze()\n",
    "\n",
    "      return out, []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class birnn_ode_decay(nn.Module):\n",
    "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
    "      super(birnn_ode_decay, self).__init__()\n",
    "      \n",
    "      # Embedding dimensions\n",
    "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))+1\n",
    "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))+1\n",
    "\n",
    "      # Embedding layers\n",
    "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
    "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
    "\n",
    "      # GRU layers\n",
    "      self.gru_dp_fw = GRUOdeDecay(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim)\n",
    "      self.gru_cp_fw = GRUOdeDecay(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim)\n",
    "      self.gru_dp_bw = GRUOdeDecay(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim)\n",
    "      self.gru_cp_bw = GRUOdeDecay(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim)\n",
    "      \n",
    "      # Fully connected output\n",
    "      self.fc_dp  = nn.Linear(2*self.embed_dp_dim, 1)\n",
    "      self.fc_cp  = nn.Linear(2*self.embed_cp_dim, 1)\n",
    "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
    "      \n",
    "      # Others\n",
    "      self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
    "      # Compute time delta\n",
    "      ## output dim: batch_size x seq_len\n",
    "      dp_t_delta_fw = abs_time_to_delta(dp_t)\n",
    "      cp_t_delta_fw = abs_time_to_delta(cp_t)\n",
    "      ## Round\n",
    "      dp_t_delta_fw = torch.round(100*dp_t_delta_fw)/100\n",
    "      cp_t_delta_fw = torch.round(100*cp_t_delta_fw)/100            \n",
    "      dp_t_delta_bw = abs_time_to_delta(torch.flip(dp_t, [1]))\n",
    "      cp_t_delta_bw = abs_time_to_delta(torch.flip(cp_t, [1]))    \n",
    "    \n",
    "      # Embedding\n",
    "      ## output dim: batch_size x seq_len x embedding_dim\n",
    "      embedded_dp_fw = self.embed_dp(dp)\n",
    "      embedded_cp_fw = self.embed_cp(cp)\n",
    "      embedded_dp_bw = torch.flip(embedded_dp_fw, [1])\n",
    "      embedded_cp_bw = torch.flip(embedded_cp_fw, [1])\n",
    "      ## Dropout\n",
    "      embedded_dp_fw = self.dropout(embedded_dp_fw)\n",
    "      embedded_cp_fw = self.dropout(embedded_cp_fw)\n",
    "      embedded_dp_bw = self.dropout(embedded_dp_bw)\n",
    "      embedded_cp_bw = self.dropout(embedded_cp_bw)\n",
    "      \n",
    "      # GRU\n",
    "      ## output dim rnn:        batch_size x seq_len x embedding_dim\n",
    "      rnn_dp_fw = self.gru_dp_fw(embedded_dp_fw, dp_t_delta_fw)\n",
    "      rnn_cp_fw = self.gru_cp_fw(embedded_cp_fw, cp_t_delta_fw)\n",
    "      rnn_dp_bw = self.gru_dp_bw(embedded_dp_bw, dp_t_delta_bw)\n",
    "      rnn_cp_bw = self.gru_cp_bw(embedded_cp_bw, cp_t_delta_bw)      \n",
    "      ## output dim rnn_hidden: batch_size x embedding_dim\n",
    "      rnn_dp_fw = rnn_dp_fw[:,-1,:]\n",
    "      rnn_cp_fw = rnn_cp_fw[:,-1,:]\n",
    "      rnn_dp_bw = rnn_dp_bw[:,-1,:]\n",
    "      rnn_cp_bw = rnn_cp_bw[:,-1,:]\n",
    "      ## concatenate forward and backward: batch_size x 2*embedding_dim\n",
    "      rnn_dp = torch.cat((rnn_dp_fw, rnn_dp_bw), dim=-1)\n",
    "      rnn_cp = torch.cat((rnn_cp_fw, rnn_cp_bw), dim=-1)\n",
    "      \n",
    "      # Scores\n",
    "      score_dp = self.fc_dp(self.dropout(rnn_dp))\n",
    "      score_cp = self.fc_cp(self.dropout(rnn_cp))\n",
    "\n",
    "      # Concatenate to variable collection\n",
    "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
    "      \n",
    "      # Final linear projection\n",
    "      out = self.fc_all(self.dropout(all)).squeeze()\n",
    "\n",
    "      return out, []      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class birnn_time_decay_attention(nn.Module):\n",
    "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
    "      super(birnn_time_decay_attention, self).__init__()\n",
    "      \n",
    "      # Embedding dimensions\n",
    "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))+1\n",
    "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))+1\n",
    "\n",
    "      # Embedding layers\n",
    "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
    "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
    "\n",
    "      # GRU layers\n",
    "      self.gru_dp_fw = GRUExponentialDecay(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim)\n",
    "      self.gru_cp_fw = GRUExponentialDecay(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim)\n",
    "      self.gru_dp_bw = GRUExponentialDecay(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim)\n",
    "      self.gru_cp_bw = GRUExponentialDecay(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim)\n",
    "\n",
    "      # Attention layers\n",
    "      self.attention_dp = Attention(embedding_dim=2*self.embed_dp_dim)\n",
    "      self.attention_cp = Attention(embedding_dim=2*self.embed_cp_dim)\n",
    "      \n",
    "      # Fully connected output\n",
    "      self.fc_dp  = nn.Linear(2*self.embed_dp_dim, 1)\n",
    "      self.fc_cp  = nn.Linear(2*self.embed_cp_dim, 1)\n",
    "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
    "      \n",
    "      # Others\n",
    "      self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
    "      # Compute time delta\n",
    "      ## output dim: batch_size x seq_len\n",
    "      dp_t_delta_fw = abs_time_to_delta(dp_t)\n",
    "      cp_t_delta_fw = abs_time_to_delta(cp_t)\n",
    "      dp_t_delta_bw = abs_time_to_delta(torch.flip(dp_t, [1]))\n",
    "      cp_t_delta_bw = abs_time_to_delta(torch.flip(cp_t, [1]))    \n",
    "    \n",
    "      # Embedding\n",
    "      ## output dim: batch_size x seq_len x embedding_dim\n",
    "      embedded_dp_fw = self.embed_dp(dp)\n",
    "      embedded_cp_fw = self.embed_cp(cp)\n",
    "      embedded_dp_bw = torch.flip(embedded_dp_fw, [1])\n",
    "      embedded_cp_bw = torch.flip(embedded_cp_fw, [1])\n",
    "      ## Dropout\n",
    "      embedded_dp_fw = self.dropout(embedded_dp_fw)\n",
    "      embedded_cp_fw = self.dropout(embedded_cp_fw)\n",
    "      embedded_dp_bw = self.dropout(embedded_dp_bw)\n",
    "      embedded_cp_bw = self.dropout(embedded_cp_bw)\n",
    "      \n",
    "      # GRU\n",
    "      ## output dim rnn:        batch_size x seq_len x embedding_dim\n",
    "      rnn_dp_fw = self.gru_dp_fw(embedded_dp_fw, dp_t_delta_fw)\n",
    "      rnn_cp_fw = self.gru_cp_fw(embedded_cp_fw, cp_t_delta_fw)\n",
    "      rnn_dp_bw = self.gru_dp_bw(embedded_dp_bw, dp_t_delta_bw)\n",
    "      rnn_cp_bw = self.gru_cp_bw(embedded_cp_bw, cp_t_delta_bw)\n",
    "      # concatenate forward and backward\n",
    "      ## output dim: batch_size x seq_len x 2*embedding_dim\n",
    "      rnn_dp = torch.cat((rnn_dp_fw, torch.flip(rnn_dp_bw, [1])), dim=-1)\n",
    "      rnn_cp = torch.cat((rnn_cp_fw, torch.flip(rnn_cp_bw, [1])), dim=-1)\n",
    "\n",
    "      # Attention\n",
    "      ## output dim: batch_size x 2*embedding_dim\n",
    "      attended_dp, weights_dp = self.attention_dp(rnn_dp, (dp > 0).float())\n",
    "      attended_cp, weights_cp = self.attention_cp(rnn_cp, (cp > 0).float())\n",
    "      \n",
    "      # Scores\n",
    "      score_dp = self.fc_dp(self.dropout(attended_dp))\n",
    "      score_cp = self.fc_cp(self.dropout(attended_cp))\n",
    "\n",
    "      # Concatenate to variable collection\n",
    "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
    "      \n",
    "      # Final linear projection\n",
    "      out = self.fc_all(self.dropout(all)).squeeze()\n",
    "\n",
    "      return out, []\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class birnn_time_decay(nn.Module):\n",
    "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
    "      super(birnn_time_decay, self).__init__()\n",
    "      \n",
    "      # Embedding dimensions\n",
    "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))+1\n",
    "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))+1\n",
    "\n",
    "      # Embedding layers\n",
    "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
    "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
    "\n",
    "      # GRU layers\n",
    "      self.gru_dp_fw = GRUExponentialDecay(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim)\n",
    "      self.gru_cp_fw = GRUExponentialDecay(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim)\n",
    "      self.gru_dp_bw = GRUExponentialDecay(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim)\n",
    "      self.gru_cp_bw = GRUExponentialDecay(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim)\n",
    "      \n",
    "      # Fully connected output\n",
    "      self.fc_dp  = nn.Linear(2*self.embed_dp_dim, 1)\n",
    "      self.fc_cp  = nn.Linear(2*self.embed_cp_dim, 1)\n",
    "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
    "      \n",
    "      # Others\n",
    "      self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
    "      # Compute time delta\n",
    "      ## output dim: batch_size x seq_len\n",
    "      dp_t_delta_fw = abs_time_to_delta(dp_t)\n",
    "      cp_t_delta_fw = abs_time_to_delta(cp_t)\n",
    "      dp_t_delta_bw = abs_time_to_delta(torch.flip(dp_t, [1]))\n",
    "      cp_t_delta_bw = abs_time_to_delta(torch.flip(cp_t, [1]))    \n",
    "    \n",
    "      # Embedding\n",
    "      ## output dim: batch_size x seq_len x embedding_dim\n",
    "      embedded_dp_fw = self.embed_dp(dp)\n",
    "      embedded_cp_fw = self.embed_cp(cp)\n",
    "      embedded_dp_bw = torch.flip(embedded_dp_fw, [1])\n",
    "      embedded_cp_bw = torch.flip(embedded_cp_fw, [1])\n",
    "      ## Dropout\n",
    "      embedded_dp_fw = self.dropout(embedded_dp_fw)\n",
    "      embedded_cp_fw = self.dropout(embedded_cp_fw)\n",
    "      embedded_dp_bw = self.dropout(embedded_dp_bw)\n",
    "      embedded_cp_bw = self.dropout(embedded_cp_bw)\n",
    "      \n",
    "      # GRU\n",
    "      ## output dim rnn:        batch_size x seq_len x embedding_dim\n",
    "      rnn_dp_fw = self.gru_dp_fw(embedded_dp_fw, dp_t_delta_fw)\n",
    "      rnn_cp_fw = self.gru_cp_fw(embedded_cp_fw, cp_t_delta_fw)\n",
    "      rnn_dp_bw = self.gru_dp_bw(embedded_dp_bw, dp_t_delta_bw)\n",
    "      rnn_cp_bw = self.gru_cp_bw(embedded_cp_bw, cp_t_delta_bw)      \n",
    "      ## output dim rnn_hidden: batch_size x embedding_dim\n",
    "      rnn_dp_fw = rnn_dp_fw[:,-1,:]\n",
    "      rnn_cp_fw = rnn_cp_fw[:,-1,:]\n",
    "      rnn_dp_bw = rnn_dp_bw[:,-1,:]\n",
    "      rnn_cp_bw = rnn_cp_bw[:,-1,:]\n",
    "      ## concatenate forward and backward: batch_size x 2*embedding_dim\n",
    "      rnn_dp = torch.cat((rnn_dp_fw, rnn_dp_bw), dim=-1)\n",
    "      rnn_cp = torch.cat((rnn_cp_fw, rnn_cp_bw), dim=-1)\n",
    "      \n",
    "      # Scores\n",
    "      score_dp = self.fc_dp(self.dropout(rnn_dp))\n",
    "      score_cp = self.fc_cp(self.dropout(rnn_cp))\n",
    "\n",
    "      # Concatenate to variable collection\n",
    "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
    "      \n",
    "      # Final linear projection\n",
    "      out = self.fc_all(self.dropout(all)).squeeze()\n",
    "\n",
    "      return out, []\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    " class ode_attention(nn.Module):\n",
    "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
    "      super(ode_attention, self).__init__()\n",
    "      \n",
    "      # Embedding dimensions\n",
    "      self.embed_dp_dim = int(2*np.ceil(num_dp_codes**0.25))+1\n",
    "      self.embed_cp_dim = int(2*np.ceil(num_cp_codes**0.25))+1\n",
    "\n",
    "      # Embedding layers\n",
    "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
    "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
    "      \n",
    "      # ODE layers\n",
    "      self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "      self.ode_dp = ODENet(self.device, self.embed_dp_dim, self.embed_dp_dim, output_dim=self.embed_dp_dim, augment_dim=0, time_dependent=False, non_linearity='softplus', tol=1e-3, adjoint=True)\n",
    "      self.ode_cp = ODENet(self.device, self.embed_cp_dim, self.embed_cp_dim, output_dim=self.embed_cp_dim, augment_dim=0, time_dependent=False, non_linearity='softplus', tol=1e-3, adjoint=True)\n",
    "      \n",
    "      # Attention layers\n",
    "      self.attention_dp = Attention(embedding_dim=self.embed_dp_dim)\n",
    "      self.attention_cp = Attention(embedding_dim=self.embed_cp_dim)\n",
    "      \n",
    "      # Fully connected output\n",
    "      self.fc_dp  = nn.Linear(self.embed_dp_dim, 1)\n",
    "      self.fc_cp  = nn.Linear(self.embed_cp_dim, 1)\n",
    "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
    "      \n",
    "      # Others\n",
    "      self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
    "      # Embedding\n",
    "      ## output dim: batch_size x seq_len x embedding_dim\n",
    "      embedded_dp = self.embed_dp(dp)\n",
    "      embedded_cp = self.embed_cp(cp)\n",
    "      \n",
    "      # ODE\n",
    "      ## Round times\n",
    "      dp_t = torch.round(100*dp_t)/100\n",
    "      cp_t = torch.round(100*cp_t)/100\n",
    "      \n",
    "      embedded_dp_long = embedded_dp.view(-1, self.embed_dp_dim)\n",
    "      dp_t_long = dp_t.view(-1)\n",
    "      dp_t_long_unique, inverse_indices = torch.unique(dp_t_long, sorted=True, return_inverse=True)\n",
    "      ode_dp_long = self.ode_dp(embedded_dp_long, dp_t_long_unique)\n",
    "      ode_dp_long = ode_dp_long[inverse_indices, torch.arange(0, inverse_indices.size(0)), :]\n",
    "      ode_dp = ode_dp_long.view(dp.size(0), dp.size(1), self.embed_dp_dim)\n",
    "\n",
    "      embedded_cp_long = embedded_cp.view(-1, self.embed_cp_dim)\n",
    "      cp_t_long = cp_t.view(-1)\n",
    "      cp_t_long_unique, inverse_indices = torch.unique(cp_t_long, sorted=True, return_inverse=True)\n",
    "      ode_cp_long = self.ode_cp(embedded_cp_long, cp_t_long_unique)\n",
    "      ode_cp_long = ode_cp_long[inverse_indices, torch.arange(0, inverse_indices.size(0)), :]\n",
    "      ode_cp = ode_cp_long.view(cp.size(0), cp.size(1), self.embed_cp_dim)\n",
    "\n",
    "      ## Dropout\n",
    "      ode_dp = self.dropout(ode_dp)\n",
    "      ode_cp = self.dropout(ode_cp)\n",
    "      \n",
    "      # Attention\n",
    "      ## output dim: batch_size x (embedding_dim+1)\n",
    "      attended_dp, weights_dp = self.attention_dp(ode_dp, (dp > 0).float())\n",
    "      attended_cp, weights_cp = self.attention_cp(ode_cp, (cp > 0).float())\n",
    "      \n",
    "      # Scores\n",
    "      score_dp = self.fc_dp(self.dropout(attended_dp))\n",
    "      score_cp = self.fc_cp(self.dropout(attended_cp))\n",
    "\n",
    "      # Concatenate to variable collection\n",
    "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
    "      \n",
    "      # Final linear projection\n",
    "      out = self.fc_all(self.dropout(all)).squeeze()\n",
    "\n",
    "      return out, []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    " class ode_birnn(nn.Module):\n",
    "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
    "      super(ode_birnn, self).__init__()\n",
    "      \n",
    "      # Embedding dimensions\n",
    "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))+1\n",
    "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))+1\n",
    "\n",
    "      # Embedding layers\n",
    "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
    "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
    "      \n",
    "      # ODE layers\n",
    "      self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "      self.ode_dp = ODENet(self.device, self.embed_dp_dim, self.embed_dp_dim, output_dim=self.embed_dp_dim, augment_dim=0, time_dependent=False, non_linearity='softplus', tol=1e-3, adjoint=True)\n",
    "      self.ode_cp = ODENet(self.device, self.embed_cp_dim, self.embed_cp_dim, output_dim=self.embed_cp_dim, augment_dim=0, time_dependent=False, non_linearity='softplus', tol=1e-3, adjoint=True)\n",
    "\n",
    "      # GRU layers\n",
    "      self.gru_dp_fw = nn.GRU(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim, num_layers=1, batch_first=True)\n",
    "      self.gru_cp_fw = nn.GRU(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim, num_layers=1, batch_first=True)\n",
    "      self.gru_dp_bw = nn.GRU(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim, num_layers=1, batch_first=True)\n",
    "      self.gru_cp_bw = nn.GRU(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim, num_layers=1, batch_first=True)\n",
    "      \n",
    "      # Fully connected output\n",
    "      self.fc_dp  = nn.Linear(2*self.embed_dp_dim, 1)\n",
    "      self.fc_cp  = nn.Linear(2*self.embed_cp_dim, 1)\n",
    "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
    "      \n",
    "      # Others\n",
    "      self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
    "      # Embedding\n",
    "      ## output dim: batch_size x seq_len x embedding_dim\n",
    "      embedded_dp = self.embed_dp(dp)\n",
    "      embedded_cp = self.embed_cp(cp)\n",
    "      \n",
    "      # ODE\n",
    "      ## Round times\n",
    "      dp_t = torch.round(100*dp_t)/100\n",
    "      cp_t = torch.round(100*cp_t)/100\n",
    "      \n",
    "      embedded_dp_long = embedded_dp.view(-1, self.embed_dp_dim)\n",
    "      dp_t_long = dp_t.view(-1)\n",
    "      dp_t_long_unique, inverse_indices = torch.unique(dp_t_long, sorted=True, return_inverse=True)\n",
    "      ode_dp_long = self.ode_dp(embedded_dp_long, dp_t_long_unique)\n",
    "      ode_dp_long = ode_dp_long[inverse_indices, torch.arange(0, inverse_indices.size(0)), :]\n",
    "      ode_dp = ode_dp_long.view(dp.size(0), dp.size(1), self.embed_dp_dim)\n",
    "\n",
    "      embedded_cp_long = embedded_cp.view(-1, self.embed_cp_dim)\n",
    "      cp_t_long = cp_t.view(-1)\n",
    "      cp_t_long_unique, inverse_indices = torch.unique(cp_t_long, sorted=True, return_inverse=True)\n",
    "      ode_cp_long = self.ode_cp(embedded_cp_long, cp_t_long_unique)\n",
    "      ode_cp_long = ode_cp_long[inverse_indices, torch.arange(0, inverse_indices.size(0)), :]\n",
    "      ode_cp = ode_cp_long.view(cp.size(0), cp.size(1), self.embed_cp_dim)\n",
    "      \n",
    "      ## Dropout\n",
    "      ode_dp = self.dropout(ode_dp)\n",
    "      ode_cp = self.dropout(ode_cp)\n",
    "\n",
    "      # Forward and backward sequences\n",
    "      ## output dim: batch_size x seq_len x embedding_dim\n",
    "      ode_dp_fw = ode_dp\n",
    "      ode_cp_fw = ode_cp\n",
    "      ode_dp_bw = torch.flip(ode_dp_fw, [1])\n",
    "      ode_cp_bw = torch.flip(ode_cp_fw, [1])\n",
    "      \n",
    "      # GRU\n",
    "      ## output dim rnn:        batch_size x seq_len x embedding_dim\n",
    "      ## output dim rnn_hidden: batch_size x 1 x embedding_dim\n",
    "      rnn_dp_fw, rnn_hidden_dp_fw = self.gru_dp_fw(ode_dp_fw)\n",
    "      rnn_cp_fw, rnn_hidden_cp_fw = self.gru_cp_fw(ode_cp_fw)\n",
    "      rnn_dp_bw, rnn_hidden_dp_bw = self.gru_dp_bw(ode_dp_bw)\n",
    "      rnn_cp_bw, rnn_hidden_cp_bw = self.gru_cp_bw(ode_cp_bw)      \n",
    "      ## output dim rnn_hidden: batch_size x embedding_dim\n",
    "      rnn_hidden_dp_fw = rnn_hidden_dp_fw.view(-1, self.embed_dp_dim)\n",
    "      rnn_hidden_cp_fw = rnn_hidden_cp_fw.view(-1, self.embed_cp_dim)\n",
    "      rnn_hidden_dp_bw = rnn_hidden_dp_bw.view(-1, self.embed_dp_dim)\n",
    "      rnn_hidden_cp_bw = rnn_hidden_cp_bw.view(-1, self.embed_cp_dim)\n",
    "      ## concatenate forward and backward: batch_size x 2*embedding_dim\n",
    "      rnn_hidden_dp = torch.cat((rnn_hidden_dp_fw, rnn_hidden_dp_bw), dim=-1)\n",
    "      rnn_hidden_cp = torch.cat((rnn_hidden_cp_fw, rnn_hidden_cp_bw), dim=-1)\n",
    "      \n",
    "      # Scores\n",
    "      score_dp = self.fc_dp(self.dropout(rnn_hidden_dp))\n",
    "      score_cp = self.fc_cp(self.dropout(rnn_hidden_cp))\n",
    "\n",
    "      # Concatenate to variable collection\n",
    "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
    "      \n",
    "      # Final linear projection\n",
    "      out = self.fc_all(self.dropout(all)).squeeze()\n",
    "\n",
    "      return out, []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    " class ode_birnn_attention(nn.Module):\n",
    "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
    "      super(ode_birnn_attention, self).__init__()\n",
    "      \n",
    "      # Embedding dimensions\n",
    "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))+1\n",
    "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))+1\n",
    "\n",
    "      # Embedding layers\n",
    "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
    "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
    "      \n",
    "      # ODE layers\n",
    "      self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "      self.ode_dp = ODENet(self.device, self.embed_dp_dim, self.embed_dp_dim, output_dim=self.embed_dp_dim, augment_dim=0, time_dependent=False, non_linearity='softplus', tol=1e-3, adjoint=True)\n",
    "      self.ode_cp = ODENet(self.device, self.embed_cp_dim, self.embed_cp_dim, output_dim=self.embed_cp_dim, augment_dim=0, time_dependent=False, non_linearity='softplus', tol=1e-3, adjoint=True)\n",
    "\n",
    "      # GRU layers\n",
    "      self.gru_dp_fw = nn.GRU(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim, num_layers=1, batch_first=True)\n",
    "      self.gru_cp_fw = nn.GRU(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim, num_layers=1, batch_first=True)\n",
    "      self.gru_dp_bw = nn.GRU(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim, num_layers=1, batch_first=True)\n",
    "      self.gru_cp_bw = nn.GRU(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim, num_layers=1, batch_first=True)\n",
    "\n",
    "      # Attention layers\n",
    "      self.attention_dp = Attention(embedding_dim=2*self.embed_dp_dim)\n",
    "      self.attention_cp = Attention(embedding_dim=2*self.embed_cp_dim)\n",
    "      \n",
    "      # Fully connected output\n",
    "      self.fc_dp  = nn.Linear(2*self.embed_dp_dim, 1)\n",
    "      self.fc_cp  = nn.Linear(2*self.embed_cp_dim, 1)\n",
    "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
    "      \n",
    "      # Others\n",
    "      self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
    "      # Embedding\n",
    "      ## output dim: batch_size x seq_len x embedding_dim\n",
    "      embedded_dp = self.embed_dp(dp)\n",
    "      embedded_cp = self.embed_cp(cp)\n",
    "      \n",
    "      # ODE\n",
    "      ## Round times\n",
    "      dp_t = torch.round(100*dp_t)/100\n",
    "      cp_t = torch.round(100*cp_t)/100\n",
    "      \n",
    "      embedded_dp_long = embedded_dp.view(-1, self.embed_dp_dim)\n",
    "      dp_t_long = dp_t.view(-1)\n",
    "      dp_t_long_unique, inverse_indices = torch.unique(dp_t_long, sorted=True, return_inverse=True)\n",
    "      ode_dp_long = self.ode_dp(embedded_dp_long, dp_t_long_unique)\n",
    "      ode_dp_long = ode_dp_long[inverse_indices, torch.arange(0, inverse_indices.size(0)), :]\n",
    "      ode_dp = ode_dp_long.view(dp.size(0), dp.size(1), self.embed_dp_dim)\n",
    "\n",
    "      embedded_cp_long = embedded_cp.view(-1, self.embed_cp_dim)\n",
    "      cp_t_long = cp_t.view(-1)\n",
    "      cp_t_long_unique, inverse_indices = torch.unique(cp_t_long, sorted=True, return_inverse=True)\n",
    "      ode_cp_long = self.ode_cp(embedded_cp_long, cp_t_long_unique)\n",
    "      ode_cp_long = ode_cp_long[inverse_indices, torch.arange(0, inverse_indices.size(0)), :]\n",
    "      ode_cp = ode_cp_long.view(cp.size(0), cp.size(1), self.embed_cp_dim)\n",
    "      \n",
    "      ## Dropout\n",
    "      ode_dp = self.dropout(ode_dp)\n",
    "      ode_cp = self.dropout(ode_cp)\n",
    "\n",
    "      # Forward and backward sequences\n",
    "      ## output dim: batch_size x seq_len x embedding_dim\n",
    "      ode_dp_fw = ode_dp\n",
    "      ode_cp_fw = ode_cp\n",
    "      ode_dp_bw = torch.flip(ode_dp_fw, [1])\n",
    "      ode_cp_bw = torch.flip(ode_cp_fw, [1])\n",
    "      \n",
    "      # GRU\n",
    "      ## output dim rnn:        batch_size x seq_len x embedding_dim\n",
    "      ## output dim rnn_hidden: batch_size x 1 x embedding_dim\n",
    "      rnn_dp_fw, rnn_hidden_dp_fw = self.gru_dp_fw(ode_dp_fw)\n",
    "      rnn_cp_fw, rnn_hidden_cp_fw = self.gru_cp_fw(ode_cp_fw)\n",
    "      rnn_dp_bw, rnn_hidden_dp_bw = self.gru_dp_bw(ode_dp_bw)\n",
    "      rnn_cp_bw, rnn_hidden_cp_bw = self.gru_cp_bw(ode_cp_bw)      \n",
    "      # concatenate forward and backward\n",
    "      ## output dim: batch_size x seq_len x 2*embedding_dim\n",
    "      rnn_dp = torch.cat((rnn_dp_fw, torch.flip(rnn_dp_bw, [1])), dim=-1)\n",
    "      rnn_cp = torch.cat((rnn_cp_fw, torch.flip(rnn_cp_bw, [1])), dim=-1)\n",
    "\n",
    "      # Attention\n",
    "      ## output dim: batch_size x 2*embedding_dim\n",
    "      attended_dp, weights_dp = self.attention_dp(rnn_dp, (dp > 0).float())\n",
    "      attended_cp, weights_cp = self.attention_cp(rnn_cp, (cp > 0).float())\n",
    "      \n",
    "      # Scores\n",
    "      score_dp = self.fc_dp(self.dropout(attended_dp))\n",
    "      score_cp = self.fc_cp(self.dropout(attended_cp))\n",
    "\n",
    "      # Concatenate to variable collection\n",
    "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
    "      \n",
    "      # Final linear projection\n",
    "      out = self.fc_all(self.dropout(all)).squeeze()\n",
    "\n",
    "      return out, []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class birnn_ode_decay_attention(nn.Module):\n",
    "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
    "      super(birnn_ode_decay_attention, self).__init__()\n",
    "      \n",
    "      # Embedding dimensions\n",
    "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))+1\n",
    "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))+1\n",
    "\n",
    "      # Embedding layers\n",
    "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
    "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
    "\n",
    "      # GRU layers\n",
    "      self.gru_dp_fw = GRUOdeDecay(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim)\n",
    "      self.gru_cp_fw = GRUOdeDecay(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim)\n",
    "      self.gru_dp_bw = GRUOdeDecay(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim)\n",
    "      self.gru_cp_bw = GRUOdeDecay(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim)\n",
    "\n",
    "      # Attention layers\n",
    "      self.attention_dp = Attention(embedding_dim=2*self.embed_dp_dim)\n",
    "      self.attention_cp = Attention(embedding_dim=2*self.embed_cp_dim)\n",
    "      \n",
    "      # Fully connected output\n",
    "      self.fc_dp  = nn.Linear(2*self.embed_dp_dim, 1)\n",
    "      self.fc_cp  = nn.Linear(2*self.embed_cp_dim, 1)\n",
    "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
    "      \n",
    "      # Others\n",
    "      self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
    "      # Compute time delta\n",
    "      ## output dim: batch_size x seq_len\n",
    "      dp_t_delta_fw = abs_time_to_delta(dp_t)\n",
    "      cp_t_delta_fw = abs_time_to_delta(cp_t)\n",
    "      ## Round\n",
    "      dp_t_delta_fw = torch.round(100*dp_t_delta_fw)/100\n",
    "      cp_t_delta_fw = torch.round(100*cp_t_delta_fw)/100      \n",
    "      dp_t_delta_bw = abs_time_to_delta(torch.flip(dp_t, [1]))\n",
    "      cp_t_delta_bw = abs_time_to_delta(torch.flip(cp_t, [1]))    \n",
    "    \n",
    "      # Embedding\n",
    "      ## output dim: batch_size x seq_len x embedding_dim\n",
    "      embedded_dp_fw = self.embed_dp(dp)\n",
    "      embedded_cp_fw = self.embed_cp(cp)\n",
    "      embedded_dp_bw = torch.flip(embedded_dp_fw, [1])\n",
    "      embedded_cp_bw = torch.flip(embedded_cp_fw, [1])\n",
    "      ## Dropout\n",
    "      embedded_dp_fw = self.dropout(embedded_dp_fw)\n",
    "      embedded_cp_fw = self.dropout(embedded_cp_fw)\n",
    "      embedded_dp_bw = self.dropout(embedded_dp_bw)\n",
    "      embedded_cp_bw = self.dropout(embedded_cp_bw)\n",
    "      \n",
    "      # GRU\n",
    "      ## output dim rnn:        batch_size x seq_len x embedding_dim\n",
    "      rnn_dp_fw = self.gru_dp_fw(embedded_dp_fw, dp_t_delta_fw)\n",
    "      rnn_cp_fw = self.gru_cp_fw(embedded_cp_fw, cp_t_delta_fw)\n",
    "      rnn_dp_bw = self.gru_dp_bw(embedded_dp_bw, dp_t_delta_bw)\n",
    "      rnn_cp_bw = self.gru_cp_bw(embedded_cp_bw, cp_t_delta_bw)\n",
    "      # concatenate forward and backward\n",
    "      ## output dim: batch_size x seq_len x 2*embedding_dim\n",
    "      rnn_dp = torch.cat((rnn_dp_fw, torch.flip(rnn_dp_bw, [1])), dim=-1)\n",
    "      rnn_cp = torch.cat((rnn_cp_fw, torch.flip(rnn_cp_bw, [1])), dim=-1)\n",
    "\n",
    "      # Attention\n",
    "      ## output dim: batch_size x 2*embedding_dim\n",
    "      attended_dp, weights_dp = self.attention_dp(rnn_dp, (dp > 0).float())\n",
    "      attended_cp, weights_cp = self.attention_cp(rnn_cp, (cp > 0).float())\n",
    "      \n",
    "      # Scores\n",
    "      score_dp = self.fc_dp(self.dropout(attended_dp))\n",
    "      score_cp = self.fc_cp(self.dropout(attended_cp))\n",
    "\n",
    "      # Concatenate to variable collection\n",
    "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
    "      \n",
    "      # Final linear projection\n",
    "      out = self.fc_all(self.dropout(all)).squeeze()\n",
    "\n",
    "      return out, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention_concat_time(nn.Module):\n",
    "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
    "      super(attention_concat_time, self).__init__()\n",
    "      \n",
    "      # Embedding dimensions\n",
    "      self.embed_dp_dim = int(2*np.ceil(num_dp_codes**0.25))\n",
    "      self.embed_cp_dim = int(2*np.ceil(num_cp_codes**0.25))\n",
    "\n",
    "      # Embedding layers\n",
    "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
    "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
    "      \n",
    "      # Attention layers\n",
    "      self.attention_dp = Attention(embedding_dim=self.embed_dp_dim+1) #+1 for the concatenated time\n",
    "      self.attention_cp = Attention(embedding_dim=self.embed_cp_dim+1)\n",
    "      \n",
    "      # Fully connected output\n",
    "      self.fc_dp  = nn.Linear(self.embed_dp_dim+1, 1)\n",
    "      self.fc_cp  = nn.Linear(self.embed_cp_dim+1, 1)\n",
    "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
    "      \n",
    "      # Others\n",
    "      self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
    "      # Embedding\n",
    "      ## output dim: batch_size x seq_len x embedding_dim\n",
    "      embedded_dp = self.embed_dp(dp)\n",
    "      embedded_cp = self.embed_cp(cp)\n",
    "      \n",
    "      # Concatate with time\n",
    "      ## output dim: batch_size x seq_len x (embedding_dim+1)\n",
    "      concat_dp = torch.cat((embedded_dp, torch.unsqueeze(dp_t, dim=-1)), dim=-1)\n",
    "      concat_cp = torch.cat((embedded_cp, torch.unsqueeze(cp_t, dim=-1)), dim=-1)\n",
    "      ## Dropout\n",
    "      concat_dp = self.dropout(concat_dp)\n",
    "      concat_cp = self.dropout(concat_cp)\n",
    "      \n",
    "      # Attention\n",
    "      ## output dim: batch_size x (embedding_dim+1)\n",
    "      attended_dp, weights_dp = self.attention_dp(concat_dp, (dp > 0).float())\n",
    "      attended_cp, weights_cp = self.attention_cp(concat_cp, (cp > 0).float())\n",
    "      \n",
    "      # Scores\n",
    "      score_dp = self.fc_dp(self.dropout(attended_dp))\n",
    "      score_cp = self.fc_cp(self.dropout(attended_cp))\n",
    "\n",
    "      # Concatenate to variable collection\n",
    "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
    "      \n",
    "      # Final linear projection\n",
    "      out = self.fc_all(self.dropout(all)).squeeze()\n",
    "\n",
    "      return out, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    " class mce_attention(nn.Module):\n",
    "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
    "      super(mce_attention, self).__init__()\n",
    "      \n",
    "      # Embedding dimensions\n",
    "      self.embed_dp_dim = int(2*np.ceil(num_dp_codes**0.25))\n",
    "      self.embed_cp_dim = int(2*np.ceil(num_cp_codes**0.25))\n",
    "\n",
    "      # Precomputed embedding weights\n",
    "      data_dir = current_dir + '/data/'\n",
    "      self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "      self.emb_weight_dp = torch.Tensor(np.load(data_dir + 'emb_weight_dp_13.npy')).to(self.device)\n",
    "      self.emb_weight_cp = torch.Tensor(np.load(data_dir + 'emb_weight_cp_11.npy')).to(self.device)\n",
    "      \n",
    "      # Attention layers\n",
    "      self.attention_dp = Attention(embedding_dim=self.embed_dp_dim+1) #+1 for the concatenated time\n",
    "      self.attention_cp = Attention(embedding_dim=self.embed_cp_dim+1)\n",
    "      \n",
    "      # Fully connected output\n",
    "      self.fc_dp  = nn.Linear(self.embed_dp_dim+1, 1)\n",
    "      self.fc_cp  = nn.Linear(self.embed_cp_dim+1, 1)\n",
    "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
    "      \n",
    "      # Others\n",
    "      self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
    "      # Embedding\n",
    "      ## output dim: batch_size x seq_len x embedding_dim\n",
    "      embedded_dp = F.embedding(dp, self.emb_weight_dp, padding_idx=0)\n",
    "      embedded_cp = F.embedding(cp, self.emb_weight_cp, padding_idx=0)\n",
    "      ## Dropout\n",
    "      embedded_dp = self.dropout(embedded_dp)\n",
    "      embedded_cp = self.dropout(embedded_cp)\n",
    "      \n",
    "      # Attention\n",
    "      ## output dim: batch_size x (embedding_dim+1)\n",
    "      attended_dp, weights_dp = self.attention_dp(embedded_dp, (dp > 0).float())\n",
    "      attended_cp, weights_cp = self.attention_cp(embedded_cp, (cp > 0).float())\n",
    "      \n",
    "      # Scores\n",
    "      score_dp = self.fc_dp(self.dropout(attended_dp))\n",
    "      score_cp = self.fc_cp(self.dropout(attended_cp))\n",
    "\n",
    "      # Concatenate to variable collection\n",
    "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
    "      \n",
    "      # Final linear projection\n",
    "      out = self.fc_all(self.dropout(all)).squeeze()\n",
    "\n",
    "      return out, []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    " class mce_birnn(nn.Module):\n",
    "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
    "      super(mce_birnn, self).__init__()\n",
    "      \n",
    "      # Embedding dimensions\n",
    "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))\n",
    "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))\n",
    "\n",
    "      # Precomputed embedding weights\n",
    "      self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "      self.emb_weight_dp = torch.Tensor(np.load(data_dir + 'emb_weight_dp_7.npy')).to(self.device)\n",
    "      self.emb_weight_cp = torch.Tensor(np.load(data_dir + 'emb_weight_cp_6.npy')).to(self.device)\n",
    "\n",
    "      # GRU layers\n",
    "      self.gru_dp_fw = nn.GRU(input_size=self.embed_dp_dim+1, hidden_size=self.embed_dp_dim+1, num_layers=1, batch_first=True)\n",
    "      self.gru_cp_fw = nn.GRU(input_size=self.embed_cp_dim+1, hidden_size=self.embed_cp_dim+1, num_layers=1, batch_first=True)\n",
    "      self.gru_dp_bw = nn.GRU(input_size=self.embed_dp_dim+1, hidden_size=self.embed_dp_dim+1, num_layers=1, batch_first=True)\n",
    "      self.gru_cp_bw = nn.GRU(input_size=self.embed_cp_dim+1, hidden_size=self.embed_cp_dim+1, num_layers=1, batch_first=True)\n",
    "      \n",
    "      # Fully connected output\n",
    "      self.fc_dp  = nn.Linear(2*(self.embed_dp_dim+1), 1)\n",
    "      self.fc_cp  = nn.Linear(2*(self.embed_cp_dim+1), 1)\n",
    "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
    "      \n",
    "      # Others\n",
    "      self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
    "      # Embedding\n",
    "      ## output dim: batch_size x seq_len x embedding_dim\n",
    "      embedded_dp_fw = F.embedding(dp, self.emb_weight_dp, padding_idx=0)\n",
    "      embedded_cp_fw = F.embedding(cp, self.emb_weight_cp, padding_idx=0)\n",
    "      embedded_dp_bw = torch.flip(embedded_dp_fw, [1])\n",
    "      embedded_cp_bw = torch.flip(embedded_cp_fw, [1])\n",
    "      ## Dropout\n",
    "      embedded_dp_fw = self.dropout(embedded_dp_fw)\n",
    "      embedded_cp_fw = self.dropout(embedded_cp_fw)\n",
    "      embedded_dp_bw = self.dropout(embedded_dp_bw)\n",
    "      embedded_cp_bw = self.dropout(embedded_cp_bw)\n",
    "      \n",
    "      # GRU\n",
    "      ## output dim rnn:        batch_size x seq_len x (embedding_dim+1)\n",
    "      ## output dim rnn_hidden: batch_size x 1 x (embedding_dim+1)\n",
    "      rnn_dp_fw, rnn_hidden_dp_fw = self.gru_dp_fw(embedded_dp_fw)\n",
    "      rnn_cp_fw, rnn_hidden_cp_fw = self.gru_cp_fw(embedded_cp_fw)\n",
    "      rnn_dp_bw, rnn_hidden_dp_bw = self.gru_dp_bw(embedded_dp_bw)\n",
    "      rnn_cp_bw, rnn_hidden_cp_bw = self.gru_cp_bw(embedded_cp_bw)      \n",
    "      ## output dim rnn_hidden: batch_size x (embedding_dim+1)\n",
    "      rnn_hidden_dp_fw = rnn_hidden_dp_fw.view(-1, self.embed_dp_dim+1)\n",
    "      rnn_hidden_cp_fw = rnn_hidden_cp_fw.view(-1, self.embed_cp_dim+1)\n",
    "      rnn_hidden_dp_bw = rnn_hidden_dp_bw.view(-1, self.embed_dp_dim+1)\n",
    "      rnn_hidden_cp_bw = rnn_hidden_cp_bw.view(-1, self.embed_cp_dim+1)\n",
    "      ## concatenate forward and backward: batch_size x 2*(embedding_dim+1)\n",
    "      rnn_hidden_dp = torch.cat((rnn_hidden_dp_fw, rnn_hidden_dp_bw), dim=-1)\n",
    "      rnn_hidden_cp = torch.cat((rnn_hidden_cp_fw, rnn_hidden_cp_bw), dim=-1)\n",
    "      \n",
    "      # Scores\n",
    "      score_dp = self.fc_dp(self.dropout(rnn_hidden_dp))\n",
    "      score_cp = self.fc_cp(self.dropout(rnn_hidden_cp))\n",
    "\n",
    "      # Concatenate to variable collection\n",
    "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
    "      \n",
    "      # Final linear projection\n",
    "      out = self.fc_all(self.dropout(all)).squeeze()\n",
    "\n",
    "      return out, []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    " class mce_birnn_attention(nn.Module):\n",
    "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
    "      super(mce_birnn_attention, self).__init__()\n",
    "      \n",
    "      # Embedding dimensions\n",
    "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))\n",
    "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))\n",
    "\n",
    "      # Precomputed embedding weights\n",
    "      self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "      self.emb_weight_dp = torch.Tensor(np.load(data_dir + 'emb_weight_dp_7.npy')).to(self.device)\n",
    "      self.emb_weight_cp = torch.Tensor(np.load(data_dir + 'emb_weight_cp_6.npy')).to(self.device)\n",
    "\n",
    "      # GRU layers\n",
    "      self.gru_dp_fw = nn.GRU(input_size=self.embed_dp_dim+1, hidden_size=self.embed_dp_dim+1, num_layers=1, batch_first=True)\n",
    "      self.gru_cp_fw = nn.GRU(input_size=self.embed_cp_dim+1, hidden_size=self.embed_cp_dim+1, num_layers=1, batch_first=True)\n",
    "      self.gru_dp_bw = nn.GRU(input_size=self.embed_dp_dim+1, hidden_size=self.embed_dp_dim+1, num_layers=1, batch_first=True)\n",
    "      self.gru_cp_bw = nn.GRU(input_size=self.embed_cp_dim+1, hidden_size=self.embed_cp_dim+1, num_layers=1, batch_first=True)\n",
    "\n",
    "      # Attention layers\n",
    "      self.attention_dp = Attention(embedding_dim=2*(self.embed_dp_dim+1)) #+1 for the concatenated time\n",
    "      self.attention_cp = Attention(embedding_dim=2*(self.embed_cp_dim+1))\n",
    "            \n",
    "      # Fully connected output\n",
    "      self.fc_dp  = nn.Linear(2*(self.embed_dp_dim+1), 1)\n",
    "      self.fc_cp  = nn.Linear(2*(self.embed_cp_dim+1), 1)\n",
    "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
    "      \n",
    "      # Others\n",
    "      self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
    "      # Embedding\n",
    "      ## output dim: batch_size x seq_len x embedding_dim\n",
    "      embedded_dp_fw = F.embedding(dp, self.emb_weight_dp, padding_idx=0)\n",
    "      embedded_cp_fw = F.embedding(cp, self.emb_weight_cp, padding_idx=0)\n",
    "      embedded_dp_bw = torch.flip(embedded_dp_fw, [1])\n",
    "      embedded_cp_bw = torch.flip(embedded_cp_fw, [1])\n",
    "      ## Dropout\n",
    "      embedded_dp_fw = self.dropout(embedded_dp_fw)\n",
    "      embedded_cp_fw = self.dropout(embedded_cp_fw)\n",
    "      embedded_dp_bw = self.dropout(embedded_dp_bw)\n",
    "      embedded_cp_bw = self.dropout(embedded_cp_bw)\n",
    "      \n",
    "      # GRU\n",
    "      ## output dim rnn:        batch_size x seq_len x (embedding_dim+1)\n",
    "      ## output dim rnn_hidden: batch_size x 1 x (embedding_dim+1)\n",
    "      rnn_dp_fw, rnn_hidden_dp_fw = self.gru_dp_fw(embedded_dp_fw)\n",
    "      rnn_cp_fw, rnn_hidden_cp_fw = self.gru_cp_fw(embedded_cp_fw)\n",
    "      rnn_dp_bw, rnn_hidden_dp_bw = self.gru_dp_bw(embedded_dp_bw)\n",
    "      rnn_cp_bw, rnn_hidden_cp_bw = self.gru_cp_bw(embedded_cp_bw)   \n",
    "      # concatenate forward and backward\n",
    "      ## output dim: batch_size x seq_len x 2*(embedding_dim+1)\n",
    "      rnn_dp = torch.cat((rnn_dp_fw, torch.flip(rnn_dp_bw, [1])), dim=-1)\n",
    "      rnn_cp = torch.cat((rnn_cp_fw, torch.flip(rnn_cp_bw, [1])), dim=-1)\n",
    "\n",
    "      # Attention\n",
    "      ## output dim: batch_size x 2*(embedding_dim+1)\n",
    "      attended_dp, weights_dp = self.attention_dp(rnn_dp, (dp > 0).float())\n",
    "      attended_cp, weights_cp = self.attention_cp(rnn_cp, (cp > 0).float())\n",
    "      \n",
    "      # Scores\n",
    "      score_dp = self.fc_dp(self.dropout(attended_dp))\n",
    "      score_cp = self.fc_cp(self.dropout(attended_cp))\n",
    "\n",
    "      # Concatenate to variable collection\n",
    "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
    "      \n",
    "      # Final linear projection\n",
    "      out = self.fc_all(self.dropout(all)).squeeze()\n",
    "\n",
    "      return out, []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "def logistic_regression(bootstrap_samples, np_seed):\n",
    "    current_dir = os.getcwd()\n",
    "    data_dir = current_dir + '/data/'\n",
    "    print(data_dir)\n",
    "    print('Loading data...')\n",
    "    icu_pat = pd.read_pickle(data_dir + 'icu_pat_admit.pkl')\n",
    "    \n",
    "    print('Loading last vital signs measurements...')\n",
    "    \n",
    "    charts = pd.read_pickle(data_dir + 'charts_outputs_last_only.pkl')\n",
    "    charts = charts.drop(columns=['CHARTTIME'])\n",
    "    charts = pd.get_dummies(charts, columns = ['VALUECAT']).groupby('ICUSTAY_ID').sum()\n",
    "    charts.drop(columns=['VALUECAT_CHART_BP_n', 'VALUECAT_CHART_BT_n', 'VALUECAT_CHART_GC_n', 'VALUECAT_CHART_HR_n', 'VALUECAT_CHART_RR_n', 'VALUECAT_CHART_UO_n'], inplace=True) # drop reference columns\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Create array of static variables...')\n",
    "    \n",
    "    num_icu_stays = len(icu_pat['ICUSTAY_ID'])\n",
    "    \n",
    "    # static variables\n",
    "    print('Create static array...')\n",
    "    icu_pat = pd.get_dummies(icu_pat, columns = ['ADMISSION_LOCATION', 'INSURANCE', 'MARITAL_STATUS', 'ETHNICITY'])\n",
    "    icu_pat.drop(columns=['ADMISSION_LOCATION_Emergency Room Admit', 'INSURANCE_Medicare', 'MARITAL_STATUS_Married/Life Partner', 'ETHNICITY_White'], inplace=True) # drop reference columns\n",
    "    \n",
    "    # merge with last vital signs measurements\n",
    "    icu_pat = pd.merge(icu_pat, charts, how='left', on='ICUSTAY_ID').fillna(0)\n",
    "    \n",
    "    static_columns = icu_pat.columns.str.contains('AGE|GENDER_M|LOS|NUM_RECENT_ADMISSIONS|ADMISSION_LOCATION|INSURANCE|MARITAL_STATUS|ETHNICITY|PRE_ICU_LOS|ELECTIVE_SURGERY|VALUECAT')\n",
    "    static = icu_pat.loc[:, static_columns].values\n",
    "    static_vars = icu_pat.loc[:, static_columns].columns.values.tolist()\n",
    "    \n",
    "    # classification label\n",
    "    print('Create label array...')\n",
    "    label = icu_pat.loc[:, 'POSITIVE'].values\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Split data into train/validate/test...')\n",
    "    # Split patients to avoid data leaks\n",
    "    patients = icu_pat['SUBJECT_ID'].drop_duplicates()\n",
    "    train, validate, test = np.split(patients.sample(frac=1, random_state=123), [int(.9*len(patients)), int(.9*len(patients))])\n",
    "    train_ids = icu_pat['SUBJECT_ID'].isin(train).values\n",
    "    test_ids = icu_pat['SUBJECT_ID'].isin(test).values\n",
    "    \n",
    "    data_train = static[train_ids, :]\n",
    "    data_test = static[test_ids, :]\n",
    "    \n",
    "    label_train = label[train_ids]\n",
    "    label_test = label[test_ids]  \n",
    "    \n",
    "    # Patients in test data\n",
    "    test_ids_patients = pd.read_pickle(data_dir + 'test_ids_patients.pkl')\n",
    "    patients = test_ids_patients.drop_duplicates()\n",
    "    num_patients = patients.shape[0]\n",
    "    row_ids = pd.DataFrame({'ROW_IDX': test_ids_patients.index}, index=test_ids_patients)\n",
    "    \n",
    "    print('-----------------------------------------')  \n",
    "    \n",
    "    # Fit logistic regression model\n",
    "    print('Fit logistic regression model...')\n",
    "    regr = linear_model.LogisticRegression()\n",
    "    regr.fit(data_train, label_train)\n",
    "    \n",
    "    # BootstrappingFhp.\n",
    "    np.random.seed(np_seed)\n",
    "    avpre_vec = np.zeros(bootstrap_samples)\n",
    "    auroc_vec = np.zeros(bootstrap_samples)\n",
    "    f1_vec    = np.zeros(bootstrap_samples)\n",
    "    sensitivity_vec = np.zeros(bootstrap_samples)\n",
    "    specificity_vec = np.zeros(bootstrap_samples)\n",
    "    ppv_vec = np.zeros(bootstrap_samples)\n",
    "    npv_vec = np.zeros(bootstrap_samples)  \n",
    "    \n",
    "    for sample in range(bootstrap_samples):\n",
    "        print('Bootstrap sample {}'.format(sample))\n",
    "        \n",
    "        sample_patients = patients.sample(n=num_patients, replace=True)\n",
    "        idx = np.squeeze(row_ids.loc[sample_patients].values)\n",
    "        data_test_bs, label_test_bs = data_test[idx], label_test[idx]\n",
    "        \n",
    "        label_sigmoids = regr.predict_proba(data_test_bs)[:, 1]\n",
    "        \n",
    "        print('Evaluate...')\n",
    "        # Average precision\n",
    "        avpre = average_precision_score(label_test_bs, label_sigmoids)\n",
    "        \n",
    "        # Determine AUROC score\n",
    "        auroc = roc_auc_score(label_test_bs, label_sigmoids)\n",
    "        \n",
    "        # Sensitivity, specificity\n",
    "        fpr, tpr, thresholds = roc_curve(label_test_bs, label_sigmoids)\n",
    "        youden_idx = np.argmax(tpr - fpr)\n",
    "        sensitivity = tpr[youden_idx]\n",
    "        specificity = 1-fpr[youden_idx]\n",
    "        \n",
    "        # F1, PPV, NPV score\n",
    "        f1 = 0\n",
    "        ppv = 0\n",
    "        npv = 0\n",
    "        for t in thresholds:\n",
    "            label_pred = (np.array(label_sigmoids) >= t).astype(int)\n",
    "            f1_temp = f1_score(label_test_bs, label_pred)\n",
    "            ppv_temp = precision_score(label_test_bs, label_pred, pos_label=1)\n",
    "            npv_temp = precision_score(label_test_bs, label_pred, pos_label=0)\n",
    "            if f1_temp > f1:\n",
    "                f1 = f1_temp\n",
    "            if (ppv_temp+npv_temp) > (ppv+npv):\n",
    "                ppv = ppv_temp\n",
    "                npv = npv_temp\n",
    "        \n",
    "    # Store in vectors\n",
    "    avpre_vec[sample] = avpre\n",
    "    auroc_vec[sample] = auroc\n",
    "    f1_vec[sample]    = f1\n",
    "    sensitivity_vec[sample]  = sensitivity\n",
    "    specificity_vec[sample]  = specificity\n",
    "    ppv_vec[sample]  = ppv\n",
    "    npv_vec[sample]  = npv\n",
    "    \n",
    "    avpre_mean = np.mean(avpre_vec)\n",
    "    avpre_lci, avpre_uci = st.t.interval(0.95, bootstrap_samples-1, loc=avpre_mean, scale=st.sem(avpre_vec))\n",
    "    auroc_mean = np.mean(auroc_vec)\n",
    "    auroc_lci, auroc_uci = st.t.interval(0.95, bootstrap_samples-1, loc=auroc_mean, scale=st.sem(auroc_vec))\n",
    "    f1_mean = np.mean(f1_vec)\n",
    "    f1_lci, f1_uci = st.t.interval(0.95, bootstrap_samples-1, loc=f1_mean, scale=st.sem(f1_vec))\n",
    "    ppv_mean = np.mean(ppv_vec)\n",
    "    ppv_lci, ppv_uci = st.t.interval(0.95, bootstrap_samples-1, loc=ppv_mean, scale=st.sem(ppv_vec))\n",
    "    npv_mean = np.mean(npv_vec)\n",
    "    npv_lci, npv_uci = st.t.interval(0.95,bootstrap_samples-1, loc=npv_mean, scale=st.sem(npv_vec))  \n",
    "    sensitivity_mean = np.mean(sensitivity_vec)\n",
    "    sensitivity_lci, sensitivity_uci = st.t.interval(0.95, bootstrap_samples-1, loc=sensitivity_mean, scale=st.sem(sensitivity_vec))\n",
    "    specificity_mean = np.mean(specificity_vec)\n",
    "    specificity_lci, specificity_uci = st.t.interval(0.95, bootstrap_samples-1, loc=specificity_mean, scale=st.sem(specificity_vec))\n",
    "    \n",
    "    # print('------------------------------------------------')\n",
    "    # print('Net variant: logistic regression')\n",
    "    # print('Average Precision: {} [{},{}]'.format(round(avpre_mean), round(avpre_lci), round(avpre_uci)))\n",
    "    # print('AUROC: {} [{},{}]'.format(round(auroc_mean), round(auroc_lci), round(auroc_uci)))\n",
    "    # print('F1: {} [{},{}]'.format(round(f1_mean), round(f1_lci), round(f1_uci)))  \n",
    "    # print('PPV: {} [{},{}]'.format(round(ppv_mean), round(ppv_lci), round(ppv_uci)))\n",
    "    # print('NPV: {} [{},{}]'.format(round(npv_mean), round(npv_lci), round(npv_uci)))\n",
    "    # print('Sensitivity: {} [{},{}]'.format(round(sensitivity_mean), round(sensitivity_lci), round(sensitivity_uci)))\n",
    "    # print('Specificity: {} [{},{}]'.format(round(specificity_mean), round(specificity_lci), round(specificity_uci)))\n",
    "    # print('Done')\n",
    "\n",
    "    results_dict = {}\n",
    "    # results_dict[\"Net variant\"]= \"logistic_regression\"\n",
    "    # results_dict[\"Average Precision\"]=str(round(avpre_mean)) + '  [' + str(round(avpre_lci)) + '  ' + str(round(avpre_uci)) +']'\n",
    "    # results_dict[\"AUROC\"]= str(round(auroc_mean)) + '  [' + str(round(auroc_lci)) + '  ' + str(round(auroc_uci))+']'\n",
    "    # results_dict[\"F1\"]= str(round(f1_mean)) + '  [' + str(round(f1_lci)) + '  ' + str(round(f1_uci))+']'\n",
    "    # results_dict[\"PPV\"]= str(round(ppv_mean)) + '  [' + str(round(ppv_lci)) + '  ' + str(round(ppv_uci))+']'\n",
    "    # results_dict[\"NPV\"]= str(round(npv_mean)) + '  [' +  str(round(npv_lci)) + '  ' +  str(round(npv_uci))+']'\n",
    "    # results_dict[\"Sensitivity\"]= str(round(sensitivity_mean)) + '  [' + str(round(sensitivity_lci)) + '  ' + str(round(sensitivity_uci))+']'\n",
    "    # results_dict[\"Specificity\"]= str(round(specificity_mean)) + '  [' + str(round(specificity_lci)) + '  ' + str(round(specificity_uci))+']'\n",
    "    # results_dict[\"Time\"]= \"\" \n",
    "    # # str(round(times_mean)) + '  ' + str(round(times_lci)) + '  [' + str(round(times_uci))+ '  ' + str(round(times_std))+']'\n",
    "\n",
    "    results_dict[\"Net variant\"]= \"logistic_regression\"\n",
    "    results_dict[\"Average Precision\"]=str(round(avpre_mean)) \n",
    "    results_dict[\"AUROC\"]= str(round(auroc_mean)) \n",
    "    results_dict[\"F1\"]= str(round(f1_mean)) \n",
    "    results_dict[\"PPV\"]= str(round(ppv_mean)) \n",
    "    results_dict[\"NPV\"]= str(round(npv_mean))\n",
    "    results_dict[\"Sensitivity\"]= str(round(sensitivity_mean)) \n",
    "    results_dict[\"Specificity\"]= str(round(specificity_mean)) \n",
    "    results_dict[\"Time\"]= \"\"\n",
    "    # str(round(times_mean)) \n",
    "\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzVUQS0CHry0"
   },
   "source": [
    "### Training Code\n",
    "\n",
    "Training each model uses the data_arrays.npz file generated from the preprocessing steps discussed earlier in this notebook.  This file is approximately 430MB, so it was too large to upload to GitHub.  Training each model for 80 epochs, with most taking approximately 6-8 hours (i.e 4.5-6 minutes/epoch) running from a terminal in a M2 16GB Macbook Pro while the MCE-based models took 1-1.5 hours (~1 minute/epoch).  Total training time took between 70 - 80 hours among the 14 models we trained.  The implementation in the section below largely adheres to the implementation from the orignal paper's repo as well.\n",
    "\n",
    "* train - function that trains the model passed and captures the results in the \"logdir\" folder\n",
    "* test  - function that tests the trained model and captures the performance metrics used for evaluation in the \"results\" folder.\n",
    "\n",
    "Training hyperparameters include the following:\n",
    "* Number of epochs used (80 by default)\n",
    "* Batch size when loading data (128 by default)\n",
    "* Dropout rate (0.5 by default)\n",
    "* Learning rate (0.001 by default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n"
     ]
    }
   ],
   "source": [
    "def num_static(data):\n",
    "  return data['static_vars'].shape[0]\n",
    "\n",
    "def vocab_sizes(data):\n",
    "  return data['dp'].max()+1, data['cp'].max()+1\n",
    "\n",
    "def abs_time_to_delta(times):\n",
    "  delta = torch.cat((torch.unsqueeze(times[:, 0], dim=-1), times[:, 1:] - times[:, :-1]), dim=1)\n",
    "  delta = torch.clamp(delta, min=0)\n",
    "  return delta\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "print('Load data...')\n",
    "current_dir = os.getcwd()\n",
    "data = np.load(current_dir + '/data/data_arrays.npz', allow_pickle=True)\n",
    "# Vocabulary sizes\n",
    "num_static = num_static(data)\n",
    "num_dp_codes, num_cp_codes = vocab_sizes(data)\n",
    "\n",
    "def train(net,model_name, num_epochs):\n",
    "    trainloader, num_batches, pos_weight = get_trainloader(data, 'TRAIN')\n",
    "\n",
    "     \n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    print('Start Train...' + model_name)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = optim.Adam(net.parameters(), lr = 0.001)  \n",
    "    \n",
    "    # Create log dir\n",
    "    logdir = current_dir + '/logdir/' + model_name + '/'\n",
    "    # logdir = hp.logdir + hp.net_variant + '/'\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "    \n",
    "    # Store times\n",
    "    epoch_times = []\n",
    "    \n",
    "    # Train\n",
    "    for epoch in tqdm(range(num_epochs)): \n",
    "    # print('-----------------------------------------')\n",
    "    # print('Epoch: {}'.format(epoch))\n",
    "        net.train()\n",
    "        time_start = time()\n",
    "        for i, (stat, dp, cp, dp_t, cp_t, label) in enumerate(tqdm(trainloader), 0):\n",
    "          # move to GPU if available\n",
    "          stat  = stat.to(device)\n",
    "          dp    = dp.to(device)\n",
    "          cp    = cp.to(device)\n",
    "          dp_t  = dp_t.to(device)\n",
    "          cp_t  = cp_t.to(device)\n",
    "          label = label.to(device)\n",
    "        \n",
    "          # zero the parameter gradients\n",
    "          optimizer.zero_grad()\n",
    "        \n",
    "          # forward + backward + optimize\n",
    "          label_pred, _ = net(stat, dp, cp, dp_t, cp_t)\n",
    "          loss = criterion(label_pred, label)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "    \n",
    "    # timing\n",
    "    time_end = time()\n",
    "    epoch_times.append(time_end-time_start)\n",
    "    \n",
    "    # Save\n",
    "    print('Saving...')\n",
    "    torch.save(net.state_dict(), logdir + 'final_model.pt')\n",
    "    np.savez(logdir + 'epoch_times', epoch_times=epoch_times)\n",
    "    print('Done Train for ' + model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzVUQS0CHry0"
   },
   "source": [
    "### Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_seed = 1234\n",
    "bootstrap_samples = 1\n",
    "\n",
    "def num_static1(data):\n",
    "  return data['static_vars'].shape[0]\n",
    "\n",
    "def vocab_sizes(data):\n",
    "  return data['dp'].max()+1, data['cp'].max()+1\n",
    "\n",
    "def roundval(val):\n",
    "    if math.isnan(val):\n",
    "        return\"nan\"\n",
    "    else:\n",
    "        return round(val)\n",
    "        \n",
    "def round(num):\n",
    "  return np.round(num*1000)/1000\n",
    "    \n",
    "\n",
    "def test(net, model_name):\n",
    "    # Load data\n",
    "    # print('Load data...')\n",
    "    # # data = np.load(data_dir + 'data_arrays.npz')\n",
    "    # data = np.load(data_dir + 'data_arrays.npz', allow_pickle=True)\n",
    "    # # \n",
    "    # test_ids_patients = pd.read_pickle(data_dir + 'test_ids_patients.pkl')\n",
    "\n",
    "    print(\"Test Evaluation called for \" + str(model_name))\n",
    "    data = np.load(current_dir + '/data/data_arrays.npz', allow_pickle=True)\n",
    "    test_ids_patients = pd.read_pickle(current_dir + '/data/test_ids_patients.pkl')\n",
    "    \n",
    "    # Patients in test data\n",
    "    patients = test_ids_patients.drop_duplicates()\n",
    "    num_patients = patients.shape[0]\n",
    "    row_ids = pd.DataFrame({'ROW_IDX': test_ids_patients.index}, index=test_ids_patients)\n",
    "    \n",
    "    # Vocabulary sizes\n",
    "    num_static = num_static1(data)\n",
    "    num_dp_codes, num_cp_codes = vocab_sizes(data)\n",
    "    \n",
    "    # CUDA for PyTorch\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # Network\n",
    "    # net = Net(num_static, num_dp_codes, num_cp_codes).to(device)\n",
    "    \n",
    "    # Set log dir to read trained model from\n",
    "    # logdir = logdir + model_name + '/'\n",
    "    logdir = current_dir + '/logdir/' + model_name + '/'\n",
    "    print(logdir)\n",
    "    \n",
    "    # Restore variables from disk\n",
    "    net.load_state_dict(torch.load(logdir + 'final_model.pt', map_location=device))\n",
    "    \n",
    "    # Bootstrapping\n",
    "    np.random.seed(np_seed)\n",
    "    avpre_vec = np.zeros(bootstrap_samples)\n",
    "    auroc_vec = np.zeros(bootstrap_samples)\n",
    "    f1_vec    = np.zeros(bootstrap_samples)\n",
    "    sensitivity_vec = np.zeros(bootstrap_samples)\n",
    "    specificity_vec = np.zeros(bootstrap_samples)\n",
    "    ppv_vec = np.zeros(bootstrap_samples)\n",
    "    npv_vec = np.zeros(bootstrap_samples)\n",
    "\n",
    "    for sample in range(bootstrap_samples):\n",
    "        print('Bootstrap sample {}'.format(sample))\n",
    "        \n",
    "        # Test data\n",
    "        sample_patients = patients.sample(n=num_patients, replace=True)\n",
    "        idx = np.squeeze(row_ids.loc[sample_patients].values)\n",
    "        testloader, _, _ = get_trainloader(data, 'TEST', shuffle=False, idx=idx)\n",
    "        \n",
    "        # evaluate on test data\n",
    "        net.eval()\n",
    "        label_pred = torch.Tensor([])\n",
    "        label_test = torch.Tensor([])\n",
    "        with torch.no_grad():\n",
    "            for i, (stat, dp, cp, dp_t, cp_t, label_batch) in enumerate(tqdm(testloader), 0):\n",
    "                # move to GPU if available\n",
    "                stat  = stat.to(device)\n",
    "                dp    = dp.to(device)\n",
    "                cp    = cp.to(device)\n",
    "                dp_t  = dp_t.to(device)\n",
    "                cp_t  = cp_t.to(device)\n",
    "        \n",
    "                label_pred_batch, _ = net(stat, dp, cp, dp_t, cp_t)\n",
    "                label_pred = torch.cat((label_pred, label_pred_batch.cpu()))\n",
    "                label_test = torch.cat((label_test, label_batch))\n",
    "\n",
    "        label_sigmoids = torch.sigmoid(label_pred).cpu().numpy()\n",
    "        \n",
    "        # Average precision\n",
    "        avpre = average_precision_score(label_test, label_sigmoids)\n",
    "        \n",
    "        # Determine AUROC score\n",
    "        auroc = roc_auc_score(label_test, label_sigmoids)\n",
    "        \n",
    "        # Sensitivity, specificity\n",
    "        fpr, tpr, thresholds = roc_curve(label_test, label_sigmoids)\n",
    "        youden_idx = np.argmax(tpr - fpr)\n",
    "        sensitivity = tpr[youden_idx]\n",
    "        specificity = 1-fpr[youden_idx]\n",
    "        \n",
    "        # F1, PPV, NPV score\n",
    "        f1 = 0\n",
    "        ppv = 0\n",
    "        npv = 0\n",
    "        for t in thresholds:\n",
    "            label_pred = (np.array(label_sigmoids) >= t).astype(int)\n",
    "            f1_temp = f1_score(label_test, label_pred)\n",
    "            ppv_temp = precision_score(label_test, label_pred, pos_label=1)\n",
    "            npv_temp = precision_score(label_test, label_pred, pos_label=0)\n",
    "            if f1_temp > f1:\n",
    "                f1 = f1_temp\n",
    "            if (ppv_temp+npv_temp) > (ppv+npv):\n",
    "                ppv = ppv_temp\n",
    "                npv = npv_temp\n",
    "\n",
    "        # print(f1)\n",
    "        # Store in vectors\n",
    "        avpre_vec[sample] = avpre\n",
    "        auroc_vec[sample] = auroc\n",
    "        f1_vec[sample]    = f1\n",
    "        sensitivity_vec[sample]  = sensitivity\n",
    "        specificity_vec[sample]  = specificity\n",
    "        ppv_vec[sample]  = ppv\n",
    "        npv_vec[sample]  = npv\n",
    "\n",
    "    #     print(avpre_vec)\n",
    "    # print('==')\n",
    "    # print(avpre_vec)\n",
    "    avpre_mean = np.mean(avpre_vec)\n",
    "    # print('avpre_mean')\n",
    "    # print(avpre_mean)\n",
    "    \n",
    "    avpre_lci, avpre_uci = st.t.interval(0.95, bootstrap_samples-1, loc=avpre_mean, scale=st.sem(avpre_vec))\n",
    "    auroc_mean = np.mean(auroc_vec)\n",
    "    auroc_lci, auroc_uci = st.t.interval(0.95, bootstrap_samples-1, loc=auroc_mean, scale=st.sem(auroc_vec))\n",
    "    f1_mean = np.mean(f1_vec)\n",
    "    f1_lci, f1_uci = st.t.interval(0.95, bootstrap_samples-1, loc=f1_mean, scale=st.sem(f1_vec))\n",
    "    ppv_mean = np.mean(ppv_vec)\n",
    "    ppv_lci, ppv_uci = st.t.interval(0.95, bootstrap_samples-1, loc=ppv_mean, scale=st.sem(ppv_vec))\n",
    "    npv_mean = np.mean(npv_vec)\n",
    "    npv_lci, npv_uci = st.t.interval(0.95, bootstrap_samples-1, loc=npv_mean, scale=st.sem(npv_vec))\n",
    "    sensitivity_mean = np.mean(sensitivity_vec)\n",
    "    sensitivity_lci, sensitivity_uci = st.t.interval(0.95, bootstrap_samples-1, loc=sensitivity_mean, scale=st.sem(sensitivity_vec))\n",
    "    specificity_mean = np.mean(specificity_vec)\n",
    "    specificity_lci, specificity_uci = st.t.interval(0.95, bootstrap_samples-1, loc=specificity_mean, scale=st.sem(specificity_vec))\n",
    "    \n",
    "    # epoch_times = np.load(logdir + net_variant + '/epoch_times.npz')['epoch_times']\n",
    "    epoch_times = np.load(logdir +  'epoch_times.npz')['epoch_times']\n",
    "    # net.load_state_dict(torch.load(logdir + 'final_model.pt', map_location=device))\n",
    "    times_mean = np.mean(epoch_times)\n",
    "    times_lci, times_uci = st.t.interval(0.95, len(epoch_times)-1, loc=np.mean(epoch_times), scale=st.sem(epoch_times))\n",
    "    times_std = np.std(epoch_times)\n",
    "\n",
    "    # print('------------------------------------------------')\n",
    "    # print('Net variant: {}'.format(model_name))\n",
    "    # print('Average Precision: {} [{},{}]'.format(round(avpre_mean), round(avpre_lci), round(avpre_uci)))\n",
    "    # print('AUROC: {} [{},{}]'.format(round(auroc_mean), round(auroc_lci), round(auroc_uci)))\n",
    "    # print('F1: {} [{},{}]'.format(round(f1_mean), round(f1_lci), round(f1_uci)))\n",
    "    # print('PPV: {} [{},{}]'.format(round(ppv_mean), round(ppv_lci), round(ppv_uci)))\n",
    "    # print('NPV: {} [{},{}]'.format(round(npv_mean), round(npv_lci), round(npv_uci)))\n",
    "    # print('Sensitivity: {} [{},{}]'.format(round(sensitivity_mean), round(sensitivity_lci), round(sensitivity_uci)))\n",
    "    # print('Specificity: {} [{},{}]'.format(round(specificity_mean), round(specificity_lci), round(specificity_uci)))\n",
    "    # print('Time: {} [{},{}] std: {}'.format(round(times_mean), round(times_lci), round(times_uci), round(times_std)))\n",
    "    # print('Done')\n",
    "\n",
    "    # results_file = current_dir + '/results/' + model_name + '_results.txt'\n",
    "    # with open(results_file, 'a') as f:\n",
    "    #     f.write('\\n')\n",
    "    #     f.write('\\nNet variant: {}'.format(model_name))\n",
    "    #     f.write('\\nAverage Precision: {} [{},{}]'.format(round(avpre_mean), round(avpre_lci), round(avpre_uci)))\n",
    "    #     f.write('\\nAUROC: {} [{},{}]'.format(round(auroc_mean), round(auroc_lci), round(auroc_uci)))\n",
    "    #     f.write('\\nF1: {} [{},{}]'.format(round(f1_mean), round(f1_lci), round(f1_uci)))\n",
    "    #     f.write('\\nPPV: {} [{},{}]'.format(round(ppv_mean), round(ppv_lci), round(ppv_uci)))\n",
    "    #     f.write('\\nNPV: {} [{},{}]'.format(round(npv_mean), round(npv_lci), round(npv_uci)))\n",
    "    #     f.write('\\nSensitivity: {} [{},{}]'.format(round(sensitivity_mean), round(sensitivity_lci), round(sensitivity_uci)))\n",
    "    #     f.write('\\nSpecificity: {} [{},{}]'.format(round(specificity_mean), round(specificity_lci), round(specificity_uci)))\n",
    "    #     f.write('\\nTime: {} [{},{}] std: {}'.format(round(times_mean), round(times_lci), round(times_uci), round(times_std)))\n",
    "    #     f.write('\\n Test Complete')\n",
    "\n",
    "    \n",
    "    results_dict = {}\n",
    "    # results_dict[\"Net variant\"]= model_name\n",
    "    # results_dict[\"Average Precision\"]=str(round(avpre_mean)) + '  [' + str(round(avpre_lci)) + '  ' + str(round(avpre_uci)) +']'\n",
    "    # results_dict[\"AUROC\"]= str(round(auroc_mean)) + '  [' + str(round(auroc_lci)) + '  ' + str(round(auroc_uci))+']'\n",
    "    # results_dict[\"F1\"]= str(round(f1_mean)) + '  [' + str(round(f1_lci)) + '  ' + str(round(f1_uci))+']'\n",
    "    # results_dict[\"PPV\"]= str(round(ppv_mean)) + '  [' + str(round(ppv_lci)) + '  ' + str(round(ppv_uci))+']'\n",
    "    # results_dict[\"NPV\"]= str(round(npv_mean)) + '  [' +  str(round(npv_lci)) + '  ' +  str(round(npv_uci))+']'\n",
    "    # results_dict[\"Sensitivity\"]= str(round(sensitivity_mean)) + '  [' + str(round(sensitivity_lci)) + '  ' + str(round(sensitivity_uci))+']'\n",
    "    # results_dict[\"Specificity\"]= str(round(specificity_mean)) + '  [' + str(round(specificity_lci)) + '  ' + str(round(specificity_uci))+']'\n",
    "    # results_dict[\"Time\"]= str(round(times_mean)) + '  ' + str(round(times_lci)) + '  [' + str(round(times_uci))+ '  ' + str(round(times_std))+']'\n",
    "\n",
    "\n",
    "    results_dict[\"Net variant\"]= model_name\n",
    "    results_dict[\"Average Precision\"]=str(round(avpre_mean)) \n",
    "    results_dict[\"AUROC\"]= str(round(auroc_mean)) \n",
    "    results_dict[\"F1\"]= str(round(f1_mean)) \n",
    "    results_dict[\"PPV\"]= str(round(ppv_mean)) \n",
    "    results_dict[\"NPV\"]= str(round(npv_mean))\n",
    "    results_dict[\"Sensitivity\"]= str(round(sensitivity_mean)) \n",
    "    results_dict[\"Specificity\"]= str(round(specificity_mean)) \n",
    "    results_dict[\"Time\"]= str(round(times_mean)) \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 80\n",
    "\n",
    "# model=birnn_concat_time_delta_attention(num_static, num_dp_codes, num_cp_codes)\n",
    "# train(model, 'birnn_concat_time_delta',num_epochs)\n",
    "\n",
    "# model=birnn_ode_decay(num_static, num_dp_codes, num_cp_codes)\n",
    "# train(model, 'birnn_ode_decay',num_epochs)\n",
    "\n",
    "# model=birnn_time_decay_attention(num_static, num_dp_codes, num_cp_codes)\n",
    "# train(model, 'birnn_time_decay_attention',num_epochs)\n",
    "\n",
    "# model=ode_attention(num_static, num_dp_codes, num_cp_codes)\n",
    "# train(model, 'ode_attention',num_epochs)\n",
    "\n",
    "# model=ode_birnn_attention(num_static, num_dp_codes, num_cp_codes)\n",
    "# train(model, 'ode_birnn_attention',num_epochs)\n",
    "\n",
    "# model=ode_birnn(num_static, num_dp_codes, num_cp_codes)\n",
    "# train(model, 'ode_birnn',num_epochs)\n",
    "\n",
    "# model=attention_concat_time(num_static, num_dp_codes, num_cp_codes)\n",
    "# train(model, 'attention_concat_time',num_epochs)\n",
    "\n",
    "# model=birnn_ode_decay_attention(num_static, num_dp_codes, num_cp_codes)\n",
    "# train(model, 'birnn_ode_decay_attention',num_epochs)\n",
    "\n",
    "# model=mce_attention(num_static, num_dp_codes, num_cp_codes)\n",
    "# train(model, 'mce_attention',num_epochs)\n",
    "\n",
    "# model=mce_birnn(num_static, num_dp_codes, num_cp_codes)\n",
    "# train(model, 'mce_birnn',num_epochs)\n",
    "\n",
    "# model=mce_birnn_attention(num_static, num_dp_codes, num_cp_codes)\n",
    "# train(model, 'mce_birnn_attention',num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX6bCcZNuxmz"
   },
   "source": [
    "# Results\n",
    "\n",
    "According to our paper, the paper's results indicate that the deep learning architectures, particularly those with a recurrent component, outperformed the logistic regression baseline model in predicting ICU readmissions.\n",
    "\n",
    "Baseline Characteristics: The analysis involved 23 static variables, 992 unique ICD-9 diagnosis codes, 298 unique ICD-9 procedure codes, 586 unique medications, and 32 codes related to vital signs. Each patient's electronic medical record (EMR) contained a maximum of 552 ICD-9 diagnosis and procedure codes and 392 medications and vital sign codes associated with the current ICU stay.\n",
    "\n",
    "Performance Trends: Models with a recurrent component generally performed better (average precision range: 0.298–0.331) than those based solely on attention layers (average precision range: 0.269–0.294). This suggests that incorporating recurrent neural network (RNN) components improved the predictive power for ICU readmissions.\n",
    "\n",
    "Therefore, the deep learning architectures, especially those combining recurrent components, showed significantly improved predictive accuracy compared to traditional logistic regression models when predicting ICU readmissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the tables of our results from each model we tested.\n",
    "1. <b>Results of model from the paper</b>: These are the base results from the paper itself.<ul>display_originalpaper_results()\n",
    "    </ul>\n",
    "   \n",
    "2. <b>Results of our training model</b> : These are the results based on running our reproduced trained models for bootstrap_samples = 100 similar to the paper.<ul>display_reproduced_results()</ul>  \n",
    "   \n",
    "3. <b>Results for quick evaluation and validation</b> : Knowing that the full run takes time, we have provided an alternate version, with bootstrap_samples =1 that can be run if needed as part of the evaluation.<ul>eval_shorter_run()<br>display_shorter_run_results()</ul> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_shorter_run():\n",
    "    current_dir = os.getcwd()\n",
    "    all_results={}\n",
    "    \n",
    "    model=ode_birnn_attention(num_static, num_dp_codes, num_cp_codes)\n",
    "    all_results[\"ode_birnn_attention\"]=test(model, 'ode_birnn_attention')\n",
    "    \n",
    "    model=ode_birnn(num_static, num_dp_codes, num_cp_codes)\n",
    "    all_results[\"ode_birnn\"]=test(model, 'ode_birnn')\n",
    "    \n",
    "    model=birnn_ode_decay_attention(num_static, num_dp_codes, num_cp_codes)\n",
    "    all_results[\"birnn_ode_decay_attention\"]=test(model, 'birnn_ode_decay_attention')\n",
    "    \n",
    "    model=birnn_ode_decay(num_static, num_dp_codes, num_cp_codes)\n",
    "    all_results[\"birnn_ode_decay\"]=test(model, 'birnn_ode_decay')\n",
    "    \n",
    "    model=birnn_time_decay_attention(num_static, num_dp_codes, num_cp_codes)\n",
    "    all_results[\"birnn_time_decay_attention\"]=test(model, 'birnn_time_decay_attention')\n",
    "    \n",
    "    model=birnn_time_decay(num_static, num_dp_codes, num_cp_codes)\n",
    "    all_results[\"birnn_time_decay\"]=test(model, 'birnn_time_decay')\n",
    "    \n",
    "    model=birnn_concat_time_delta_attention(num_static, num_dp_codes, num_cp_codes)\n",
    "    all_results[\"birnn_concat_time_delta_attention\"]=test(model, 'birnn_concat_time_delta_attention')\n",
    "    \n",
    "    model=birnn_concat_time_delta(num_static, num_dp_codes, num_cp_codes)\n",
    "    all_results[\"birnn_concat_time_delta\"]=test(model, 'birnn_concat_time_delta')\n",
    "    \n",
    "    model=ode_attention(num_static, num_dp_codes, num_cp_codes)\n",
    "    all_results[\"ode_attention\"]=test(model, 'ode_attention')\n",
    "    \n",
    "    model=attention_concat_time(num_static, num_dp_codes, num_cp_codes)\n",
    "    all_results[\"attention_concat_time\"]=test(model, 'attention_concat_time')\n",
    "    \n",
    "    model=mce_birnn_attention(num_static, num_dp_codes, num_cp_codes)\n",
    "    all_results[\"mce_birnn_attention\"]=test(model, 'mce_birnn_attention')\n",
    "    \n",
    "    model=mce_birnn(num_static, num_dp_codes, num_cp_codes)\n",
    "    all_results[\"mce_birnn\"]=test(model, 'mce_birnn')\n",
    "    \n",
    "    model=mce_attention(num_static, num_dp_codes, num_cp_codes)\n",
    "    all_results[\"mce_attention\"]=test(model, 'mce_attention')\n",
    "    \n",
    "    all_results[\"logistic_regression\"]=logistic_regression(1,1234)\n",
    "    \n",
    "    sr_file = current_dir + '/results/shortresults.pickle'\n",
    "    \n",
    "    file = open(sr_file, 'wb')\n",
    "    pickle.dump(all_results, file)\n",
    "    file.close()\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_shorter_run_results():\n",
    "    current_dir = os.getcwd()\n",
    "    shortresults_data = current_dir + '/results/shortresults.pickle'\n",
    "    \n",
    "    file = open(shortresults_data, 'rb')\n",
    "    all_results = pickle.load(file)\n",
    "    \n",
    "     \n",
    "    headers = [\"Net variant\", \"Average Precision\", \"AUROC\", \"F1\", \"Sensitivity\", \"Specificity\"]\n",
    "    \n",
    "    data = []\n",
    "    for result in all_results.values():\n",
    "        data.append([result[key] for key in headers])\n",
    "    \n",
    "    print(\"Results of our training model\")\n",
    "    print(tabulate(data, headers=headers,tablefmt='presto',numalign=\"center\"))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_originalpaper_results():\n",
    "    \n",
    "    results = {\n",
    "        \"ODE + RNN + Attention\": {\n",
    "            \"Net variant\": \"ODE + RNN + Attention\",\n",
    "            \"Average Precision\": \"0.314 \",\n",
    "            \"AUROC\": \"0.739 \",\n",
    "            \"F1\": \"0.376 \",\n",
    "            \"Sensitivity\": \"0.685 \",\n",
    "            \"Specificity\": \"0.677 \",\n",
    "        },\n",
    "    \n",
    "        \"ODE + RNN\": {\n",
    "            \"Net variant\": \"ODE + RNN\",\n",
    "            \"Average Precision\": \"0.331 \",\n",
    "            \"AUROC\": \"0.739 \",\n",
    "            \"F1\": \"0.372 \",\n",
    "            \"Sensitivity\": \"0.672\",\n",
    "            \"Specificity\": \"0.697 \",\n",
    "        },\n",
    "        \"RNN (ODE time decay) + Attention\": {\n",
    "            \"Net variant\": \"RNN (ODE time decay) + Attention\",\n",
    "            \"Average Precision\": \"0.316 \",\n",
    "            \"AUROC\": \"0.743\",\n",
    "            \"F1\": \"0.375 \",\n",
    "            \"Sensitivity\": \"0.648 \",\n",
    "            \"Specificity\": \"0.733 \",\n",
    "        },\n",
    "        \"RNN (ODE time decay)\": {\n",
    "            \"Net variant\": \"RNN (ODE time decay)\",\n",
    "            \"Average Precision\": \"0.300 \",\n",
    "            \"AUROC\": \"0.741 \",\n",
    "            \"F1\": \"0.372 \",\n",
    "            \"Sensitivity\": \"0.710 \",\n",
    "            \"Specificity\": \"0.667 \",\n",
    "        },\n",
    "        \"RNN (exp time decay) + Attention\": {\n",
    "            \"Net variant\": \"RNN (exp time decay) + Attention\",\n",
    "            \"Average Precision\": \"0.320 \",\n",
    "            \"AUROC\": \"0.748 \",\n",
    "            \"F1\": \"0.377  \",\n",
    "            \"Sensitivity\": \"0.704 \",\n",
    "            \"Specificity\": \"0.680 \",\n",
    "        },\n",
    "        \"RNN (exp time decay)\": {\n",
    "            \"Net variant\": \"RNN (exp time decay)\",\n",
    "            \"Average Precision\": \"0.304 \",\n",
    "            \"AUROC\": \"0.735\",\n",
    "            \"F1\": \"0.368 \",\n",
    "            \"Sensitivity\": \"0.707 \",\n",
    "            \"Specificity\": \"0.670 \",\n",
    "        },\n",
    "        \"RNN (concatenated Δtime) + Attention\": {\n",
    "            \"Net variant\": \"RNN (concatenated Δtime) + Attention\",\n",
    "            \"Average Precision\": \"0.312 \",\n",
    "            \"AUROC\": \"0.741 \",\n",
    "            \"F1\": \"0.368 \",\n",
    "            \"Sensitivity\": \"0.687 \",\n",
    "            \"Specificity\": \"0.688 \",\n",
    "        },\n",
    "        \"RNN (concatenated Δtime)\": {\n",
    "            \"Net variant\": \"RNN (concatenated Δtime)\",\n",
    "            \"Average Precision\": \"0.311 \",\n",
    "            \"AUROC\": \"0.739 \",\n",
    "            \"F1\": \"0.364 \",\n",
    "            \"Sensitivity\": \"0.698 \",\n",
    "            \"Specificity\": \"0.688 \",\n",
    "        },\n",
    "        \"ODE + Attention\": {\n",
    "            \"Net variant\": \"ODE + Attention\",\n",
    "            \"Average Precision\": \"0.294 \",\n",
    "            \"AUROC\": \"0.717 \",\n",
    "            \"F1\": \"0.333 \",\n",
    "            \"Sensitivity\": \"0.776 \",\n",
    "            \"Specificity\": \"0.554 \",\n",
    "        },\n",
    "        \"Attention (concatenated time)\": {\n",
    "            \"Net variant\": \"Attention (concatenated time)\",\n",
    "            \"Average Precision\": \"0.286 \",\n",
    "            \"AUROC\": \"0.711 \",\n",
    "            \"F1\": \"0.330 \",\n",
    "            \"Sensitivity\": \"0.700 \",\n",
    "            \"Specificity\": \"0.614 \",\n",
    "        },\n",
    "        \"MCE + RNN + Attention\": {\n",
    "            \"Net variant\": \"MCE + RNN + Attention\",\n",
    "            \"Average Precision\": \" 0.317 \",\n",
    "            \"AUROC\": \"0.736\",\n",
    "            \"F1\": \"0.373\",\n",
    "            \"Sensitivity\": \"0.630 \",\n",
    "            \"Specificity\": \"0.744\",\n",
    "        },\n",
    "        \"MCE + RNN\": {\n",
    "            \"Net variant\": \"MCE + RNN\",\n",
    "            \"Average Precision\": \"0.298\",\n",
    "            \"AUROC\": \"0.727 \",\n",
    "            \"F1\": \"0.361\",\n",
    "            \"Sensitivity\": \"0.654 \",\n",
    "            \"Specificity\": \"0.706 \",\n",
    "        },\n",
    "        \"MCE + Attention\": {\n",
    "            \"Net variant\": \"MCE + Attention\",\n",
    "            \"Average Precision\": \" 0.269 \",\n",
    "            \"AUROC\": \"0.689 \",\n",
    "            \"F1\": \"0.312 \",\n",
    "            \"Sensitivity\": \"0.686 \",\n",
    "            \"Specificity\": \"0.616 \",\n",
    "        },\n",
    "        \"Logistic Regression\": {\n",
    "            \"Net variant\": \"Logistic Regression\",\n",
    "            \"Average Precision\": \"0.257 \",\n",
    "            \"AUROC\": \"0.659 \",\n",
    "            \"F1\": \"0.296 \",\n",
    "            \"Sensitivity\": \"0.606 \",\n",
    "            \"Specificity\": \"0.647 \",\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    headers = [\"Net variant\", \"Average Precision\", \"AUROC\", \"F1\", \"Sensitivity\", \"Specificity\"]\n",
    "    \n",
    "    data = []\n",
    "    for result in results.values():\n",
    "        data.append([result[key] for key in headers])\n",
    "    \n",
    "    print(\"Results of model from the paper\")\n",
    "    print(tabulate(data, headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_reproduced_results():\n",
    "    current_dir = os.getcwd()\n",
    "    all_results={}\n",
    "    \n",
    "    model_results = current_dir + '/results/ode_birnn_attention.pickle'\n",
    "    file = open(model_results, 'rb')\n",
    "    result = pickle.load(file)\n",
    "    all_results[\"ode_birnn_attention\"]=result\n",
    "    file.close\n",
    "    \n",
    "    model_results = current_dir + '/results/ode_birnn.pickle'\n",
    "    file = open(model_results, 'rb')\n",
    "    result = pickle.load(file)\n",
    "    all_results[\"ode_birnn\"]=result\n",
    "    file.close\n",
    "    \n",
    "    \n",
    "    model_results = current_dir + '/results/birnn_ode_decay_attention.pickle'\n",
    "    file = open(model_results, 'rb')\n",
    "    result = pickle.load(file)\n",
    "    all_results[\"birnn_ode_decay_attention\"]=result\n",
    "    file.close\n",
    "    \n",
    "    \n",
    "    model_results = current_dir + '/results/birnn_ode_decay.pickle'\n",
    "    file = open(model_results, 'rb')\n",
    "    result = pickle.load(file)\n",
    "    all_results[\"birnn_ode_decay\"]=result\n",
    "    file.close\n",
    "    \n",
    "    model_results = current_dir + '/results/birnn_time_decay_attention.pickle'\n",
    "    file = open(model_results, 'rb')\n",
    "    result = pickle.load(file)\n",
    "    all_results[\"birnn_time_decay_attention\"]=result\n",
    "    file.close\n",
    "    \n",
    "    model_results = current_dir + '/results/birnn_time_decay.pickle'\n",
    "    file = open(model_results, 'rb')\n",
    "    result = pickle.load(file)\n",
    "    all_results[\"birnn_time_decay\"]=result\n",
    "    file.close\n",
    "    \n",
    "    model_results = current_dir + '/results/birnn_concat_time_delta_attention.pickle'\n",
    "    file = open(model_results, 'rb')\n",
    "    result = pickle.load(file)\n",
    "    all_results[\"birnn_concat_time_delta_attention\"]=result\n",
    "    file.close\n",
    "    \n",
    "    \n",
    "    model_results = current_dir + '/results/birnn_concat_time_delta.pickle'\n",
    "    file = open(model_results, 'rb')\n",
    "    result = pickle.load(file)\n",
    "    all_results[\"birnn_concat_time_delta\"]=result\n",
    "    file.close\n",
    "    \n",
    "    \n",
    "    model_results = current_dir + '/results/ode_attention.pickle'\n",
    "    file = open(model_results, 'rb')\n",
    "    result = pickle.load(file)\n",
    "    all_results[\"ode_attention\"]=result\n",
    "    file.close\n",
    "    \n",
    "    \n",
    "    model_results = current_dir + '/results/attention_concat_time.pickle'\n",
    "    file = open(model_results, 'rb')\n",
    "    result = pickle.load(file)\n",
    "    all_results[\"attention_concat_time\"]=result\n",
    "    file.close\n",
    "    \n",
    "    \n",
    "    \n",
    "    model_results = current_dir + '/results/mce_birnn_attention.pickle'\n",
    "    file = open(model_results, 'rb')\n",
    "    result = pickle.load(file)\n",
    "    all_results[\"mce_birnn_attention\"]=result\n",
    "    file.close\n",
    "    \n",
    "    model_results = current_dir + '/results/mce_birnn.pickle'\n",
    "    file = open(model_results, 'rb')\n",
    "    result = pickle.load(file)\n",
    "    all_results[\"mce_birnn\"]=result\n",
    "    file.close\n",
    "    \n",
    "    model_results = current_dir + '/results/mce_attention.pickle'\n",
    "    file = open(model_results, 'rb')\n",
    "    result = pickle.load(file)\n",
    "    all_results[\"mce_attention\"]=result\n",
    "    file.close\n",
    "    \n",
    "    model_results = current_dir + '/results/logistic_regression.pickle'\n",
    "    file = open(model_results, 'rb')\n",
    "    result = pickle.load(file)\n",
    "    all_results[\"logistic_regression\"]=result\n",
    "    file.close\n",
    "    \n",
    "    \n",
    "    headers = [\"Net variant\", \"Average Precision\", \"AUROC\", \"F1\",  \"Sensitivity\", \"Specificity\"]\n",
    "    \n",
    "    data = []\n",
    "    for result in all_results.values():\n",
    "        data.append([result[key] for key in headers])\n",
    "    \n",
    "    print(\"Results of our training model\")\n",
    "    print(tabulate(data, headers=headers,tablefmt='presto',numalign=\"center\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------------------\n",
      "Results of model from the paper\n",
      "Net variant                             Average Precision    AUROC     F1    Sensitivity    Specificity\n",
      "------------------------------------  -------------------  -------  -----  -------------  -------------\n",
      "ODE + RNN + Attention                               0.314    0.739  0.376          0.685          0.677\n",
      "ODE + RNN                                           0.331    0.739  0.372          0.672          0.697\n",
      "RNN (ODE time decay) + Attention                    0.316    0.743  0.375          0.648          0.733\n",
      "RNN (ODE time decay)                                0.3      0.741  0.372          0.71           0.667\n",
      "RNN (exp time decay) + Attention                    0.32     0.748  0.377          0.704          0.68\n",
      "RNN (exp time decay)                                0.304    0.735  0.368          0.707          0.67\n",
      "RNN (concatenated Δtime) + Attention                0.312    0.741  0.368          0.687          0.688\n",
      "RNN (concatenated Δtime)                            0.311    0.739  0.364          0.698          0.688\n",
      "ODE + Attention                                     0.294    0.717  0.333          0.776          0.554\n",
      "Attention (concatenated time)                       0.286    0.711  0.33           0.7            0.614\n",
      "MCE + RNN + Attention                               0.317    0.736  0.373          0.63           0.744\n",
      "MCE + RNN                                           0.298    0.727  0.361          0.654          0.706\n",
      "MCE + Attention                                     0.269    0.689  0.312          0.686          0.616\n",
      "Logistic Regression                                 0.257    0.659  0.296          0.606          0.647\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "Results of our training model\n",
      " Net variant                       |  Average Precision  |  AUROC  |  F1   |  Sensitivity  |  Specificity\n",
      "-----------------------------------+---------------------+---------+-------+---------------+---------------\n",
      " ode_birnn_attention               |        0.307        |  0.732  | 0.364 |     0.641     |     0.714\n",
      " ode_birnn                         |        0.329        |  0.741  | 0.383 |     0.648     |     0.738\n",
      " birnn_ode_decay_attention         |        0.319        |  0.742  | 0.377 |     0.69      |     0.688\n",
      " birnn_ode_decay                   |        0.307        |  0.74   | 0.373 |     0.661     |     0.71\n",
      " birnn_time_decay_attention        |        0.297        |  0.74   | 0.369 |     0.683     |     0.689\n",
      " birnn_time_decay                  |        0.304        |  0.735  | 0.365 |     0.713     |     0.672\n",
      " birnn_concat_time_delta_attention |        0.312        |  0.737  | 0.365 |     0.676     |     0.686\n",
      " birnn_concat_time_delta           |        0.316        |  0.738  | 0.374 |     0.681     |     0.696\n",
      " ode_attention                     |        0.282        |  0.709  | 0.326 |     0.714     |     0.618\n",
      " attention_concat_time             |        0.289        |  0.716  | 0.33  |     0.667     |     0.665\n",
      " mce_birnn_attention               |        0.308        |  0.733  | 0.362 |     0.713     |     0.655\n",
      " mce_birnn                         |        0.305        |  0.73   | 0.365 |     0.669     |     0.698\n",
      " mce_attention                     |        0.286        |  0.675  | 0.318 |     0.609     |     0.681\n",
      " Logistic Regression               |        0.257        |  0.661  | 0.296 |     0.603     |     0.648\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------------------------------------------------------------------------------------------\")\n",
    "display_originalpaper_results()\n",
    "print(\"-----------------------------------------------------------------------------------------------------------\")\n",
    "display_reproduced_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EAWAy_LwHlV"
   },
   "source": [
    "## Summary of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across all models, we obtained similar results to the paper for average precision, AUROC, and F1.  Sensitivity and specificity measures, however, tended to vary more from the paper's findings.  The results from our trained models reinforced the fundamental hypotheses of the paper, which were that all deep learning models performed better than the baseline logistic regression, and that models that used an RNN (birnn in the model names above) tended to perform most accurately.  The ablation study was implicit in the design of the original paper--combinations of deep learning approaches such as RNN, MCE, ODE, and attention were used to build separate models, and some models could be directly compared to others such as MCE + RNN vs. MCE + RNN + Attention.  Models with attention tended to perform better against their counterparts that did not use it, with the exception of the ODE + RNN models.\n",
    "\n",
    "In our replication attempts, we did not choose to pursue experiments beyond the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Evaluation called for ode_birnn_attention\n",
      "/Users/austinwang/Documents/Graduate School/Deep Learning For Healthcare (CS 598)/Project Code/DLH_Project/logdir/ode_birnn_attention/\n",
      "Bootstrap sample 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:10<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Evaluation called for ode_birnn\n",
      "/Users/austinwang/Documents/Graduate School/Deep Learning For Healthcare (CS 598)/Project Code/DLH_Project/logdir/ode_birnn/\n",
      "Bootstrap sample 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:09<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Evaluation called for birnn_ode_decay_attention\n",
      "/Users/austinwang/Documents/Graduate School/Deep Learning For Healthcare (CS 598)/Project Code/DLH_Project/logdir/birnn_ode_decay_attention/\n",
      "Bootstrap sample 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:06<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Evaluation called for birnn_ode_decay\n",
      "/Users/austinwang/Documents/Graduate School/Deep Learning For Healthcare (CS 598)/Project Code/DLH_Project/logdir/birnn_ode_decay/\n",
      "Bootstrap sample 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:05<00:00,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Evaluation called for birnn_time_decay_attention\n",
      "/Users/austinwang/Documents/Graduate School/Deep Learning For Healthcare (CS 598)/Project Code/DLH_Project/logdir/birnn_time_decay_attention/\n",
      "Bootstrap sample 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:04<00:00,  7.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Evaluation called for birnn_time_decay\n",
      "/Users/austinwang/Documents/Graduate School/Deep Learning For Healthcare (CS 598)/Project Code/DLH_Project/logdir/birnn_time_decay/\n",
      "Bootstrap sample 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:04<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Evaluation called for birnn_concat_time_delta_attention\n",
      "/Users/austinwang/Documents/Graduate School/Deep Learning For Healthcare (CS 598)/Project Code/DLH_Project/logdir/birnn_concat_time_delta_attention/\n",
      "Bootstrap sample 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:03<00:00, 11.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Evaluation called for birnn_concat_time_delta\n",
      "/Users/austinwang/Documents/Graduate School/Deep Learning For Healthcare (CS 598)/Project Code/DLH_Project/logdir/birnn_concat_time_delta/\n",
      "Bootstrap sample 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:02<00:00, 12.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Evaluation called for ode_attention\n",
      "/Users/austinwang/Documents/Graduate School/Deep Learning For Healthcare (CS 598)/Project Code/DLH_Project/logdir/ode_attention/\n",
      "Bootstrap sample 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:12<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Evaluation called for attention_concat_time\n",
      "/Users/austinwang/Documents/Graduate School/Deep Learning For Healthcare (CS 598)/Project Code/DLH_Project/logdir/attention_concat_time/\n",
      "Bootstrap sample 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:01<00:00, 29.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Evaluation called for mce_birnn_attention\n",
      "/Users/austinwang/Documents/Graduate School/Deep Learning For Healthcare (CS 598)/Project Code/DLH_Project/logdir/mce_birnn_attention/\n",
      "Bootstrap sample 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:03<00:00, 11.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Evaluation called for mce_birnn\n",
      "/Users/austinwang/Documents/Graduate School/Deep Learning For Healthcare (CS 598)/Project Code/DLH_Project/logdir/mce_birnn/\n",
      "Bootstrap sample 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:02<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Evaluation called for mce_attention\n",
      "/Users/austinwang/Documents/Graduate School/Deep Learning For Healthcare (CS 598)/Project Code/DLH_Project/logdir/mce_attention/\n",
      "Bootstrap sample 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:01<00:00, 30.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/austinwang/Documents/Graduate School/Deep Learning For Healthcare (CS 598)/Project Code/DLH_Project/data/\n",
      "Loading data...\n",
      "Loading last vital signs measurements...\n",
      "-----------------------------------------\n",
      "Create array of static variables...\n",
      "Create static array...\n",
      "Create label array...\n",
      "-----------------------------------------\n",
      "Split data into train/validate/test...\n",
      "-----------------------------------------\n",
      "Fit logistic regression model...\n",
      "Bootstrap sample 0\n",
      "Evaluate...\n"
     ]
    }
   ],
   "source": [
    "# This cell and the one below it can be run to see a shorter run evaluation with one bootstrap sample against our training models.\n",
    "eval_shorter_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of our training model\n",
      " Net variant                       |  Average Precision  |  AUROC  |  F1   |  Sensitivity  |  Specificity\n",
      "-----------------------------------+---------------------+---------+-------+---------------+---------------\n",
      " ode_birnn_attention               |        0.317        |  0.721  | 0.368 |     0.496     |     0.825\n",
      " ode_birnn                         |        0.33         |  0.72   | 0.388 |     0.58      |     0.776\n",
      " birnn_ode_decay_attention         |        0.323        |  0.724  | 0.38  |     0.489     |     0.844\n",
      " birnn_ode_decay                   |        0.317        |  0.724  | 0.375 |     0.587     |     0.757\n",
      " birnn_time_decay_attention        |        0.309        |  0.731  | 0.372 |     0.655     |     0.696\n",
      " birnn_time_decay                  |        0.314        |  0.72   | 0.368 |     0.674     |     0.69\n",
      " birnn_concat_time_delta_attention |        0.298        |  0.718  | 0.351 |     0.598     |     0.722\n",
      " birnn_concat_time_delta           |        0.318        |  0.718  | 0.369 |     0.637     |     0.698\n",
      " ode_attention                     |        0.294        |   0.7   | 0.33  |     0.653     |     0.652\n",
      " attention_concat_time             |        0.304        |  0.71   | 0.34  |     0.623     |     0.707\n",
      " mce_birnn_attention               |        0.318        |  0.722  | 0.357 |     0.735     |     0.62\n",
      " mce_birnn                         |        0.311        |  0.718  | 0.357 |     0.721     |     0.623\n",
      " mce_attention                     |        0.286        |  0.675  | 0.318 |     0.609     |     0.681\n",
      " logistic_regression               |        0.262        |  0.648  | 0.293 |     0.638     |     0.597\n"
     ]
    }
   ],
   "source": [
    "display_shorter_run_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH75TNU71eRH"
   },
   "source": [
    "# Discussion\n",
    "\n",
    "As mentioned in the summary of results section, precision, AUROC, and F1 of our trained models were close to those that were presented in the paper, while sensitivity and specificity measures varied. However, since the hypothesis of the paper revolved around the first three measures, we found the paper is reproducible and came to the same conclusions that the paper presented.<br>\n",
    "\n",
    "Overall, reproducing the results of the paper was generally easy, with the vast majority of code being usable as is and the underlying dataset (MIMIC-III) being publicly available. The main part that was difficult to reproduce was the sensitivity and specificity numbers.  There was not much detail on how these numbers were computed in the paper, and executing the code in the original GitHub repo simply gave us different results. In addition, it would have been more straightforward if the authors had included a requirements.txt file with their code, which would have provided benchmark library versions to make replication more predictable.  Mentioning the version of Python used in the original paper would have helped us debug and investigate any result inconsistencies. It was up to us to figure out how to run the preprocessing steps; had the authors provided the dependencies and order to run the preprocessing steps, that would have helped us get set up more quickly.  Lastly, training most models took 6-8 hours of time on a personal computer due to having only CPU available and not GPU.\n",
    "\n",
    "The main thing that would have helped us in reproducing the original paper's results is better code documentation and a README file that instructs the user on potential code dependencies.  While we don't expect the paper to cover such implementation details, some documentation on the formulas used and how to run the scripts would have reduced the chance for human error.  That is not to say that the code was undocumented; the models were well-documented and helped us understand how they were being set up.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHMI2chl9omn"
   },
   "source": [
    "# References\n",
    "1.  “Benchmarking Deep Learning Architectures for Predicting Readmission to the ICU and Describing Patients-at-Risk”, Sebastiano Barbieri, James Kemp, 2020\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "uiuc_kernel",
   "language": "python",
   "name": "uiuc_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
