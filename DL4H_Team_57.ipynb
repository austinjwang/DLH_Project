{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j01aH0PR4Sg-"
   },
   "source": [
    "# (TODO: Will delete this section before submitting draft) FAQ and Attentions\n",
    "* Copy and move this template to your Google Drive. Name your notebook by your team ID (upper-left corner). Don't eidt this original file.\n",
    "* This template covers most questions we want to ask about your reproduction experiment. You don't need to exactly follow the template, however, you should address the questions. Please feel free to customize your report accordingly.\n",
    "* any report must have run-able codes and necessary annotations (in text and code comments).\n",
    "* The notebook is like a demo and only uses small-size data (a subset of original data or processed data), the entire runtime of the notebook including data reading, data process, model training, printing, figure plotting, etc,\n",
    "must be within 8 min, otherwise, you may get penalty on the grade.\n",
    "  * If the raw dataset is too large to be loaded  you can select a subset of data and pre-process the data, then, upload the subset or processed data to Google Drive and load them in this notebook.\n",
    "  * If the whole training is too long to run, you can only set the number of training epoch to a small number, e.g., 3, just show that the training is runable.\n",
    "  * For results model validation, you can train the model outside this notebook in advance, then, load pretrained model and use it for validation (display the figures, print the metrics).\n",
    "* The post-process is important! For post-process of the results,please use plots/figures. The code to summarize results and plot figures may be tedious, however, it won't be waste of time since these figures can be used for presentation. While plotting in code, the figures should have titles or captions if necessary (e.g., title your figure with \"Figure 1. xxxx\")\n",
    "* There is not page limit to your notebook report, you can also use separate notebooks for the report, just make sure your grader can access and run/test them.\n",
    "* If you use outside resources, please refer them (in any formats). Include the links to the resources if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ0sNuMePBXx"
   },
   "source": [
    "# (TODO) Introduction\n",
    "This is an introduction to your report, you should edit this text/mardown section to compose. In this text/markdown, you should introduce:\n",
    "\n",
    "*   Background of the problem\n",
    "  * what type of problem: disease/readmission/mortality prediction,  feature engineeing, data processing, etc\n",
    "  * what is the importance/meaning of solving the problem\n",
    "  * what is the difficulty of the problem\n",
    "  * the state of the art methods and effectiveness.\n",
    "*   Paper explanation\n",
    "  * what did the paper propose\n",
    "  * what is the innovations of the method\n",
    "  * how well the proposed method work (in its own metrics)\n",
    "  * what is the contribution to the reasearch regime (referring the Background above, how important the paper is to the problem).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABD4VhFZbehA"
   },
   "outputs": [],
   "source": [
    "# code comment is used as inline annotations for your coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uygL9tTPSVHB"
   },
   "source": [
    "# (TODO) Scope of Reproducibility:\n",
    "\n",
    "List hypotheses from the paper you will test and the corresponding experiments you will run.\n",
    "\n",
    "\n",
    "1.   Hypothesis 1: xxxxxxx\n",
    "2.   Hypothesis 2: xxxxxxx\n",
    "\n",
    "You can insert images in this notebook text, [see this link](https://stackoverflow.com/questions/50670920/how-to-insert-an-inline-image-in-google-colaboratory-from-google-drive) and example below:\n",
    "\n",
    "![sample_image.png](https://drive.google.com/uc?export=view&id=1g2efvsRJDxTxKz-OY3loMhihrEUdBxbc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LM4WUjz64C3B"
   },
   "source": [
    "\n",
    "You can also use code to display images, see the code below.\n",
    "\n",
    "The images must be saved in Google Drive first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRksCB1vbYwJ"
   },
   "outputs": [],
   "source": [
    "# no code is required for this section\n",
    "'''\n",
    "# if you want to use an image outside this notebook for explanaition,\n",
    "# you can upload it to your google drive and show it with OpenCV or matplotlib\n",
    "# '''\n",
    "# # mount this notebook to your google drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# # define dirs to workspace and data\n",
    "# img_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-your-image>'\n",
    "\n",
    "# import cv2\n",
    "# img = cv2.imread(img_dir)\n",
    "# cv2.imshow(\"Title\", img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWAHJ_1CdtaA"
   },
   "source": [
    "# (TODO) Methodology\n",
    "\n",
    "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
    "\n",
    "The methodology at least contains two subsections **data** and **model** in your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yu61Jp1xrnKk"
   },
   "outputs": [],
   "source": [
    "# import required packages\n",
    "# from hyperparameters import Hyperparameters as hp # Need to figure out how to represent hyperparameters in this notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchdiffeq import odeint, odeint_adjoint\n",
    "# from data_load import * # Need to figure out how to represent these functions in this notebook\n",
    "# from modules import * # Need to figure out how to represent these functions in this notebook\n",
    "# from modules_ode import * # Need to figure out how to represent these functions in this notebook\n",
    "import os\n",
    "from time import time\n",
    "import scipy.stats as st\n",
    "# from train import Net # Need to figure out how to represent this class in this notebook\n",
    "from sklearn.metrics import *\n",
    "import torch.utils.data as utils\n",
    "\n",
    "from torch.utils.data.dataset import (\n",
    "    ChainDataset,\n",
    "    ConcatDataset,\n",
    "    Dataset,\n",
    "    IterableDataset,\n",
    "    StackDataset,\n",
    "    Subset,\n",
    "    TensorDataset,\n",
    "    random_split,\n",
    ")\n",
    "\n",
    "from torch.utils.data.dataloader import (\n",
    "    DataLoader,\n",
    "    _DatasetKind,\n",
    "    get_worker_info,\n",
    "    default_collate,\n",
    "    default_convert,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NbPHUTMbkD3"
   },
   "source": [
    "##  (TODO) Data\n",
    "Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).\n",
    "  * Source of the data: where the data is collected from; if data is synthetic or self-generated, explain how. If possible, please provide a link to the raw datasets.\n",
    "  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n",
    "  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.\n",
    "  * Illustration: printing results, plotting figures for illustration.\n",
    "  * You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzVUQS0CHry0"
   },
   "source": [
    "### Preprocessing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: MIMIC-III ICUSTAYS and PATIENTS tables\n",
    "mimic_dir = '../MIMIC-III Clinical Database/uncompressed/'\n",
    "data_dir = './data/'\n",
    "\n",
    "min_count = 100 # words whose occurred less than min_cnt are encoded as OTHER\n",
    "\n",
    "def create_icu_pat_admit():\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    # Load icustays table\n",
    "    # Table purpose: Defines each ICUSTAY_ID in the database, i.e. defines a single ICU stay\n",
    "    print('Load ICU stays...')\n",
    "    dtype = {'SUBJECT_ID': 'int32',\n",
    "           'HADM_ID': 'int32',\n",
    "           'ICUSTAY_ID': 'int32',\n",
    "           'INTIME': 'str',\n",
    "           'OUTTIME': 'str',\n",
    "           'LOS': 'float32'}\n",
    "    parse_dates = ['INTIME', 'OUTTIME']\n",
    "    icustays = pd.read_csv(mimic_dir + 'ICUSTAYS.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    # Load patients table\n",
    "    # Table purpose: Contains all charted data for all patients.\n",
    "    print('Load patients...')\n",
    "    dtype = {'SUBJECT_ID': 'int32',\n",
    "           'GENDER': 'str',\n",
    "           'DOB': 'str',\n",
    "           'DOD': 'str'}\n",
    "    parse_dates = ['DOB', 'DOD']\n",
    "    patients = pd.read_csv(mimic_dir + 'PATIENTS.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)  \n",
    "    \n",
    "    # Adjust shifted DOBs for older patients (median imputation)\n",
    "    old_patient = patients['DOB'].dt.year < 2000\n",
    "    date_offset = pd.DateOffset(years=(300-91), days=(-0.4*365))\n",
    "    patients['DOB'][old_patient] = patients['DOB'][old_patient].apply(lambda x: x + date_offset)\n",
    "    \n",
    "    # Replace GENDER by dummy binary column \n",
    "    patients = pd.get_dummies(patients, columns = ['GENDER'], drop_first=True)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    print('Load admissions...')\n",
    "    # Load admissions table\n",
    "    # Table purpose: Define a patients hospital admission, HADM_ID.\n",
    "    dtype = {'SUBJECT_ID': 'int32', \n",
    "           'HADM_ID': 'int32',\n",
    "           'ADMISSION_LOCATION': 'str',\n",
    "           'INSURANCE': 'str',\n",
    "           'MARITAL_STATUS': 'str',\n",
    "           'ETHNICITY': 'str',\n",
    "           'ADMITTIME': 'str',\n",
    "           'ADMISSION_TYPE': 'str'}\n",
    "    parse_dates = ['ADMITTIME']\n",
    "    admissions = pd.read_csv(mimic_dir + 'ADMISSIONS.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    print('Load services...')\n",
    "    # Load services table\n",
    "    # Table purpose: Lists services that a patient was admitted/transferred under.\n",
    "    dtype = {'SUBJECT_ID': 'int32', \n",
    "           'HADM_ID': 'int32',\n",
    "           'TRANSFERTIME': 'str',\n",
    "           'CURR_SERVICE': 'str'}\n",
    "    parse_dates = ['TRANSFERTIME']\n",
    "    services = pd.read_csv(mimic_dir + 'SERVICES.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    # Link icustays and patients tables\n",
    "    print('Link icustays and patients tables...')\n",
    "    icu_pat = pd.merge(icustays, patients, how='inner', on='SUBJECT_ID')\n",
    "    icu_pat.sort_values(by=['SUBJECT_ID', 'OUTTIME'], ascending=[True, False], inplace=True)\n",
    "    assert len(icu_pat['SUBJECT_ID'].unique()) == 46476\n",
    "    assert len(icu_pat['ICUSTAY_ID'].unique()) == 61532\n",
    "    \n",
    "    # Exclude icu stays during which patient died\n",
    "    icu_pat = icu_pat[~(icu_pat['DOD'] <= icu_pat['OUTTIME'])]\n",
    "    assert len(icu_pat['SUBJECT_ID'].unique()) == 43126\n",
    "    assert len(icu_pat['ICUSTAY_ID'].unique()) == 56745\n",
    "    \n",
    "    # Determine number of icu discharges in the last 365 days\n",
    "    print('Compute number of recent admissions...')\n",
    "    icu_pat['NUM_RECENT_ADMISSIONS'] = 0\n",
    "    for name, group in tqdm(icu_pat.groupby(['SUBJECT_ID'])):\n",
    "        for index, row in group.iterrows():\n",
    "            days_diff = (row['OUTTIME']-group['OUTTIME']).dt.days\n",
    "            icu_pat.at[index, 'NUM_RECENT_ADMISSIONS'] = len(group[(days_diff > 0) & (days_diff <=365)])\n",
    "    \n",
    "    # Create age variable and exclude patients < 18 y.o.\n",
    "    icu_pat['AGE'] = (icu_pat['OUTTIME'] - icu_pat['DOB']).dt.days/365.\n",
    "    icu_pat = icu_pat[icu_pat['AGE'] >= 18]\n",
    "    assert len(icu_pat['SUBJECT_ID'].unique()) == 35233\n",
    "    assert len(icu_pat['ICUSTAY_ID'].unique()) == 48616\n",
    "    \n",
    "    # Time to next admission (discharge to admission!)\n",
    "    icu_pat['DAYS_TO_NEXT'] = (icu_pat.groupby(['SUBJECT_ID']).shift(1)['INTIME'] - icu_pat['OUTTIME']).dt.days\n",
    "    \n",
    "    # Add early readmission flag (less than 30 days after discharge)\n",
    "    icu_pat['POSITIVE'] = (icu_pat['DAYS_TO_NEXT'] <= 30)\n",
    "    assert icu_pat['POSITIVE'].sum() == 5495\n",
    "    \n",
    "    # Add early death flag (less than 30 days after discharge)\n",
    "    early_death = ((icu_pat['DOD'] - icu_pat['OUTTIME']).dt.days <= 30)\n",
    "    assert early_death.sum() == 3795\n",
    "    \n",
    "    # Censor negative patients who died within less than 30 days after discharge (no chance of readmission)\n",
    "    icu_pat = icu_pat[icu_pat['POSITIVE'] | ~early_death]\n",
    "    assert len(icu_pat['SUBJECT_ID'].unique()) == 33150\n",
    "    assert len(icu_pat['ICUSTAY_ID'].unique()) == 45298\n",
    "    \n",
    "    # Clean up\n",
    "    icu_pat.drop(columns=['DOB', 'DOD', 'DAYS_TO_NEXT'], inplace=True)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    # Link icu_pat and admissions tables\n",
    "    print('Link icu_pat and admissions tables...')\n",
    "    icu_pat_admit = pd.merge(icu_pat, admissions, how='left', on=['SUBJECT_ID', 'HADM_ID'])\n",
    "    print(icu_pat_admit.isnull().sum())\n",
    "    \n",
    "    print('Some data cleaning on admissions...')\n",
    "    icu_pat_admit.loc[icu_pat_admit['ETHNICITY'].str.contains('WHITE'), 'ETHNICITY']    = 'WHITE'\n",
    "    icu_pat_admit.loc[icu_pat_admit['ETHNICITY'].str.contains('BLACK'), 'ETHNICITY']    = 'BLACK/AFRICAN AMERICAN'\n",
    "    icu_pat_admit.loc[icu_pat_admit['ETHNICITY'].str.contains('ASIAN'), 'ETHNICITY']    = 'ASIAN'\n",
    "    icu_pat_admit.loc[icu_pat_admit['ETHNICITY'].str.contains('HISPANIC'), 'ETHNICITY'] = 'HISPANIC/LATINO'\n",
    "    icu_pat_admit.loc[icu_pat_admit['ETHNICITY'].str.contains('DECLINED'), 'ETHNICITY'] = 'OTHER/UNKNOWN'\n",
    "    icu_pat_admit.loc[icu_pat_admit['ETHNICITY'].str.contains('MULTI'), 'ETHNICITY']    = 'OTHER/UNKNOWN'\n",
    "    icu_pat_admit.loc[icu_pat_admit['ETHNICITY'].str.contains('UNKNOWN'), 'ETHNICITY']  = 'OTHER/UNKNOWN'\n",
    "    icu_pat_admit.loc[icu_pat_admit['ETHNICITY'].str.contains('OTHER'), 'ETHNICITY']  = 'OTHER/UNKNOWN'\n",
    "    \n",
    "    icu_pat_admit['MARITAL_STATUS'].fillna('UNKNOWN', inplace=True)\n",
    "    icu_pat_admit.loc[icu_pat_admit['MARITAL_STATUS'].str.contains('MARRIED'), 'MARITAL_STATUS']      = 'MARRIED/LIFE PARTNER'\n",
    "    icu_pat_admit.loc[icu_pat_admit['MARITAL_STATUS'].str.contains('LIFE PARTNER'), 'MARITAL_STATUS'] = 'MARRIED/LIFE PARTNER'\n",
    "    icu_pat_admit.loc[icu_pat_admit['MARITAL_STATUS'].str.contains('WIDOWED'), 'MARITAL_STATUS']      = 'WIDOWED/DIVORCED/SEPARATED'\n",
    "    icu_pat_admit.loc[icu_pat_admit['MARITAL_STATUS'].str.contains('DIVORCED'), 'MARITAL_STATUS']     = 'WIDOWED/DIVORCED/SEPARATED'\n",
    "    icu_pat_admit.loc[icu_pat_admit['MARITAL_STATUS'].str.contains('SEPARATED'), 'MARITAL_STATUS']    = 'WIDOWED/DIVORCED/SEPARATED'\n",
    "    icu_pat_admit.loc[icu_pat_admit['MARITAL_STATUS'].str.contains('UNKNOWN'), 'MARITAL_STATUS']      = 'OTHER/UNKNOWN'\n",
    "    \n",
    "    columns_to_mask = ['ADMISSION_LOCATION',\n",
    "                     'INSURANCE',\n",
    "                     'MARITAL_STATUS',\n",
    "                     'ETHNICITY']\n",
    "    icu_pat_admit = icu_pat_admit.apply(lambda x: x.mask(x.map(x.value_counts()) < min_count, 'OTHER/UNKNOWN') if x.name in columns_to_mask else x)                   \n",
    "    icu_pat_admit = icu_pat_admit.apply(lambda x: x.str.title() if x.name in columns_to_mask else x)\n",
    "    \n",
    "    # Compute pre-ICU length of stay in fractional days\n",
    "    icu_pat_admit['PRE_ICU_LOS'] = (icu_pat_admit['INTIME'] - icu_pat_admit['ADMITTIME']) / np.timedelta64(1, 'D')\n",
    "    icu_pat_admit.loc[icu_pat_admit['PRE_ICU_LOS']<0, 'PRE_ICU_LOS'] = 0\n",
    "    \n",
    "    # Clean up\n",
    "    icu_pat_admit.drop(columns=['ADMITTIME'], inplace=True)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    # Link services table\n",
    "    # Keep first service only\n",
    "    services.sort_values(by=['HADM_ID', 'TRANSFERTIME'], ascending=True, inplace=True)\n",
    "    services = services.groupby(['HADM_ID']).nth(0).reset_index()\n",
    "    \n",
    "    # Check if first service is a surgery\n",
    "    services['SURGERY'] = services['CURR_SERVICE'].str.contains('SURG') | (services['CURR_SERVICE'] == 'ORTHO')\n",
    "    \n",
    "    print('Link services table...')  \n",
    "    icu_pat_admit = pd.merge(icu_pat_admit, services, how='left', on=['SUBJECT_ID', 'HADM_ID'])\n",
    "    \n",
    "    # Get elective surgery admissions\n",
    "    icu_pat_admit['ELECTIVE_SURGERY'] = ((icu_pat_admit['ADMISSION_TYPE'] == 'ELECTIVE') & icu_pat_admit['SURGERY']).astype(int)\n",
    "    \n",
    "    # Clean up\n",
    "    icu_pat_admit.drop(columns=['TRANSFERTIME', 'CURR_SERVICE', 'ADMISSION_TYPE', 'SURGERY'], inplace=True)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    # Baseline characteristics table\n",
    "    pos = icu_pat_admit[icu_pat_admit['POSITIVE']==1]\n",
    "    neg = icu_pat_admit[icu_pat_admit['POSITIVE']==0]\n",
    "    print('Total pos {}'.format(len(pos)))\n",
    "    print('Total neg {}'.format(len(neg)))\n",
    "    print(pos['LOS'].describe())\n",
    "    print(neg['LOS'].describe())\n",
    "    print((pos['PRE_ICU_LOS']).describe())\n",
    "    print((neg['PRE_ICU_LOS']).describe())\n",
    "    pd.set_option('display.precision', 1)\n",
    "    print(pos['AGE'].describe())\n",
    "    print(neg['AGE'].describe())\n",
    "    print(pos['NUM_RECENT_ADMISSIONS'].describe())\n",
    "    print(neg['NUM_RECENT_ADMISSIONS'].describe())\n",
    "    print(pd.DataFrame({'COUNTS': pos['GENDER_M'].value_counts(), 'PERC': pos['GENDER_M'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': neg['GENDER_M'].value_counts(), 'PERC': neg['GENDER_M'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': pos['ADMISSION_LOCATION'].value_counts(), 'PERC': pos['ADMISSION_LOCATION'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': neg['ADMISSION_LOCATION'].value_counts(), 'PERC': neg['ADMISSION_LOCATION'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': pos['INSURANCE'].value_counts(), 'PERC': pos['INSURANCE'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': neg['INSURANCE'].value_counts(), 'PERC': neg['INSURANCE'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': pos['MARITAL_STATUS'].value_counts(), 'PERC': pos['MARITAL_STATUS'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': neg['MARITAL_STATUS'].value_counts(), 'PERC': neg['MARITAL_STATUS'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': pos['ETHNICITY'].value_counts(), 'PERC': pos['ETHNICITY'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': neg['ETHNICITY'].value_counts(), 'PERC': neg['ETHNICITY'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': pos['ELECTIVE_SURGERY'].value_counts(), 'PERC': pos['ELECTIVE_SURGERY'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': neg['ELECTIVE_SURGERY'].value_counts(), 'PERC': neg['ELECTIVE_SURGERY'].value_counts(normalize=True)*100}))  \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Save...')\n",
    "    assert len(icu_pat_admit) == 45298\n",
    "    icu_pat_admit.sort_values(by='ICUSTAY_ID', ascending=True, inplace=True)\n",
    "    icu_pat_admit.to_pickle(data_dir + 'icu_pat_admit.pkl')\n",
    "    icu_pat_admit.to_csv(data_dir + 'icu_pat_admit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: reduce charts step\n",
    "def reduce_chart_events():\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    \n",
    "    # Relevant ITEMIDs\n",
    "    gcs_eye_opening          = [184, 220739, 226756, 227011]\n",
    "    gcs_verbal_response      = [723, 223900, 226758, 227014]\n",
    "    gcs_motor_response       = [454, 223901, 226757, 227012]\n",
    "    gcs_total                = [198, 226755]\n",
    "    diastolic_blood_pressure = [8364, 8368, 8440, 8441, 8502, 8503, 8506, 8555, 220051, 220180, 224643, 225310, 227242]\n",
    "    systolic_blood_pressure  = [   6,   51,  442,  455, 3313, 3315, 3321, 6701, 220050, 220179, 224167, 225309, 227243]\n",
    "    mean_blood_pressure      = [52, 443, 456, 2293, 2294, 2647, 3312, 3314, 3320, 6590, 6702, 6927, 7620, 220052, 220181, 225312]\n",
    "    heart_rate               = [211, 220045, 227018]\n",
    "    fraction_inspired_oxygen = [189, 190, 727, 1040, 1206, 1863, 2518, 2981, 3420, 3422, 7018, 7041, 7570, 223835, 226754, 227009, 227010]\n",
    "    respiratory_rate         = [614, 615, 618, 619, 651, 653, 1884, 3603, 6749, 7884, 8113, 220210, 224422, 224688, 224689, 224690, 226774, 227050]\n",
    "    body_temperature         = [676, 677, 678, 679, 3652, 3654, 6643, 223761, 223762, 226778, 227054]\n",
    "    weight                   = [763, 3580, 3581, 3582, 3693, 224639, 226512, 226531]\n",
    "    height                   = [1394, 226707, 226730]\n",
    "    \n",
    "    def inch_to_cm(value):\n",
    "        return value*2.54\n",
    "    \n",
    "    def lb_to_kg(value):\n",
    "        return value/2.205\n",
    "    \n",
    "    def oz_to_kg(value):\n",
    "        return value/35.274\n",
    "    \n",
    "    def f_to_c(value):\n",
    "        return (value-32)*5/9\n",
    "    \n",
    "    def frac_to_perc(value):\n",
    "        return value*100\n",
    "    \n",
    "    # Relevant ITEMIDs\n",
    "    body_temperature_F       = [678, 679, 3652, 3654, 6643, 223761, 226778, 227054]\n",
    "    weight_lb                = [3581, 226531]\n",
    "    weight_oz                = [3582]\n",
    "    height_inch              = [1394, 226707]\n",
    "    \n",
    "    relevant_ids = (gcs_eye_opening + gcs_verbal_response + gcs_motor_response + gcs_total + mean_blood_pressure + \n",
    "                  heart_rate + fraction_inspired_oxygen + respiratory_rate + body_temperature + weight + height)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    print('Load item definitions')\n",
    "    dtype = {'ITEMID': 'int32',\n",
    "           'LABEL': 'str',\n",
    "           'UNITNAME': 'str'}\n",
    "    defs = pd.read_csv(mimic_dir + 'D_ITEMS.csv', usecols=dtype.keys(), dtype=dtype)\n",
    "    print('GCS_EYE_OPENING')\n",
    "    print(defs[defs['ITEMID'].isin(gcs_eye_opening)])\n",
    "    print('GCS_VERBAL_RESPONSE')\n",
    "    print(defs[defs['ITEMID'].isin(gcs_verbal_response)])\n",
    "    print('GCS_MOTOR_RESPONSE')\n",
    "    print(defs[defs['ITEMID'].isin(gcs_motor_response)])\n",
    "    print('GCS_TOTAL')\n",
    "    print(defs[defs['ITEMID'].isin(gcs_total)])\n",
    "    print('DIASTOLIC_BP')\n",
    "    print(defs[defs['ITEMID'].isin(diastolic_blood_pressure)])\n",
    "    print('SYSTOLIC_BP')\n",
    "    print(defs[defs['ITEMID'].isin(systolic_blood_pressure)])\n",
    "    print('MEAN_BP')\n",
    "    print(defs[defs['ITEMID'].isin(mean_blood_pressure)])\n",
    "    print('HEART_RATE')\n",
    "    print(defs[defs['ITEMID'].isin(heart_rate)])\n",
    "    print('FRACTION_INSPIRED_OXYGEN')\n",
    "    print(defs[defs['ITEMID'].isin(fraction_inspired_oxygen)])\n",
    "    print('RESPIRATORY_RATE')\n",
    "    print(defs[defs['ITEMID'].isin(respiratory_rate)])\n",
    "    print('BODY_TEMPERATURE')\n",
    "    print(defs[defs['ITEMID'].isin(body_temperature)])\n",
    "    print('WEIGHT')\n",
    "    print(defs[defs['ITEMID'].isin(weight)])\n",
    "    print('HEIGHT')\n",
    "    print(defs[defs['ITEMID'].isin(height)])\n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Loading Chart Events')\n",
    "    dtype = {'SUBJECT_ID': 'int32',\n",
    "           'HADM_ID': 'int32',\n",
    "           'ICUSTAY_ID': 'str',\n",
    "           'ITEMID': 'int32',\n",
    "           'CHARTTIME': 'str',\n",
    "           'VALUENUM': 'float32'}\n",
    "    parse_dates = ['CHARTTIME']\n",
    "    # Load chartevents table\n",
    "    # Table purpose: Contains all charted data for all patients.\n",
    "    chunksize = 1000000\n",
    "    i = 0\n",
    "    # Not parsing dates\n",
    "    for df in tqdm(pd.read_csv(mimic_dir + 'CHARTEVENTS.csv', usecols=dtype.keys(), dtype=dtype, chunksize=chunksize)):\n",
    "        df = df[df['ICUSTAY_ID'].notna() & df['VALUENUM'].notna() & (df['ITEMID'].isin(relevant_ids)) & (df['VALUENUM'] > 0)]\n",
    "        # convert units\n",
    "        df.loc[df['ITEMID'].isin(body_temperature_F), 'VALUENUM'] = f_to_c(df[df['ITEMID'].isin(body_temperature_F)].VALUENUM)\n",
    "        df.loc[df['ITEMID'].isin(weight_lb), 'VALUENUM'] = lb_to_kg(df[df['ITEMID'].isin(weight_lb)].VALUENUM)\n",
    "        df.loc[df['ITEMID'].isin(weight_oz), 'VALUENUM'] = oz_to_kg(df[df['ITEMID'].isin(weight_oz)].VALUENUM)\n",
    "        df.loc[df['ITEMID'].isin(height_inch), 'VALUENUM'] = inch_to_cm(df[df['ITEMID'].isin(height_inch)].VALUENUM)\n",
    "        df.loc[(df['ITEMID'].isin(fraction_inspired_oxygen)) & (df['VALUENUM']<=1), 'VALUENUM'] = frac_to_perc(df[(df['ITEMID'].isin(fraction_inspired_oxygen)) & (df['VALUENUM']<=1)].VALUENUM)\n",
    "        # remove implausible measurements\n",
    "        df = df[~(df['ITEMID'].isin(gcs_total) & (df.VALUENUM < 3))]\n",
    "        df = df[~(df['ITEMID'].isin(diastolic_blood_pressure + systolic_blood_pressure + mean_blood_pressure) & (df.VALUENUM > 250))]\n",
    "        df = df[~(df['ITEMID'].isin(heart_rate) & ((df.VALUENUM < 1) | (df.VALUENUM > 250)))]\n",
    "        df = df[~(df['ITEMID'].isin(fraction_inspired_oxygen) & (df.VALUENUM > 100))]\n",
    "        df = df[~(df['ITEMID'].isin(respiratory_rate) & ((df.VALUENUM < 1) | (df.VALUENUM > 100)))]\n",
    "        df = df[~(df['ITEMID'].isin(body_temperature) & (df.VALUENUM > 50))]\n",
    "        df = df[~(df['ITEMID'].isin(weight) & (df.VALUENUM > 700))]\n",
    "        df = df[~(df['ITEMID'].isin(height) & (df.VALUENUM > 300))]\n",
    "        df = df[df['VALUENUM'] > 0]\n",
    "        # label\n",
    "        df['CE_TYPE'] = ''\n",
    "        df.loc[df['ITEMID'].isin(gcs_eye_opening), 'CE_TYPE'] = 'GCS_EYE_OPENING'\n",
    "        df.loc[df['ITEMID'].isin(gcs_verbal_response), 'CE_TYPE'] = 'GCS_VERBAL_RESPONSE'\n",
    "        df.loc[df['ITEMID'].isin(gcs_motor_response), 'CE_TYPE'] = 'GCS_MOTOR_RESPONSE'\n",
    "        df.loc[df['ITEMID'].isin(gcs_total), 'CE_TYPE'] = 'GCS_TOTAL'\n",
    "        df.loc[df['ITEMID'].isin(diastolic_blood_pressure), 'CE_TYPE'] = 'DIASTOLIC_BP'\n",
    "        df.loc[df['ITEMID'].isin(systolic_blood_pressure), 'CE_TYPE'] = 'SYSTOLIC_BP'\n",
    "        df.loc[df['ITEMID'].isin(mean_blood_pressure), 'CE_TYPE'] = 'MEAN_BP'\n",
    "        df.loc[df['ITEMID'].isin(heart_rate), 'CE_TYPE'] = 'HEART_RATE'\n",
    "        df.loc[df['ITEMID'].isin(fraction_inspired_oxygen), 'CE_TYPE'] = 'FRACTION_INSPIRED_OXYGEN'\n",
    "        df.loc[df['ITEMID'].isin(respiratory_rate), 'CE_TYPE'] = 'RESPIRATORY_RATE'\n",
    "        df.loc[df['ITEMID'].isin(body_temperature), 'CE_TYPE'] = 'BODY_TEMPERATURE'\n",
    "        df.loc[df['ITEMID'].isin(weight), 'CE_TYPE'] = 'WEIGHT'\n",
    "        df.loc[df['ITEMID'].isin(height), 'CE_TYPE'] = 'HEIGHT'    \n",
    "        df.drop(columns=['ITEMID'], inplace=True)\n",
    "        \n",
    "        # save\n",
    "        if i == 0:\n",
    "            df.to_csv(data_dir + 'chartevents_reduced.csv', index=False)\n",
    "        else:\n",
    "            df.to_csv(data_dir + 'chartevents_reduced.csv', mode='a', header=False, index=False)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: reduce outputs events step\n",
    "def reduce_output_events():\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    \n",
    "    # Relevant ITEMIDs, from https://github.com/vincentmajor/mimicfilters/blob/master/lists/OASIS_components/preprocess_urine_awk_str.txt\n",
    "    urine_output = [42810, 43171, 43173, 43175, 43348, 43355, 43365, 43372, 43373, 43374, 43379, 43380, 43431, 43462, 43522, 40405, 40428, 40534, \n",
    "    40288, 42042, 42068, 42111, 42119, 42209, 41857, 40715, 40056, 40061, 40085, 40094, 40096, 42001, 42676, 42556, 43093, 44325, 44706,\n",
    "    44506, 42859, 44237, 44313, 44752, 44824, 44837, 43576, 43589, 43633, 44911, 44925, 42362, 42463, 42507, 42510, 40055, 40057, 40065,\n",
    "    40069, 45804, 45841, 43811, 43812, 43856, 43897, 43931, 43966, 44080, 44103, 44132, 45304, 46177, 46532, 46578, 46658, 46748, 40651,\n",
    "    43053, 43057, 40473, 42130, 41922, 44253, 44278, 46180, 44684, 43333, 43347, 42592, 42666, 42765, 42892, 45927, 44834, 43638, 43654,\n",
    "    43519, 43537, 42366, 45991, 46727, 46804, 43987, 44051, 227489, 226566, 226627, 226631, 45415, 42111, 41510, 40055, 226559, 40428,\n",
    "    40580, 40612, 40094, 40848, 43685, 42362, 42463, 42510, 46748, 40972, 40973, 46456, 226561, 226567, 226632, 40096, 40651, 226557,\n",
    "    226558, 40715, 226563]\n",
    "    \n",
    "    # Relevant ITEMIDs\n",
    "    print('-----------------------------------------')\n",
    "    print('Load item definitions')\n",
    "    dtype = {'ITEMID': 'int32',\n",
    "           'LABEL': 'str',\n",
    "           'UNITNAME': 'str',\n",
    "           'LINKSTO': 'str'}\n",
    "    defs = pd.read_csv(mimic_dir + 'D_ITEMS.csv', usecols=dtype.keys(), dtype=dtype)\n",
    "    print('URINE_OUTPUT')\n",
    "    defs = defs[defs['ITEMID'].isin(urine_output)]\n",
    "    defs['LABEL'] = defs['LABEL'].str.lower()\n",
    "    # Remove measurements in /kg/hr\n",
    "    defs = defs[~(defs['LABEL'].str.contains('hr') | defs['LABEL'].str.contains('kg')) | defs['LABEL'].str.contains('nephro')]\n",
    "    print(defs['LABEL'])\n",
    "    urine_output = defs['ITEMID'].tolist()\n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Loading Output Events')\n",
    "    dtype = {'ICUSTAY_ID': 'str',\n",
    "           'ITEMID': 'int32',\n",
    "           'CHARTTIME': 'str',\n",
    "           'VALUE': 'float32'}\n",
    "    parse_dates = ['CHARTTIME']\n",
    "    \n",
    "    # Load outputevents table\n",
    "    # Table purpose: Output data for patients.\n",
    "    df = pd.read_csv(mimic_dir + 'OUTPUTEVENTS.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)\n",
    "    df = df.rename(columns={'VALUE': 'VALUENUM'})\n",
    "    df = df[df['ICUSTAY_ID'].notna() & df['VALUENUM'].notna() & (df['ITEMID'].isin(urine_output)) & (df['VALUENUM'] > 0)]\n",
    "    df['ICUSTAY_ID'] = df['ICUSTAY_ID'].astype('int32')\n",
    "    \n",
    "    # remove implausible measurements\n",
    "    df = df[~(df.VALUENUM > 10000)]\n",
    "    \n",
    "    # sum all outputs in one day\n",
    "    df.drop(columns=['ITEMID'], inplace=True)\n",
    "    df['CHARTTIME'] = df['CHARTTIME'].dt.date\n",
    "    df = df.groupby(['ICUSTAY_ID', 'CHARTTIME']).sum()\n",
    "    df['CE_TYPE'] = 'URINE_OUTPUT'\n",
    "    df = df[~(df.VALUENUM > 10000)]\n",
    "    \n",
    "    print('Remove admission and discharge days (since data on urine output is incomplete)')\n",
    "    # Load icustays table\n",
    "    # Table purpose: Defines each ICUSTAY_ID in the database, i.e. defines a single ICU stay\n",
    "    print('Load ICU stays...')\n",
    "    dtype = {'ICUSTAY_ID': 'int32',\n",
    "           'INTIME': 'str',\n",
    "           'OUTTIME': 'str'}\n",
    "    parse_dates = ['INTIME', 'OUTTIME']\n",
    "    icustays = pd.read_csv(mimic_dir + 'ICUSTAYS.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)\n",
    "    icustays['INTIME'] = icustays['INTIME'].dt.date\n",
    "    icustays['OUTTIME'] = icustays['OUTTIME'].dt.date\n",
    "    \n",
    "    # Merge\n",
    "    tmp = icustays[['ICUSTAY_ID', 'INTIME']].drop_duplicates()\n",
    "    tmp = tmp.rename(columns={'INTIME': 'CHARTTIME'})\n",
    "    tmp['ID_IN'] = 1\n",
    "    df = pd.merge(df, tmp, how='left', on=['ICUSTAY_ID', 'CHARTTIME'])\n",
    "    tmp = icustays[['ICUSTAY_ID', 'OUTTIME']].drop_duplicates()\n",
    "    tmp = tmp.rename(columns={'OUTTIME': 'CHARTTIME'})\n",
    "    tmp['ID_OUT'] = 1\n",
    "    df = pd.merge(df, tmp, how='left', on=['ICUSTAY_ID', 'CHARTTIME'])\n",
    "    \n",
    "    # Remove admission and discharge days\n",
    "    df = df[df['ID_IN'].isnull() & df['ID_OUT'].isnull()]\n",
    "    df.drop(columns=['ID_IN', 'ID_OUT'], inplace=True)\n",
    "    \n",
    "    # Add SUBJECT_ID and HADM_ID\n",
    "    icustays.drop(columns=['INTIME', 'OUTTIME'], inplace=True)  \n",
    "    df['CHARTTIME'] = pd.to_datetime(df['CHARTTIME']) + pd.DateOffset(hours=12)\n",
    "    \n",
    "    # Save\n",
    "    df.to_pickle(data_dir + 'outputevents_reduced.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: merge charts outputs step\n",
    "def merge_chart_outputs():\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    # Load (reduced) chartevents table\n",
    "    print('Loading chart events...')\n",
    "    dtype = {'SUBJECT_ID': 'int32',\n",
    "           'ICUSTAY_ID': 'int32',\n",
    "           'CE_TYPE': 'str',\n",
    "           'CHARTTIME': 'str',\n",
    "           'VALUENUM': 'float32'}\n",
    "    parse_dates = ['CHARTTIME']\n",
    "    charts = pd.read_csv(data_dir + 'chartevents_reduced.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Compute BMI and GCS total...')\n",
    "    charts.sort_values(by=['SUBJECT_ID', 'ICUSTAY_ID', 'CHARTTIME'], ascending=[True, True, False], inplace=True)\n",
    "    \n",
    "    # Compute BMI\n",
    "    rows_bmi = (charts['CE_TYPE']=='WEIGHT') | (charts['CE_TYPE']=='HEIGHT')\n",
    "    charts_bmi = charts[rows_bmi]\n",
    "    charts_bmi = charts_bmi.pivot_table(index=['SUBJECT_ID', 'ICUSTAY_ID', 'CHARTTIME'], columns='CE_TYPE', values='VALUENUM')\n",
    "    charts_bmi = charts_bmi.rename_axis(None, axis=1).reset_index()\n",
    "    charts_bmi['HEIGHT'] = charts_bmi.groupby('SUBJECT_ID')['HEIGHT'].ffill()\n",
    "    charts_bmi['HEIGHT'] = charts_bmi.groupby('SUBJECT_ID')['HEIGHT'].bfill()\n",
    "    charts_bmi =  charts_bmi[~pd.isnull(charts_bmi).any(axis=1)]\n",
    "    charts_bmi['VALUENUM'] = charts_bmi['WEIGHT']/charts_bmi['HEIGHT']/charts_bmi['HEIGHT']*10000\n",
    "    charts_bmi['CE_TYPE'] = 'BMI'\n",
    "    charts_bmi.drop(columns=['HEIGHT', 'WEIGHT'], inplace=True)\n",
    "    \n",
    "    # Compute GCS total if not available\n",
    "    rows_gcs = (charts['CE_TYPE']=='GCS_EYE_OPENING') | (charts['CE_TYPE']=='GCS_VERBAL_RESPONSE') | (charts['CE_TYPE']=='GCS_MOTOR_RESPONSE') | (charts['CE_TYPE']=='GCS_TOTAL')\n",
    "    charts_gcs = charts[rows_gcs]\n",
    "    charts_gcs = charts_gcs.pivot_table(index=['SUBJECT_ID', 'ICUSTAY_ID', 'CHARTTIME'], columns='CE_TYPE', values='VALUENUM')\n",
    "    charts_gcs = charts_gcs.rename_axis(None, axis=1).reset_index()\n",
    "    null_gcs_total = charts_gcs['GCS_TOTAL'].isnull()\n",
    "    charts_gcs.loc[null_gcs_total, 'GCS_TOTAL'] = charts_gcs[null_gcs_total].GCS_EYE_OPENING + charts_gcs[null_gcs_total].GCS_VERBAL_RESPONSE + charts_gcs[null_gcs_total].GCS_MOTOR_RESPONSE\n",
    "    charts_gcs =  charts_gcs[~charts_gcs['GCS_TOTAL'].isnull()]\n",
    "    charts_gcs = charts_gcs.rename(columns={'GCS_TOTAL': 'VALUENUM'})\n",
    "    charts_gcs['CE_TYPE'] = 'GCS_TOTAL'\n",
    "    charts_gcs.drop(columns=['GCS_EYE_OPENING', 'GCS_VERBAL_RESPONSE', 'GCS_MOTOR_RESPONSE'], inplace=True)\n",
    "    \n",
    "    # Merge back with rest of the table\n",
    "    rows_others = ~rows_bmi & ~rows_gcs\n",
    "    charts = pd.concat([charts_bmi, charts_gcs, charts[rows_others]], ignore_index=True, sort=False)\n",
    "    charts.drop(columns=['SUBJECT_ID'], inplace=True)\n",
    "    charts.sort_values(by=['ICUSTAY_ID', 'CHARTTIME'], ascending=[True, False], inplace=True)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    # Load (reduced) outputevents table\n",
    "    print('Loading output events...')\n",
    "    outputs = pd.read_pickle(data_dir + 'outputevents_reduced.pkl')\n",
    "    df = pd.concat([charts, outputs], ignore_index=True, sort=False)\n",
    "    df.sort_values(by=['ICUSTAY_ID', 'CHARTTIME'], ascending=[True, False], inplace=True)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Create categorical variable...')\n",
    "    # Bin according to OASIS severity score\n",
    "    heart_rate_bins               = np.array([-1, 32.99, 88.5, 106.5, 125.5, np.Inf])\n",
    "    respiratory_rate_bins         = np.array([-1, 5.99, 12.5, 22.5, 30.5, 44.5, np.Inf])\n",
    "    body_temperature_bins         = np.array([-1, 33.21, 35.93, 36.39, 36.88, 39.88, np.Inf])\n",
    "    mean_bp_bins                  = np.array([-1, 20.64, 50.99, 61.32, 143.44, np.Inf])\n",
    "    fraction_inspired_oxygen_bins = np.array([-1, np.Inf])\n",
    "    gcs_total_bins                = np.array([-1, 7, 13, 14, 15])\n",
    "    bmi_bins                      = np.array([-1, 15, 16, 18.5, 25, 30, 35, 40, 45, 50, 60, np.Inf])\n",
    "    urine_output_bins             = np.array([-1, 670.99, 1426.99, 2543.99, 6896, np.Inf])\n",
    "    bins = [heart_rate_bins, respiratory_rate_bins, body_temperature_bins, mean_bp_bins, fraction_inspired_oxygen_bins, gcs_total_bins, urine_output_bins]\n",
    "\n",
    "    # Labels \n",
    "    heart_rate_labels               = ['CHART_HR_m1', 'CHART_HR_n', 'CHART_HR_p1', 'CHART_HR_p2', 'CHART_HR_p3']\n",
    "    respiratory_rate_labels         = ['CHART_RR_m2', 'CHART_RR_m1', 'CHART_RR_n', 'CHART_RR_p1', 'CHART_RR_p2', 'CHART_RR_p3']\n",
    "    body_temperature_labels         = ['CHART_BT_m3', 'CHART_BT_m2', 'CHART_BT_m1', 'CHART_BT_n', 'CHART_BT_p1', 'CHART_BT_p2']\n",
    "    mean_bp_labels                  = ['CHART_BP_m3', 'CHART_BP_m2', 'CHART_BP_m1', 'CHART_BP_n', 'CHART_BP_p1']\n",
    "    fraction_inspired_oxygen_labels = ['CHART_VENT']\n",
    "    gcs_total_labels                = ['CHART_GC_m3', 'CHART_GC_m2', 'CHART_GC_m1', 'CHART_GC_n']\n",
    "    bmi_labels                      = ['CHART_BM_m3', 'CHART_BM_m2', 'CHART_BM_m1', 'CHART_BM_n', 'CHART_BM_p1', 'CHART_BM_p2', 'CHART_BM_p3', 'CHART_BM_p4', 'CHART_BM_p5', 'CHART_BM_p6', 'CHART_BM_p7']\n",
    "    urine_output_labels             = ['CHART_UO_m3', 'CHART_UO_m2', 'CHART_UO_m1', 'CHART_UO_n', 'CHART_UO_p1']\n",
    "    labels = [heart_rate_labels, respiratory_rate_labels, body_temperature_labels, mean_bp_labels, fraction_inspired_oxygen_labels, gcs_total_labels, urine_output_labels]\n",
    "\n",
    "    # Chart event types\n",
    "    ce_types = ['HEART_RATE', 'RESPIRATORY_RATE', 'BODY_TEMPERATURE', 'MEAN_BP', 'FRACTION_INSPIRED_OXYGEN', 'GCS_TOTAL', 'URINE_OUTPUT']\n",
    "    \n",
    "    df_list = []\n",
    "    df_list_last_only = [] # for logistic regression\n",
    "    for type, label, bin in zip(ce_types, labels, bins):\n",
    "        # get chart events of a specific type\n",
    "        tmp = df[df['CE_TYPE'] == type]\n",
    "\n",
    "        # bin them and sort\n",
    "        tmp['VALUECAT'] = pd.cut(tmp['VALUENUM'], bins=bin, labels=label)\n",
    "        tmp.drop(columns=['CE_TYPE', 'VALUENUM'], inplace=True)\n",
    "        tmp.sort_values(by=['ICUSTAY_ID', 'CHARTTIME'], ascending=[True, False], inplace=True)\n",
    "\n",
    "        # remove consecutive duplicates\n",
    "        tmp = tmp[(tmp[['ICUSTAY_ID', 'VALUECAT']] != tmp[['ICUSTAY_ID', 'VALUECAT']].shift()).any(axis=1)]\n",
    "        df_list.append(tmp)\n",
    "\n",
    "        # for logistic regression, keep only the last measurement\n",
    "        tmp = tmp.drop_duplicates(subset='ICUSTAY_ID')\n",
    "        df_list_last_only.append(tmp)\n",
    "    \n",
    "    df = pd.concat(df_list, ignore_index=True, sort=False)\n",
    "    df.sort_values(by=['ICUSTAY_ID', 'CHARTTIME'], ascending=[True, False], inplace=True)\n",
    "    \n",
    "    # drop duplicates to keep size manageable\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Save...')\n",
    "    df.to_pickle(data_dir + 'charts_outputs_reduced.pkl')\n",
    "    df.to_csv(data_dir + 'charts_outputs_reduced.csv', index=False)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Save data for logistic regression...')\n",
    "    \n",
    "    # for logistic regression\n",
    "    df_last_only = pd.concat(df_list_last_only, ignore_index=True, sort=False)\n",
    "    df_last_only.sort_values(by=['ICUSTAY_ID', 'CHARTTIME'], ascending=[True, False], inplace=True)\n",
    "    df_last_only.to_pickle(data_dir + 'charts_outputs_last_only.pkl')\n",
    "    df_last_only.to_csv(data_dir + 'charts_outputs_last_only.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: diagnoses procedures step\n",
    "def link_diagnoses_procedures():\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    # Load icu_pat table\n",
    "    print('Loading icu_pat...')\n",
    "    icu_pat = pd.read_pickle(data_dir + 'icu_pat_admit.pkl')\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    print('Load admissions...')\n",
    "    # Load admissions table\n",
    "    # Table purpose: Define a patients hospital admission, HADM_ID.\n",
    "    dtype = {'HADM_ID': 'int32',\n",
    "           'ADMITTIME': 'str',\n",
    "           'DISCHTIME': 'str'}\n",
    "    parse_dates = ['ADMITTIME', 'DISCHTIME']\n",
    "    admissions = pd.read_csv(mimic_dir + 'ADMISSIONS.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    print('Load diagnoses and procedures...')\n",
    "    dtype = {'SUBJECT_ID': 'int32',\n",
    "           'HADM_ID': 'int32',\n",
    "           'ICD9_CODE': 'str'}\n",
    "\n",
    "    # Load diagnosis_icd table\n",
    "    # Table purpose: Contains ICD diagnoses for patients, most notably ICD-9 diagnoses.\n",
    "    diagnoses = pd.read_csv(mimic_dir + 'DIAGNOSES_ICD.csv', usecols=dtype.keys(), dtype=dtype)\n",
    "    diagnoses = diagnoses.dropna()\n",
    "\n",
    "    # Load procedures_icd table\n",
    "    # Table purpose: Contains ICD procedures for patients, most notably ICD-9 procedures.\n",
    "    procedures = pd.read_csv(mimic_dir + 'PROCEDURES_ICD.csv', usecols=dtype.keys(), dtype=dtype)\n",
    "    procedures = procedures.dropna()\n",
    "    \n",
    "    # Merge diagnoses and procedures\n",
    "    diagnoses['ICD9_CODE'] = 'DIAGN_' + diagnoses['ICD9_CODE'].str.lower().str.strip()\n",
    "    procedures['ICD9_CODE'] = 'PROCE_' + procedures['ICD9_CODE'].str.lower().str.strip()\n",
    "    diag_proc = pd.concat([diagnoses, procedures], ignore_index=True, sort=False)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    # Link diagnoses/procedures and admissions tables\n",
    "    print('Link diagnoses/procedures and admissions tables...')\n",
    "    diag_proc = pd.merge(diag_proc, admissions, how='inner', on='HADM_ID').drop(columns=['HADM_ID'])\n",
    "    \n",
    "    # Link diagnoses/procedures and icu_pat tables\n",
    "    print('Link diagnoses/procedures and icu_pat tables...')\n",
    "    diag_proc = pd.merge(icu_pat[['SUBJECT_ID', 'ICUSTAY_ID', 'OUTTIME']], diag_proc, how='left', on=['SUBJECT_ID'])\n",
    "    \n",
    "    # Remove codes related to future admissions using time difference to ADMITTIME\n",
    "    diag_proc['DAYS_TO_OUT'] = (diag_proc['OUTTIME']-diag_proc['ADMITTIME']) / np.timedelta64(1, 'D')\n",
    "    diag_proc = diag_proc[(diag_proc['DAYS_TO_OUT'] >= 0) | diag_proc['DAYS_TO_OUT'].isna()]\n",
    "\n",
    "    # Reset time value using time difference to DISCHTIME (0 if negative)\n",
    "    diag_proc['DAYS_TO_OUT'] = (diag_proc['OUTTIME']-diag_proc['DISCHTIME']) / np.timedelta64(1, 'D')\n",
    "    diag_proc.loc[diag_proc['DAYS_TO_OUT'] < 0, 'DAYS_TO_OUT'] = 0\n",
    "    diag_proc = diag_proc.drop(columns=['SUBJECT_ID', 'OUTTIME', 'ADMITTIME', 'DISCHTIME'])\n",
    "\n",
    "    # Lost some ICUSTAY_IDs with only negative DAYS_TO_OUT, merge back\n",
    "    diag_proc = pd.merge(icu_pat[['ICUSTAY_ID']], diag_proc, how='left', on=['ICUSTAY_ID'])\n",
    "    \n",
    "    print('Drop duplicates...')\n",
    "    diag_proc = diag_proc.drop_duplicates()\n",
    "    \n",
    "    print('Map rare codes to OTHER...')\n",
    "    diag_proc = diag_proc.apply(lambda x: x.mask(x.map(x.value_counts()) < min_count, 'other') if x.name in ['ICD9_CODE'] else x)                   \n",
    "    \n",
    "    print('-----------------------------------------')  \n",
    "    print('Save...')\n",
    "    assert len(diag_proc['ICUSTAY_ID'].unique()) == 45298\n",
    "    diag_proc.sort_values(by=['ICUSTAY_ID', 'DAYS_TO_OUT'], ascending=[True, True], inplace=True)\n",
    "    diag_proc.to_pickle(data_dir + 'diag_proc.pkl')\n",
    "    diag_proc.to_csv(data_dir + 'diag_proc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: charts prescriptions step\n",
    "def link_charts_prescriptions():\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    # Load icu_pat table\n",
    "    print('Loading icu_pat...')\n",
    "    icu_pat = pd.read_pickle(data_dir + 'icu_pat_admit.pkl')\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    print('Load charts and outputs...')\n",
    "    charts_outputs = pd.read_pickle(data_dir + 'charts_outputs_reduced.pkl')\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    print('Load prescriptions...')\n",
    "    dtype = {'ICUSTAY_ID': 'str',\n",
    "           'DRUG': 'str',\n",
    "           'STARTDATE': 'str'}\n",
    "    parse_dates = ['STARTDATE']\n",
    "    # Load prescriptions table\n",
    "    # Table purpose: Contains medication related order entries, i.e. prescriptions\n",
    "    prescriptions = pd.read_csv(mimic_dir + 'PRESCRIPTIONS.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)\n",
    "    prescriptions = prescriptions.dropna()\n",
    "    prescriptions['ICUSTAY_ID'] = prescriptions['ICUSTAY_ID'].astype('int32')\n",
    "    prescriptions['DRUG'] = 'PRESC_' + prescriptions['DRUG'].str.lower().replace('\\s+', '', regex=True)\n",
    "    prescriptions = prescriptions.rename(columns={'DRUG': 'VALUECAT', 'STARTDATE': 'CHARTTIME'})\n",
    "    df = pd.concat([charts_outputs, prescriptions], ignore_index=True, sort=False)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    # Link charts/outputs and icu_pat tables\n",
    "    print('Link charts/outputs and icu_pat tables...')\n",
    "    df = pd.merge(icu_pat[['ICUSTAY_ID', 'OUTTIME']], df, how='left', on=['ICUSTAY_ID'])\n",
    "    \n",
    "    # Reset time value using time difference to DISCHTIME (0 if negative)\n",
    "    df['HOURS_TO_OUT'] = (df['OUTTIME']-df['CHARTTIME']) / np.timedelta64(1, 'h')\n",
    "    df.loc[df['HOURS_TO_OUT'] < 0, 'HOURS_TO_OUT'] = 0\n",
    "    df = df.drop(columns=['OUTTIME', 'CHARTTIME'])\n",
    "    \n",
    "    print('Drop duplicates...')\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    print('Map rare codes to OTHER...')\n",
    "    df = df.apply(lambda x: x.mask(x.map(x.value_counts()) < min_count, 'other') if x.name in ['VALUECAT'] else x)                   \n",
    "    \n",
    "    print('-----------------------------------------')  \n",
    "    print('Save...')\n",
    "    assert len(df['ICUSTAY_ID'].unique()) == 45298\n",
    "    df.sort_values(by=['ICUSTAY_ID', 'HOURS_TO_OUT'], ascending=[True, True], inplace=True)\n",
    "    df.to_pickle(data_dir + 'charts_prescriptions.pkl')\n",
    "    df.to_csv(data_dir + 'charts_prescriptions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: create data arrays step\n",
    "def create_data_arrays():\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    def get_arrays(df, code_column, time_column, quantile=1):\n",
    "      df['COUNT'] = df.groupby(['ICUSTAY_ID']).cumcount()\n",
    "      df = df[df['COUNT'] < df.groupby(['ICUSTAY_ID']).size().quantile(q=quantile)]\n",
    "      max_count_df = df['COUNT'].max()+1\n",
    "      print('max_count {}'.format(max_count_df))\n",
    "      multiindex_df = pd.MultiIndex.from_product([icu_pat['ICUSTAY_ID'], range(max_count_df)], names = ['ICUSTAY_ID', 'COUNT'])\n",
    "      df = df.set_index(['ICUSTAY_ID', 'COUNT'])\n",
    "    \n",
    "      print('Reindex df...')\n",
    "      df = df.reindex(multiindex_df).fillna(0)\n",
    "      print('done')\n",
    "      df_times = df[time_column].values.reshape((num_icu_stays, max_count_df))\n",
    "      df[code_column] = df[code_column].astype('category')\n",
    "      dict_df = dict(enumerate(df[code_column].cat.categories))\n",
    "      df[code_column] = df[code_column].cat.codes\n",
    "      df = df[code_column].values.reshape((num_icu_stays, max_count_df))\n",
    "    \n",
    "      return df, df_times, dict_df\n",
    "  \n",
    "    # Load icu_pat table\n",
    "    print('Loading icu_pat...')\n",
    "    icu_pat = pd.read_pickle(data_dir + 'icu_pat_admit.pkl')\n",
    "    \n",
    "    print('Loading diagnoses/procedures...')\n",
    "    dp = pd.read_pickle(data_dir + 'diag_proc.pkl')\n",
    "    \n",
    "    print('Loading charts/prescriptions...')\n",
    "    cp = pd.read_pickle(data_dir + 'charts_prescriptions.pkl')\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    num_icu_stays = len(icu_pat['ICUSTAY_ID'])\n",
    "    \n",
    "    # static variables\n",
    "    print('Create static array...')\n",
    "    icu_pat = pd.get_dummies(icu_pat, columns = ['ADMISSION_LOCATION', 'INSURANCE', 'MARITAL_STATUS', 'ETHNICITY'])\n",
    "    icu_pat.drop(columns=['ADMISSION_LOCATION_Emergency Room Admit', 'INSURANCE_Medicare', 'MARITAL_STATUS_Married/Life Partner', 'ETHNICITY_White'], inplace=True) # drop reference columns\n",
    "    static_columns = icu_pat.columns.str.contains('AGE|GENDER_M|LOS|NUM_RECENT_ADMISSIONS|ADMISSION_LOCATION|INSURANCE|MARITAL_STATUS|ETHNICITY|PRE_ICU_LOS|ELECTIVE_SURGERY')\n",
    "    static = icu_pat.loc[:, static_columns].values\n",
    "    static_vars = icu_pat.loc[:, static_columns].columns.values.tolist()\n",
    "    \n",
    "    # classification label\n",
    "    print('Create label array...')\n",
    "    label = icu_pat.loc[:, 'POSITIVE'].values\n",
    "    \n",
    "    # diagnoses/procedures and charts/prescriptions\n",
    "    print('Create diagnoses/procedures and charts/prescriptions array...')\n",
    "    dp, dp_times, dict_dp = get_arrays(dp, 'ICD9_CODE', 'DAYS_TO_OUT', 1)\n",
    "    cp, cp_times, dict_cp = get_arrays(cp, 'VALUECAT', 'HOURS_TO_OUT', 0.95)\n",
    "    \n",
    "    # Normalize times\n",
    "    dp_times = dp_times/dp_times.max()\n",
    "    cp_times = cp_times/cp_times.max()\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Split data into train/validate/test...')\n",
    "    # Split patients to avoid data leaks\n",
    "    patients = icu_pat['SUBJECT_ID'].drop_duplicates()\n",
    "    train, validate, test = np.split(patients.sample(frac=1, random_state=123), [int(.9*len(patients)), int(.9*len(patients))])\n",
    "    train_ids = icu_pat['SUBJECT_ID'].isin(train).values\n",
    "    validate_ids = icu_pat['SUBJECT_ID'].isin(validate).values\n",
    "    test_ids = icu_pat['SUBJECT_ID'].isin(test).values\n",
    "    \n",
    "    print('Get patients corresponding to test ids')\n",
    "    test_ids_patients = icu_pat['SUBJECT_ID'].iloc[test_ids].reset_index(drop=True)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Save...')\n",
    "    np.savez(data_dir + 'data_arrays.npz', static=static, static_vars=static_vars, label=label,\n",
    "           dp=dp, cp=cp, dp_times=dp_times, cp_times=cp_times, dict_dp=dict_dp, dict_cp=dict_cp,\n",
    "           train_ids=train_ids, validate_ids=validate_ids, test_ids=test_ids)\n",
    "    # np.savez(data_dir + 'data_dictionaries.npz', dict_dp=dict_dp, dict_cp=dict_cp)\n",
    "    test_ids_patients.to_pickle(data_dir + 'test_ids_patients.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZScZNbROw-N"
   },
   "outputs": [],
   "source": [
    "# create_icu_pat_admit()\n",
    "\n",
    "#-------------------------------------------------------\n",
    "\n",
    "# # dir and function to load raw data\n",
    "# raw_data_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-raw-data>'\n",
    "\n",
    "# def load_raw_data(raw_data_dir):\n",
    "#   # implement this function to load raw data to dataframe/numpy array/tensor\n",
    "#   return None\n",
    "\n",
    "# raw_data = load_raw_data(raw_data_dir)\n",
    "\n",
    "# # calculate statistics\n",
    "# def calculate_stats(raw_data):\n",
    "#   # implement this function to calculate the statistics\n",
    "#   # it is encouraged to print out the results\n",
    "#   return None\n",
    "\n",
    "# # process raw data\n",
    "# def process_data(raw_data):\n",
    "#     # implement this function to process the data as you need\n",
    "#   return None\n",
    "\n",
    "# processed_data = process_data(raw_data)\n",
    "\n",
    "# ''' you can load the processed data directly\n",
    "# processed_data_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-raw-data>'\n",
    "# def load_processed_data(raw_data_dir):\n",
    "#   pass\n",
    "\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  batch_size = 128\n",
    "  num_epochs = 80\n",
    "  dropout_rate = 0.5\n",
    "  patience = 10 # early stopping\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data, type):\n",
    "  # Data\n",
    "  static       = data['static'].astype('float32')\n",
    "  label        = data['label'].astype('float32')\n",
    "  dp           = data['dp'].astype('int64') # diagnoses/procedures\n",
    "  cp           = data['cp'].astype('int64') # charts/prescriptions\n",
    "  dp_times     = data['dp_times'].astype('float32')\n",
    "  cp_times     = data['cp_times'].astype('float32')\n",
    "  train_ids    = data['train_ids']\n",
    "  validate_ids = data['validate_ids']\n",
    "  test_ids     = data['test_ids']  \n",
    "\n",
    "  if (type == 'TRAIN'):\n",
    "    ids = train_ids\n",
    "  elif (type == 'VALIDATE'):\n",
    "    ids = validate_ids\n",
    "  elif (type == 'TEST'):\n",
    "    ids = test_ids\n",
    "  elif (type == 'ALL'):\n",
    "    ids = np.full_like(label, True, dtype=bool)\n",
    "\n",
    "  static   = static[ids, :]\n",
    "  label    = label[ids]\n",
    "  dp       = dp[ids, :]\n",
    "  cp       = cp[ids, :]\n",
    "  dp_times = dp_times[ids, :]\n",
    "  cp_times = cp_times[ids, :]\n",
    "  \n",
    "  return static, dp, cp, dp_times, cp_times, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_trainloader(data, type, shuffle=True, idx=None):\n",
    "  # Data\n",
    "  static, dp, cp, dp_times, cp_times, label = get_data(data, type)\n",
    "\n",
    "  # Bootstrap\n",
    "  if idx is not None:\n",
    "    static, dp, cp, dp_times, cp_times, label = static[idx], dp[idx], cp[idx], dp_times[idx], cp_times[idx], label[idx]\n",
    "\n",
    "  # Compute total batch count\n",
    "  num_batches = len(label) // batch_size\n",
    "  \n",
    "  # Create dataset\n",
    "  dataset = utils.TensorDataset(torch.from_numpy(static), \n",
    "                                torch.from_numpy(dp),\n",
    "                                torch.from_numpy(cp),\n",
    "                                torch.from_numpy(dp_times),\n",
    "                                torch.from_numpy(cp_times),\n",
    "                                torch.from_numpy(label))\n",
    "\n",
    "  # # Create batch queues\n",
    "  trainloader = utils.DataLoader(dataset,\n",
    "                                 batch_size = batch_size, \n",
    "                                 shuffle = shuffle,\n",
    "                                 sampler = None,\n",
    "                                 num_workers = 2,\n",
    "                                 drop_last = True)\n",
    "                                 \n",
    "  # # Weight of positive samples for training\n",
    "  pos_weight = torch.tensor((len(label) - np.sum(label))/np.sum(label))\n",
    "  \n",
    "  return trainloader, num_batches, pos_weight\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3muyDPFPbozY"
   },
   "source": [
    "##   (TODO) Model\n",
    "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
    "  * Model architecture: layer number/size/type, activation function, etc\n",
    "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
    "  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
    "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
    "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will probably just add the models directly from the modules file here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class birnn_concat_time_delta(nn.Module):\n",
    "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
    "      super(birnn_concat_time_delta, self).__init__()\n",
    "      \n",
    "      # Embedding dimensions\n",
    "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))\n",
    "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))\n",
    "\n",
    "      # Embedding layers\n",
    "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
    "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
    "\n",
    "      # GRU layers\n",
    "      self.gru_dp_fw = nn.GRU(input_size=self.embed_dp_dim+1, hidden_size=self.embed_dp_dim+1, num_layers=1, batch_first=True)\n",
    "      self.gru_cp_fw = nn.GRU(input_size=self.embed_cp_dim+1, hidden_size=self.embed_cp_dim+1, num_layers=1, batch_first=True)\n",
    "      self.gru_dp_bw = nn.GRU(input_size=self.embed_dp_dim+1, hidden_size=self.embed_dp_dim+1, num_layers=1, batch_first=True)\n",
    "      self.gru_cp_bw = nn.GRU(input_size=self.embed_cp_dim+1, hidden_size=self.embed_cp_dim+1, num_layers=1, batch_first=True)\n",
    "      \n",
    "      # Fully connected output\n",
    "      self.fc_dp  = nn.Linear(2*(self.embed_dp_dim+1), 1)\n",
    "      self.fc_cp  = nn.Linear(2*(self.embed_cp_dim+1), 1)\n",
    "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
    "      \n",
    "      # Others\n",
    "      self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
    "      # Compute time delta\n",
    "      ## output dim: batch_size x seq_len\n",
    "      dp_t_delta_fw = abs_time_to_delta(dp_t)\n",
    "      cp_t_delta_fw = abs_time_to_delta(cp_t)\n",
    "      dp_t_delta_bw = abs_time_to_delta(torch.flip(dp_t, [1]))\n",
    "      cp_t_delta_bw = abs_time_to_delta(torch.flip(cp_t, [1]))    \n",
    "    \n",
    "      # Embedding\n",
    "      ## output dim: batch_size x seq_len x embedding_dim\n",
    "      embedded_dp_fw = self.embed_dp(dp)\n",
    "      embedded_cp_fw = self.embed_cp(cp)\n",
    "      embedded_dp_bw = torch.flip(embedded_dp_fw, [1])\n",
    "      embedded_cp_bw = torch.flip(embedded_cp_fw, [1])\n",
    "      \n",
    "      # Concatate with time\n",
    "      ## output dim: batch_size x seq_len x (embedding_dim+1)\n",
    "      concat_dp_fw = torch.cat((embedded_dp_fw, torch.unsqueeze(dp_t_delta_fw, dim=-1)), dim=-1)\n",
    "      concat_cp_fw = torch.cat((embedded_cp_fw, torch.unsqueeze(cp_t_delta_fw, dim=-1)), dim=-1)\n",
    "      concat_dp_bw = torch.cat((embedded_dp_bw, torch.unsqueeze(dp_t_delta_bw, dim=-1)), dim=-1)\n",
    "      concat_cp_bw = torch.cat((embedded_cp_bw, torch.unsqueeze(cp_t_delta_bw, dim=-1)), dim=-1)\n",
    "      ## Dropout\n",
    "      concat_dp_fw = self.dropout(concat_dp_fw)\n",
    "      concat_cp_fw = self.dropout(concat_cp_fw)\n",
    "      concat_dp_bw = self.dropout(concat_dp_bw)\n",
    "      concat_cp_bw = self.dropout(concat_cp_bw)\n",
    "      \n",
    "      # GRU\n",
    "      ## output dim rnn:        batch_size x seq_len x (embedding_dim+1)\n",
    "      ## output dim rnn_hidden: batch_size x 1 x (embedding_dim+1)\n",
    "      rnn_dp_fw, rnn_hidden_dp_fw = self.gru_dp_fw(concat_dp_fw)\n",
    "      rnn_cp_fw, rnn_hidden_cp_fw = self.gru_cp_fw(concat_cp_fw)\n",
    "      rnn_dp_bw, rnn_hidden_dp_bw = self.gru_dp_bw(concat_dp_bw)\n",
    "      rnn_cp_bw, rnn_hidden_cp_bw = self.gru_cp_bw(concat_cp_bw)      \n",
    "      ## output dim rnn_hidden: batch_size x (embedding_dim+1)\n",
    "      rnn_hidden_dp_fw = rnn_hidden_dp_fw.view(-1, self.embed_dp_dim+1)\n",
    "      rnn_hidden_cp_fw = rnn_hidden_cp_fw.view(-1, self.embed_cp_dim+1)\n",
    "      rnn_hidden_dp_bw = rnn_hidden_dp_bw.view(-1, self.embed_dp_dim+1)\n",
    "      rnn_hidden_cp_bw = rnn_hidden_cp_bw.view(-1, self.embed_cp_dim+1)\n",
    "      ## concatenate forward and backward: batch_size x 2*(embedding_dim+1)\n",
    "      rnn_hidden_dp = torch.cat((rnn_hidden_dp_fw, rnn_hidden_dp_bw), dim=-1)\n",
    "      rnn_hidden_cp = torch.cat((rnn_hidden_cp_fw, rnn_hidden_cp_bw), dim=-1)\n",
    "      \n",
    "      # Scores\n",
    "      score_dp = self.fc_dp(self.dropout(rnn_hidden_dp))\n",
    "      score_cp = self.fc_cp(self.dropout(rnn_hidden_cp))\n",
    "\n",
    "      # Concatenate to variable collection\n",
    "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
    "      \n",
    "      # Final linear projection\n",
    "      out = self.fc_all(self.dropout(all)).squeeze()\n",
    "\n",
    "      return out, []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUOdeDecay(nn.Module):\n",
    "  \"\"\"\n",
    "  GRU RNN module where the hidden state decays according to an ODE.\n",
    "  (see Rubanova et al. 2019, Latent ODEs for Irregularly-Sampled Time Series)\n",
    "  \n",
    "  Args:\n",
    "    inputs: A `Tensor` with embeddings in the last dimension.\n",
    "    times: A `Tensor` with the same shape as inputs containing the recorded times (but no embedding dimension).\n",
    "\n",
    "  Returns:\n",
    "    outs: Hidden states of the RNN.\n",
    "  \"\"\"\n",
    "  def __init__(self, input_size, hidden_size, bias=True):\n",
    "    super(GRUOdeDecay, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.gru_cell = nn.GRUCell(input_size, hidden_size)\n",
    "    self.decays = nn.Parameter(torch.Tensor(hidden_size)) # exponential decays vector\n",
    "    \n",
    "    # ODE\n",
    "    self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    self.ode_net = ODENet(self.device, self.input_size, self.input_size, output_dim=self.input_size, augment_dim=0, time_dependent=False, non_linearity='softplus', tol=1e-3, adjoint=True)\n",
    "  \n",
    "  def forward(self, inputs, times):\n",
    "    # initializing and then calling cuda() later isn't working for some reason\n",
    "    if torch.cuda.is_available():\n",
    "      hn = torch.zeros(inputs.size(0), self.hidden_size).cuda() # batch_size x hidden_size\n",
    "      outs = torch.zeros(inputs.size(0), inputs.size(1), self.hidden_size).cuda() # batch_size x seq_len x hidden_size\n",
    "    else:\n",
    "      hn = torch.zeros(inputs.size(0), self.hidden_size) # batch_size x hidden_size\n",
    "      outs = torch.zeros(inputs.size(0), inputs.size(1), self.hidden_size) # batch_size x seq_len x hidden_size\n",
    "\n",
    "    # this is slow\n",
    "    for seq in range(inputs.size(1)):\n",
    "      hn = self.gru_cell(inputs[:,seq,:], hn)\n",
    "      outs[:,seq,:] = hn\n",
    "      \n",
    "      times_unique, inverse_indices = torch.unique(times[:,seq], sorted=True, return_inverse=True)\n",
    "      if times_unique.size(0) > 1:\n",
    "        hn = self.ode_net(hn, times_unique)\n",
    "        hn = hn[inverse_indices, torch.arange(0, inverse_indices.size(0)), :]\n",
    "    return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(torch.nn.Module):\n",
    "  \"\"\"\n",
    "  Dot-product attention module.\n",
    "  \n",
    "  Args:\n",
    "    inputs: A `Tensor` with embeddings in the last dimension.\n",
    "    mask: A `Tensor`. Dimensions are the same as inputs but without the embedding dimension.\n",
    "      Values are 0 for 0-padding in the input and 1 elsewhere.\n",
    "\n",
    "  Returns:\n",
    "    outputs: The input `Tensor` whose embeddings in the last dimension have undergone a weighted average.\n",
    "      The second-last dimension of the `Tensor` is removed.\n",
    "    attention_weights: weights given to each embedding.\n",
    "  \"\"\"\n",
    "  def __init__(self, embedding_dim):\n",
    "    super(Attention, self).__init__()\n",
    "    self.context = nn.Parameter(torch.Tensor(embedding_dim)) # context vector\n",
    "    self.linear_hidden = nn.Linear(embedding_dim, embedding_dim)\n",
    "    self.reset_parameters()\n",
    "    \n",
    "  def reset_parameters(self):\n",
    "    nn.init.normal_(self.context)\n",
    "\n",
    "  def forward(self, inputs, mask):\n",
    "    # Hidden representation of embeddings (no change in dimensions)\n",
    "    hidden = torch.tanh(self.linear_hidden(inputs))\n",
    "    # Compute weight of each embedding\n",
    "    importance = torch.sum(hidden * self.context, dim=-1)\n",
    "    importance = importance.masked_fill(mask == 0, -1e9)\n",
    "    # Softmax so that weights sum up to one\n",
    "    attention_weights = F.softmax(importance, dim=-1)\n",
    "    # Weighted sum of embeddings\n",
    "    weighted_projection = inputs * torch.unsqueeze(attention_weights, dim=-1)\n",
    "    # Output\n",
    "    outputs = torch.sum(weighted_projection, dim=-2)\n",
    "    return outputs, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_STEPS = 1000 \n",
    "\n",
    "class ODEFunc(nn.Module):\n",
    "    \"\"\"MLP modeling the derivative of ODE system.\n",
    "    Parameters\n",
    "    ----------\n",
    "    device : torch.device\n",
    "    data_dim : int\n",
    "        Dimension of data.\n",
    "    hidden_dim : int\n",
    "        Dimension of hidden layers.\n",
    "    augment_dim: int\n",
    "        Dimension of augmentation. If 0 does not augment ODE, otherwise augments\n",
    "        it with augment_dim dimensions.\n",
    "    time_dependent : bool\n",
    "        If True adds time as input, making ODE time dependent.\n",
    "    non_linearity : string\n",
    "        One of 'relu' and 'softplus'\n",
    "    \"\"\"\n",
    "    def __init__(self, device, data_dim, hidden_dim, augment_dim=0,\n",
    "                 time_dependent=False, non_linearity='relu'):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        self.device = device\n",
    "        self.augment_dim = augment_dim\n",
    "        self.data_dim = data_dim\n",
    "        self.input_dim = data_dim + augment_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.nfe = 0  # Number of function evaluations\n",
    "        self.time_dependent = time_dependent\n",
    "\n",
    "        if time_dependent:\n",
    "            self.fc1 = nn.Linear(self.input_dim + 1, hidden_dim)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(self.input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, self.input_dim)\n",
    "\n",
    "        if non_linearity == 'relu':\n",
    "            self.non_linearity = nn.ReLU(inplace=True)\n",
    "        elif non_linearity == 'softplus':\n",
    "            self.non_linearity = nn.Softplus()\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        t : torch.Tensor\n",
    "            Current time. Shape (1,).\n",
    "        x : torch.Tensor\n",
    "            Shape (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        # Forward pass of model corresponds to one function evaluation, so\n",
    "        # increment counter\n",
    "        self.nfe += 1\n",
    "        if self.time_dependent:\n",
    "            # Shape (batch_size, 1)\n",
    "            t_vec = torch.ones(x.shape[0], 1).to(self.device) * t\n",
    "            # Shape (batch_size, data_dim + 1)\n",
    "            t_and_x = torch.cat([t_vec, x], 1)\n",
    "            # Shape (batch_size, hidden_dim)\n",
    "            out = self.fc1(t_and_x)\n",
    "        else:\n",
    "            out = self.fc1(x)\n",
    "        out = self.non_linearity(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.non_linearity(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ODEBlock(nn.Module):\n",
    "    \"\"\"Solves ODE defined by odefunc.\n",
    "    Parameters\n",
    "    ----------\n",
    "    device : torch.device\n",
    "    odefunc : ODEFunc instance or anode.conv_models.ConvODEFunc instance\n",
    "        Function defining dynamics of system.\n",
    "    is_conv : bool\n",
    "        If True, treats odefunc as a convolutional model.\n",
    "    tol : float\n",
    "        Error tolerance.\n",
    "    adjoint : bool\n",
    "        If True calculates gradient with adjoint method, otherwise\n",
    "        backpropagates directly through operations of ODE solver.\n",
    "    \"\"\"\n",
    "    def __init__(self, device, odefunc, is_conv=False, tol=1e-3, adjoint=False):\n",
    "        super(ODEBlock, self).__init__()\n",
    "        self.adjoint = adjoint\n",
    "        self.device = device\n",
    "        self.is_conv = is_conv\n",
    "        self.odefunc = odefunc\n",
    "        self.tol = tol\n",
    "\n",
    "    def forward(self, x, eval_times=None):\n",
    "        \"\"\"Solves ODE starting from x.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape (batch_size, self.odefunc.data_dim)\n",
    "        eval_times : None or torch.Tensor\n",
    "            If None, returns solution of ODE at final time t=1. If torch.Tensor\n",
    "            then returns full ODE trajectory evaluated at points in eval_times.\n",
    "        \"\"\"\n",
    "        # Forward pass corresponds to solving ODE, so reset number of function\n",
    "        # evaluations counter\n",
    "        self.odefunc.nfe = 0\n",
    "        \n",
    "        if eval_times is None:\n",
    "            integration_time = torch.tensor([0, 1]).float().type_as(x)\n",
    "        else:\n",
    "            integration_time = eval_times.type_as(x)\n",
    "\n",
    "\n",
    "        if self.odefunc.augment_dim > 0:\n",
    "            if self.is_conv:\n",
    "                # Add augmentation\n",
    "                batch_size, channels, height, width = x.shape\n",
    "                aug = torch.zeros(batch_size, self.odefunc.augment_dim,\n",
    "                                  height, width).to(self.device)\n",
    "                # Shape (batch_size, channels + augment_dim, height, width)\n",
    "                x_aug = torch.cat([x, aug], 1)\n",
    "            else:\n",
    "                # Add augmentation\n",
    "                aug = torch.zeros(x.shape[0], self.odefunc.augment_dim).to(self.device)\n",
    "                # Shape (batch_size, data_dim + augment_dim)\n",
    "                x_aug = torch.cat([x, aug], 1)\n",
    "        else:\n",
    "            x_aug = x\n",
    "\n",
    "        if self.adjoint:\n",
    "            out = odeint_adjoint(self.odefunc, x_aug, integration_time,\n",
    "                                 rtol=self.tol, atol=self.tol, method='euler',\n",
    "                                 options={'max_num_steps': MAX_NUM_STEPS})\n",
    "        else:\n",
    "            out = odeint(self.odefunc, x_aug, integration_time,\n",
    "                         rtol=self.tol, atol=self.tol, method='euler',\n",
    "                         options={'max_num_steps': MAX_NUM_STEPS})\n",
    "\n",
    "        if eval_times is None:\n",
    "            return out[1]  # Return only final time\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class ODENet(nn.Module):\n",
    "    \"\"\"An ODEBlock followed by a Linear layer.\n",
    "    Parameters\n",
    "    ----------\n",
    "    device : torch.device\n",
    "    data_dim : int\n",
    "        Dimension of data.\n",
    "    hidden_dim : int\n",
    "        Dimension of hidden layers.\n",
    "    output_dim : int\n",
    "        Dimension of output after hidden layer. Should be 1 for regression or\n",
    "        num_classes for classification.\n",
    "    augment_dim: int\n",
    "        Dimension of augmentation. If 0 does not augment ODE, otherwise augments\n",
    "        it with augment_dim dimensions.\n",
    "    time_dependent : bool\n",
    "        If True adds time as input, making ODE time dependent.\n",
    "    non_linearity : string\n",
    "        One of 'relu' and 'softplus'\n",
    "    tol : float\n",
    "        Error tolerance.\n",
    "    adjoint : bool\n",
    "        If True calculates gradient with adjoint method, otherwise\n",
    "        backpropagates directly through operations of ODE solver.\n",
    "    \"\"\"\n",
    "    def __init__(self, device, data_dim, hidden_dim, output_dim=1,\n",
    "                 augment_dim=0, time_dependent=False, non_linearity='relu',\n",
    "                 tol=1e-3, adjoint=False):\n",
    "        super(ODENet, self).__init__()\n",
    "        self.device = device\n",
    "        self.data_dim = data_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.augment_dim = augment_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.time_dependent = time_dependent\n",
    "        self.tol = tol\n",
    "\n",
    "        odefunc = ODEFunc(device, data_dim, hidden_dim, augment_dim,\n",
    "                          time_dependent, non_linearity)\n",
    "\n",
    "        self.odeblock = ODEBlock(device, odefunc, tol=tol, adjoint=adjoint)\n",
    "\n",
    "    def forward(self, x, eval_times=None):\n",
    "        features = self.odeblock(x, eval_times)\n",
    "        return features\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class birnn_concat_time_delta_attention(nn.Module):\n",
    "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
    "      super(birnn_concat_time_delta_attention, self).__init__()\n",
    "      \n",
    "      # Embedding dimensions\n",
    "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))\n",
    "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))\n",
    "\n",
    "      # Embedding layers\n",
    "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
    "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
    "\n",
    "      # GRU layers\n",
    "      self.gru_dp_fw = nn.GRU(input_size=self.embed_dp_dim+1, hidden_size=self.embed_dp_dim+1, num_layers=1, batch_first=True)\n",
    "      self.gru_cp_fw = nn.GRU(input_size=self.embed_cp_dim+1, hidden_size=self.embed_cp_dim+1, num_layers=1, batch_first=True)\n",
    "      self.gru_dp_bw = nn.GRU(input_size=self.embed_dp_dim+1, hidden_size=self.embed_dp_dim+1, num_layers=1, batch_first=True)\n",
    "      self.gru_cp_bw = nn.GRU(input_size=self.embed_cp_dim+1, hidden_size=self.embed_cp_dim+1, num_layers=1, batch_first=True)\n",
    "\n",
    "      # Attention layers\n",
    "      self.attention_dp = Attention(embedding_dim=2*(self.embed_dp_dim+1)) #+1 for the concatenated time\n",
    "      self.attention_cp = Attention(embedding_dim=2*(self.embed_cp_dim+1))\n",
    "            \n",
    "      # Fully connected output\n",
    "      self.fc_dp  = nn.Linear(2*(self.embed_dp_dim+1), 1)\n",
    "      self.fc_cp  = nn.Linear(2*(self.embed_cp_dim+1), 1)\n",
    "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
    "      \n",
    "      # Others\n",
    "      self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
    "      # Compute time delta\n",
    "      ## output dim: batch_size x seq_len\n",
    "      dp_t_delta_fw = abs_time_to_delta(dp_t)\n",
    "      cp_t_delta_fw = abs_time_to_delta(cp_t)\n",
    "      dp_t_delta_bw = abs_time_to_delta(torch.flip(dp_t, [1]))\n",
    "      cp_t_delta_bw = abs_time_to_delta(torch.flip(cp_t, [1]))    \n",
    "    \n",
    "      # Embedding\n",
    "      ## output dim: batch_size x seq_len x embedding_dim\n",
    "      embedded_dp_fw = self.embed_dp(dp)\n",
    "      embedded_cp_fw = self.embed_cp(cp)\n",
    "      embedded_dp_bw = torch.flip(embedded_dp_fw, [1])\n",
    "      embedded_cp_bw = torch.flip(embedded_cp_fw, [1])\n",
    "      \n",
    "      # Concatate with time\n",
    "      ## output dim: batch_size x seq_len x (embedding_dim+1)\n",
    "      concat_dp_fw = torch.cat((embedded_dp_fw, torch.unsqueeze(dp_t_delta_fw, dim=-1)), dim=-1)\n",
    "      concat_cp_fw = torch.cat((embedded_cp_fw, torch.unsqueeze(cp_t_delta_fw, dim=-1)), dim=-1)\n",
    "      concat_dp_bw = torch.cat((embedded_dp_bw, torch.unsqueeze(dp_t_delta_bw, dim=-1)), dim=-1)\n",
    "      concat_cp_bw = torch.cat((embedded_cp_bw, torch.unsqueeze(cp_t_delta_bw, dim=-1)), dim=-1)\n",
    "      ## Dropout\n",
    "      concat_dp_fw = self.dropout(concat_dp_fw)\n",
    "      concat_cp_fw = self.dropout(concat_cp_fw)\n",
    "      concat_dp_bw = self.dropout(concat_dp_bw)\n",
    "      concat_cp_bw = self.dropout(concat_cp_bw)\n",
    "      \n",
    "      # GRU\n",
    "      ## output dim rnn:        batch_size x seq_len x (embedding_dim+1)\n",
    "      ## output dim rnn_hidden: batch_size x 1 x (embedding_dim+1)\n",
    "      rnn_dp_fw, rnn_hidden_dp_fw = self.gru_dp_fw(concat_dp_fw)\n",
    "      rnn_cp_fw, rnn_hidden_cp_fw = self.gru_cp_fw(concat_cp_fw)\n",
    "      rnn_dp_bw, rnn_hidden_dp_bw = self.gru_dp_bw(concat_dp_bw)\n",
    "      rnn_cp_bw, rnn_hidden_cp_bw = self.gru_cp_bw(concat_cp_bw)      \n",
    "      # concatenate forward and backward\n",
    "      ## output dim: batch_size x seq_len x 2*(embedding_dim+1)\n",
    "      rnn_dp = torch.cat((rnn_dp_fw, torch.flip(rnn_dp_bw, [1])), dim=-1)\n",
    "      rnn_cp = torch.cat((rnn_cp_fw, torch.flip(rnn_cp_bw, [1])), dim=-1)\n",
    "\n",
    "      # Attention\n",
    "      ## output dim: batch_size x 2*(embedding_dim+1)\n",
    "      attended_dp, weights_dp = self.attention_dp(rnn_dp, (dp > 0).float())\n",
    "      attended_cp, weights_cp = self.attention_cp(rnn_cp, (cp > 0).float())\n",
    "      \n",
    "      # Scores\n",
    "      score_dp = self.fc_dp(self.dropout(attended_dp))\n",
    "      score_cp = self.fc_cp(self.dropout(attended_cp))\n",
    "\n",
    "      # Concatenate to variable collection\n",
    "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
    "      \n",
    "      # Final linear projection\n",
    "      out = self.fc_all(self.dropout(all)).squeeze()\n",
    "\n",
    "      return out, []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class birnn_ode_decay(nn.Module):\n",
    "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
    "      super(birnn_ode_decay, self).__init__()\n",
    "      \n",
    "      # Embedding dimensions\n",
    "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))+1\n",
    "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))+1\n",
    "\n",
    "      # Embedding layers\n",
    "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
    "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
    "\n",
    "      # GRU layers\n",
    "      self.gru_dp_fw = GRUOdeDecay(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim)\n",
    "      self.gru_cp_fw = GRUOdeDecay(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim)\n",
    "      self.gru_dp_bw = GRUOdeDecay(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim)\n",
    "      self.gru_cp_bw = GRUOdeDecay(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim)\n",
    "      \n",
    "      # Fully connected output\n",
    "      self.fc_dp  = nn.Linear(2*self.embed_dp_dim, 1)\n",
    "      self.fc_cp  = nn.Linear(2*self.embed_cp_dim, 1)\n",
    "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
    "      \n",
    "      # Others\n",
    "      self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
    "      # Compute time delta\n",
    "      ## output dim: batch_size x seq_len\n",
    "      dp_t_delta_fw = abs_time_to_delta(dp_t)\n",
    "      cp_t_delta_fw = abs_time_to_delta(cp_t)\n",
    "      ## Round\n",
    "      dp_t_delta_fw = torch.round(100*dp_t_delta_fw)/100\n",
    "      cp_t_delta_fw = torch.round(100*cp_t_delta_fw)/100            \n",
    "      dp_t_delta_bw = abs_time_to_delta(torch.flip(dp_t, [1]))\n",
    "      cp_t_delta_bw = abs_time_to_delta(torch.flip(cp_t, [1]))    \n",
    "    \n",
    "      # Embedding\n",
    "      ## output dim: batch_size x seq_len x embedding_dim\n",
    "      embedded_dp_fw = self.embed_dp(dp)\n",
    "      embedded_cp_fw = self.embed_cp(cp)\n",
    "      embedded_dp_bw = torch.flip(embedded_dp_fw, [1])\n",
    "      embedded_cp_bw = torch.flip(embedded_cp_fw, [1])\n",
    "      ## Dropout\n",
    "      embedded_dp_fw = self.dropout(embedded_dp_fw)\n",
    "      embedded_cp_fw = self.dropout(embedded_cp_fw)\n",
    "      embedded_dp_bw = self.dropout(embedded_dp_bw)\n",
    "      embedded_cp_bw = self.dropout(embedded_cp_bw)\n",
    "      \n",
    "      # GRU\n",
    "      ## output dim rnn:        batch_size x seq_len x embedding_dim\n",
    "      rnn_dp_fw = self.gru_dp_fw(embedded_dp_fw, dp_t_delta_fw)\n",
    "      rnn_cp_fw = self.gru_cp_fw(embedded_cp_fw, cp_t_delta_fw)\n",
    "      rnn_dp_bw = self.gru_dp_bw(embedded_dp_bw, dp_t_delta_bw)\n",
    "      rnn_cp_bw = self.gru_cp_bw(embedded_cp_bw, cp_t_delta_bw)      \n",
    "      ## output dim rnn_hidden: batch_size x embedding_dim\n",
    "      rnn_dp_fw = rnn_dp_fw[:,-1,:]\n",
    "      rnn_cp_fw = rnn_cp_fw[:,-1,:]\n",
    "      rnn_dp_bw = rnn_dp_bw[:,-1,:]\n",
    "      rnn_cp_bw = rnn_cp_bw[:,-1,:]\n",
    "      ## concatenate forward and backward: batch_size x 2*embedding_dim\n",
    "      rnn_dp = torch.cat((rnn_dp_fw, rnn_dp_bw), dim=-1)\n",
    "      rnn_cp = torch.cat((rnn_cp_fw, rnn_cp_bw), dim=-1)\n",
    "      \n",
    "      # Scores\n",
    "      score_dp = self.fc_dp(self.dropout(rnn_dp))\n",
    "      score_cp = self.fc_cp(self.dropout(rnn_cp))\n",
    "\n",
    "      # Concatenate to variable collection\n",
    "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
    "      \n",
    "      # Final linear projection\n",
    "      out = self.fc_all(self.dropout(all)).squeeze()\n",
    "\n",
    "      return out, []      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add training code here\n",
    "def num_static(data):\n",
    "  return data['static_vars'].shape[0]\n",
    "\n",
    "def vocab_sizes(data):\n",
    "  return data['dp'].max()+1, data['cp'].max()+1\n",
    "\n",
    "def abs_time_to_delta(times):\n",
    "  delta = torch.cat((torch.unsqueeze(times[:, 0], dim=-1), times[:, 1:] - times[:, :-1]), dim=1)\n",
    "  delta = torch.clamp(delta, min=0)\n",
    "  return delta\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "print('Load data...')\n",
    "current_dir = os.getcwd()\n",
    "data = np.load(current_dir + '\\data\\data_arrays.npz')\n",
    "# Vocabulary sizes\n",
    "num_static = num_static(data)\n",
    "num_dp_codes, num_cp_codes = vocab_sizes(data)\n",
    "\n",
    "def train(net,model_name, num_epochs):\n",
    "    trainloader, num_batches, pos_weight = get_trainloader(data, 'TRAIN')\n",
    "\n",
    "     \n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    print('Start Train...' + model_name)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = optim.Adam(net.parameters(), lr = 0.001)  \n",
    "    \n",
    "    # Create log dir\n",
    "    logdir = current_dir + '\\\\logdir\\\\' + model_name + '/'\n",
    "    # logdir = hp.logdir + hp.net_variant + '/'\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "    \n",
    "    # Store times\n",
    "    epoch_times = []\n",
    "    \n",
    "    # Train\n",
    "    for epoch in tqdm(range(num_epochs)): \n",
    "    # print('-----------------------------------------')\n",
    "    # print('Epoch: {}'.format(epoch))\n",
    "        net.train()\n",
    "        time_start = time()\n",
    "        for i, (stat, dp, cp, dp_t, cp_t, label) in enumerate(tqdm(trainloader), 0):\n",
    "          # move to GPU if available\n",
    "          stat  = stat.to(device)\n",
    "          dp    = dp.to(device)\n",
    "          cp    = cp.to(device)\n",
    "          dp_t  = dp_t.to(device)\n",
    "          cp_t  = cp_t.to(device)\n",
    "          label = label.to(device)\n",
    "        \n",
    "          # zero the parameter gradients\n",
    "          optimizer.zero_grad()\n",
    "        \n",
    "          # forward + backward + optimize\n",
    "          label_pred, _ = net(stat, dp, cp, dp_t, cp_t)\n",
    "          loss = criterion(label_pred, label)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "    \n",
    "    # timing\n",
    "    time_end = time()\n",
    "    epoch_times.append(time_end-time_start)\n",
    "    \n",
    "    # Save\n",
    "    print('Saving...')\n",
    "    torch.save(net.state_dict(), logdir + 'final_model.pt')\n",
    "    np.savez(logdir + 'epoch_times', epoch_times=epoch_times)\n",
    "    print('Done Train for ' + model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "# model=birnn_concat_time_delta(num_static, num_dp_codes, num_cp_codes)\n",
    "# train(model, 'birnn_concat_time_delta',num_epochs)\n",
    "\n",
    "# model=birnn_concat_time_delta_attention(num_static, num_dp_codes, num_cp_codes)\n",
    "# train(model, 'birnn_concat_time_delta_attention',num_epochs)\n",
    "\n",
    "model=birnn_ode_decay(num_static, num_dp_codes, num_cp_codes)\n",
    "train(model, 'birnn_ode_decay',num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBdVZoTvsSFV"
   },
   "outputs": [],
   "source": [
    "# class my_model():\n",
    "#   # use this class to define your model\n",
    "#   pass\n",
    "\n",
    "# model = my_model()\n",
    "# loss_func = None\n",
    "# optimizer = None\n",
    "\n",
    "# def train_model_one_iter(model, loss_func, optimizer):\n",
    "#   pass\n",
    "\n",
    "# num_epoch = 10\n",
    "# # model training loop: it is better to print the training/validation losses during the training\n",
    "# for i in range(num_epoch):\n",
    "#   train_model_one_iter(model, loss_func, optimizer)\n",
    "#   train_loss, valid_loss = None, None\n",
    "#   print(\"Train Loss: %.2f, Validation Loss: %.2f\" % (train_loss, valid_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX6bCcZNuxmz"
   },
   "source": [
    "# (TODO) Results\n",
    "In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n",
    "\n",
    "Please test and report results for all experiments that you run with:\n",
    "\n",
    "*   specific numbers (accuracy, AUC, RMSE, etc)\n",
    "*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n",
    "\n",
    "\n",
    "The paper's results indicate that the deep learning architectures, particularly those with a recurrent component, outperformed the logistic regression baseline model in predicting ICU readmissions. Here's a summary of the key findings:\n",
    "\n",
    "Baseline Characteristics: The analysis involved 23 static variables, 992 unique ICD-9 diagnosis codes, 298 unique ICD-9 procedure codes, 586 unique medications, and 32 codes related to vital signs. Each patient's electronic medical record (EMR) contained a maximum of 552 ICD-9 diagnosis and procedure codes and 392 medications and vital sign codes associated with the current ICU stay.\n",
    "\n",
    "Model Comparison: The deep learning architectures were evaluated based on average precision, AUROC, F1-score, sensitivity, and specificity. The ODE + RNN model achieved the highest average precision of 0.331, indicating better predictive accuracy than other models, including those with attention layers or based solely on logistic regression (which had an average precision of 0.257).\n",
    "\n",
    "Performance Trends: Models with a recurrent component generally performed better (average precision range: 0.2980.331) than those based solely on attention layers (average precision range: 0.2690.294). This suggests that incorporating recurrent neural network (RNN) components improved the predictive power for ICU readmissions.\n",
    "\n",
    "Therefore, the deep learning architectures, especially those combining recurrent components, showed significantly improved predictive accuracy compared to traditional logistic regression models when predicting ICU readmissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjW9bCkouv8O"
   },
   "outputs": [],
   "source": [
    "# metrics to evaluate my model\n",
    "\n",
    "# plot figures to better show the results\n",
    "\n",
    "# it is better to save the numbers and figures for your presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add testing code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EAWAy_LwHlV"
   },
   "source": [
    "## (TODO) Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOdhGrbwwG71"
   },
   "outputs": [],
   "source": [
    "# compare you model with others\n",
    "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH75TNU71eRH"
   },
   "source": [
    "# (TODO) Discussion\n",
    "\n",
    "In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n",
    "  * Make assessment that the paper is reproducible or not.\n",
    "  * Explain why it is not reproducible if your results are kind negative.\n",
    "  * Describe What was easy and What was difficult during the reproduction.\n",
    "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
    "  * What will you do in next phase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2VDXo5F4Frm"
   },
   "outputs": [],
   "source": [
    "# no code is required for this section\n",
    "'''\n",
    "if you want to use an image outside this notebook for explanaition,\n",
    "you can read and plot it here like the Scope of Reproducibility\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHMI2chl9omn"
   },
   "source": [
    "# (TODO) References\n",
    "1.  Benchmarking Deep Learning Architectures for Predicting Readmission to the ICU and Describing Patients-at-Risk, Sebastiano Barbieri, James Kemp, 2020\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmVuzQ724HbO"
   },
   "source": [
    "# Feel free to add new sections"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
