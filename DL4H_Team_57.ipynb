{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j01aH0PR4Sg-"
   },
   "source": [
    "# (TODO: Will delete this section before submitting draft) FAQ and Attentions\n",
    "* Copy and move this template to your Google Drive. Name your notebook by your team ID (upper-left corner). Don't eidt this original file.\n",
    "* This template covers most questions we want to ask about your reproduction experiment. You don't need to exactly follow the template, however, you should address the questions. Please feel free to customize your report accordingly.\n",
    "* any report must have run-able codes and necessary annotations (in text and code comments).\n",
    "* The notebook is like a demo and only uses small-size data (a subset of original data or processed data), the entire runtime of the notebook including data reading, data process, model training, printing, figure plotting, etc,\n",
    "must be within 8 min, otherwise, you may get penalty on the grade.\n",
    "  * If the raw dataset is too large to be loaded  you can select a subset of data and pre-process the data, then, upload the subset or processed data to Google Drive and load them in this notebook.\n",
    "  * If the whole training is too long to run, you can only set the number of training epoch to a small number, e.g., 3, just show that the training is runable.\n",
    "  * For results model validation, you can train the model outside this notebook in advance, then, load pretrained model and use it for validation (display the figures, print the metrics).\n",
    "* The post-process is important! For post-process of the results,please use plots/figures. The code to summarize results and plot figures may be tedious, however, it won't be waste of time since these figures can be used for presentation. While plotting in code, the figures should have titles or captions if necessary (e.g., title your figure with \"Figure 1. xxxx\")\n",
    "* There is not page limit to your notebook report, you can also use separate notebooks for the report, just make sure your grader can access and run/test them.\n",
    "* If you use outside resources, please refer them (in any formats). Include the links to the resources if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ0sNuMePBXx"
   },
   "source": [
    "# (TODO) Introduction\n",
    "This is an introduction to your report, you should edit this text/mardown section to compose. In this text/markdown, you should introduce:\n",
    "\n",
    "*   Background of the problem\n",
    "  * what type of problem: disease/readmission/mortality prediction,  feature engineeing, data processing, etc\n",
    "  * what is the importance/meaning of solving the problem\n",
    "  * what is the difficulty of the problem\n",
    "  * the state of the art methods and effectiveness.\n",
    "*   Paper explanation\n",
    "  * what did the paper propose\n",
    "  * what is the innovations of the method\n",
    "  * how well the proposed method work (in its own metrics)\n",
    "  * what is the contribution to the reasearch regime (referring the Background above, how important the paper is to the problem).\n",
    "\n",
    "# Background [[1](#References)]\n",
    "Modern machine learning algorithms can extract relevant features from medical data and make predictions for previously unseen patients. Current deep learning architectures used for risk prediction, based on Electronic Medical Records (EMR) data generally employ attention layers on top of recurrent layers. The models allow for state-of-the-art predictions and have enhanced interpretability by virtue of using Attention models. However, they fall short on processing time-series data (diagnosis codes and procedure codes in EMR) that was sampled at irregular time intervals. Predictive accuracy was comparable across neural network architectures. Groups of patients suffering from infectious complications, with chronic or progressive conditions, and for whom standard medical care was not suitable. Attention-based networks may be preferable to recurrent networks if an interpretable model is required, at only marginal cost in predictive accuracy.\n",
    "\n",
    "# Paper explanation\n",
    "The paper our project was based on proposed evaluating different deep learning architectures for predicting ICU readmission and describing patients at risk. The innovation of the method lies in its comparison of various deep learning architectures. This approach goes beyond traditional machine learning methods and explores the potential of state-of-the-art deep learning techniques in healthcare analytics. Model performance was gauged using metrics such as accuracy, precision, recall, and AUC. The effectiveness of the method would depend on how well it performed compared to the baseline models or existing clinical scoring systems in predicting ICU readmissions and providing interpretable insights into patient risk factors. The contribution of the research lies in its exploration of advanced deep learning techniques in a critical healthcare domain. By benchmarking and comparing different architectures, the paper provides valuable insights into the potential of deep learning for improving predictive accuracy and interpretability in ICU readmission prediction, thereby contributing to the ongoing research in healthcare analytics and patient care optimization.  The paper also concluded that several different models were viable with no one approach being by far the best, and choosing a model could depend on factors such as the need for interpretability of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uygL9tTPSVHB"
   },
   "source": [
    "# Scope of Reproducibility:\n",
    "\n",
    "The hypothesis of the paper is below:\n",
    "*   The utilization of various deep learning architectures, including LSTM, CNN, and Transformer models, will lead to improved accuracy in predicting readmission to the Intensive Care Unit (ICU) and provide valuable insights into identifying patients-at-risk compared to traditional machine learning methods or clinical scoring systems.\n",
    "\n",
    "The goal of the project is to explore different approaches to time embeddings (MCE with time aware attention, embedding layers with concatenated elapsed times and embedding layers + neural ODEâ€™s), across different deep learning architecture combinations. Our project is based on an existing study that concluded Neural ODEs applied to code embeddings did generally resulted in improved performance, suggesting that they may constitute a building block of interest for neural networks processing not only continuous time series, but also timestamped codes. \n",
    "\n",
    "We would assume that the conclusion in the study holds true and through our implementation of the models attempt to prove that out.  From a computational perspective, the size of the MIMIC III tables involved and runtime of the scripts lead us to believe that these results can be reproduced using personal computers without the need for external memory or computational power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWAHJ_1CdtaA"
   },
   "source": [
    "# (TODO) Methodology\n",
    "\n",
    "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
    "\n",
    "The methodology at least contains two subsections **data** and **model** in your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yu61Jp1xrnKk"
   },
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchdiffeq import odeint, odeint_adjoint\n",
    "import os\n",
    "from time import time\n",
    "import scipy.stats as st\n",
    "from sklearn.metrics import *\n",
    "import torch.utils.data as utils\n",
    "\n",
    "from torch.utils.data.dataset import (\n",
    "    ChainDataset,\n",
    "    ConcatDataset,\n",
    "    Dataset,\n",
    "    IterableDataset,\n",
    "    StackDataset,\n",
    "    Subset,\n",
    "    TensorDataset,\n",
    "    random_split,\n",
    ")\n",
    "\n",
    "from torch.utils.data.dataloader import (\n",
    "    DataLoader,\n",
    "    _DatasetKind,\n",
    "    get_worker_info,\n",
    "    default_collate,\n",
    "    default_convert,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NbPHUTMbkD3"
   },
   "source": [
    "## Data\n",
    "The data for this project comes from the MIMIC III Clinical Database. Specifically, we use the following tables which can be downloaded directly from physionet (https://physionet.org/content/mimiciii/1.4/) once becoming a credentialed user via training course:\n",
    "* ADMISSIONS\n",
    "* CHARTEVENTS\n",
    "* D_ITEMS\n",
    "* DIAGNOSES_ICD\n",
    "* ICUSTAYS\n",
    "* OUTPUTEVENTS\n",
    "* PATIENTS\n",
    "* PRESCRIPTIONS\n",
    "* PROCEDURES_ICD\n",
    "* SERVICES\n",
    "  \n",
    "After downloading the compressed tables from physionet, we unzip the files locally and run a series of preprocessing steps that will ultimately lead to data arrays that we will use to train and evaluate the models. The preprocessing functions are shown in the Preprocessing Code section below. In addition to the MIMIC III tables listed above, the preprocessing functions also generate intermediate data files that can be used in later preprocessing steps that will ultimately lead to constructing the training/testing data arrays.  The implementation for the preprocessing code largely reuses the preprocessing code provided in the GitHub repo of the original paper (https://github.com/sebbarb/time_aware_attention), with some additional comments added and small fixes due to package and python versioning.\n",
    "\n",
    "All intermediate files that could be compressed to a small enough size to be uploaded to Github are in the data folder of the parent repo.  The original MIMIC III tables which are required for running the preprocessing steps cannot be uploaded to the parent repo, so the preprocessing code shown below represents what works for our team when we have downloaded the files locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzVUQS0CHry0"
   },
   "source": [
    "### Descriptive Statistics\n",
    "\n",
    "Due to the limitations of the pyhealth library on the MIMIC3Dataset, we were not able to get full descriptive statistics of every table used in this project.  However, for tables that could be parsed using the library, the results are shown in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pyhealth if not already installed\n",
    "# !pip3 install pyhealth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStatistics of base dataset (dev=False):\\n\\t- Dataset: MIMIC3Dataset\\n\\t- Number of patients: 46520\\n\\t- Number of visits: 58976\\n\\t- Number of visits per patient: 1.2678\\n\\t- Number of events per visit in DIAGNOSES_ICD: 11.0384\\n\\t- Number of events per visit in PRESCRIPTIONS: 70.4013\\n\\t- Number of events per visit in PROCEDURES_ICD: 4.0711\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pyhealth.datasets import MIMIC3Dataset\n",
    "\n",
    "# ADMISSIONS and PATIENTS are part of our dataset but don't need to be included in this code since they are parsed by default\n",
    "# CHARTEVENTS, D_ITEMS, ICUSTAYS, OUTPUTEVENTS, PATIENTS, and SERVICES do not have a parser in this library as indicated\n",
    "# in the documentation: https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC3Dataset.html\n",
    "\n",
    "# The code below can be uncommented to generate statistics about a subset of tables used in this project.  The results are pasted in the comment at the bottom.\n",
    "# dataset = MIMIC3Dataset(\n",
    "#     root=\"../MIMIC-III Clinical Database/uncompressed/\",\n",
    "#     tables=[\"DIAGNOSES_ICD\", \"PRESCRIPTIONS\", \"PROCEDURES_ICD\"],\n",
    "# )\n",
    "# dataset.stat()\n",
    "# dataset.info()\n",
    "\n",
    "'''\n",
    "Statistics of base dataset (dev=False):\n",
    "\t- Dataset: MIMIC3Dataset\n",
    "\t- Number of patients: 46520\n",
    "\t- Number of visits: 58976\n",
    "\t- Number of visits per patient: 1.2678\n",
    "\t- Number of events per visit in DIAGNOSES_ICD: 11.0384\n",
    "\t- Number of events per visit in PRESCRIPTIONS: 70.4013\n",
    "\t- Number of events per visit in PROCEDURES_ICD: 4.0711\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzVUQS0CHry0"
   },
   "source": [
    "### Preprocessing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: MIMIC-III ICUSTAYS, PATIENTS, ADMISSIONS, and SERVICES tables to combine into an intermediate dataset that joins these tables together.\n",
    "mimic_dir = '../MIMIC-III Clinical Database/uncompressed/'\n",
    "data_dir = './data/'\n",
    "\n",
    "min_count = 100 # words whose occurred less than min_cnt are encoded as OTHER\n",
    "\n",
    "def create_icu_pat_admit():\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    # Load icustays table\n",
    "    # Table purpose: Defines each ICUSTAY_ID in the database, i.e. defines a single ICU stay\n",
    "    print('Load ICU stays...')\n",
    "    dtype = {'SUBJECT_ID': 'int32',\n",
    "           'HADM_ID': 'int32',\n",
    "           'ICUSTAY_ID': 'int32',\n",
    "           'INTIME': 'str',\n",
    "           'OUTTIME': 'str',\n",
    "           'LOS': 'float32'}\n",
    "    parse_dates = ['INTIME', 'OUTTIME']\n",
    "    icustays = pd.read_csv(mimic_dir + 'ICUSTAYS.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    # Load patients table\n",
    "    # Table purpose: Contains all charted data for all patients.\n",
    "    print('Load patients...')\n",
    "    dtype = {'SUBJECT_ID': 'int32',\n",
    "           'GENDER': 'str',\n",
    "           'DOB': 'str',\n",
    "           'DOD': 'str'}\n",
    "    parse_dates = ['DOB', 'DOD']\n",
    "    patients = pd.read_csv(mimic_dir + 'PATIENTS.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)  \n",
    "    \n",
    "    # Adjust shifted DOBs for older patients (median imputation)\n",
    "    old_patient = patients['DOB'].dt.year < 2000\n",
    "    date_offset = pd.DateOffset(years=(300-91), days=(-0.4*365))\n",
    "    patients['DOB'][old_patient] = patients['DOB'][old_patient].apply(lambda x: x + date_offset)\n",
    "    \n",
    "    # Replace GENDER by dummy binary column \n",
    "    patients = pd.get_dummies(patients, columns = ['GENDER'], drop_first=True)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    print('Load admissions...')\n",
    "    # Load admissions table\n",
    "    # Table purpose: Define a patients hospital admission, HADM_ID.\n",
    "    dtype = {'SUBJECT_ID': 'int32', \n",
    "           'HADM_ID': 'int32',\n",
    "           'ADMISSION_LOCATION': 'str',\n",
    "           'INSURANCE': 'str',\n",
    "           'MARITAL_STATUS': 'str',\n",
    "           'ETHNICITY': 'str',\n",
    "           'ADMITTIME': 'str',\n",
    "           'ADMISSION_TYPE': 'str'}\n",
    "    parse_dates = ['ADMITTIME']\n",
    "    admissions = pd.read_csv(mimic_dir + 'ADMISSIONS.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    print('Load services...')\n",
    "    # Load services table\n",
    "    # Table purpose: Lists services that a patient was admitted/transferred under.\n",
    "    dtype = {'SUBJECT_ID': 'int32', \n",
    "           'HADM_ID': 'int32',\n",
    "           'TRANSFERTIME': 'str',\n",
    "           'CURR_SERVICE': 'str'}\n",
    "    parse_dates = ['TRANSFERTIME']\n",
    "    services = pd.read_csv(mimic_dir + 'SERVICES.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    # Link icustays and patients tables\n",
    "    print('Link icustays and patients tables...')\n",
    "    icu_pat = pd.merge(icustays, patients, how='inner', on='SUBJECT_ID')\n",
    "    icu_pat.sort_values(by=['SUBJECT_ID', 'OUTTIME'], ascending=[True, False], inplace=True)\n",
    "    assert len(icu_pat['SUBJECT_ID'].unique()) == 46476\n",
    "    assert len(icu_pat['ICUSTAY_ID'].unique()) == 61532\n",
    "    \n",
    "    # Exclude icu stays during which patient died\n",
    "    icu_pat = icu_pat[~(icu_pat['DOD'] <= icu_pat['OUTTIME'])]\n",
    "    assert len(icu_pat['SUBJECT_ID'].unique()) == 43126\n",
    "    assert len(icu_pat['ICUSTAY_ID'].unique()) == 56745\n",
    "    \n",
    "    # Determine number of icu discharges in the last 365 days\n",
    "    print('Compute number of recent admissions...')\n",
    "    icu_pat['NUM_RECENT_ADMISSIONS'] = 0\n",
    "    for name, group in tqdm(icu_pat.groupby(['SUBJECT_ID'])):\n",
    "        for index, row in group.iterrows():\n",
    "            days_diff = (row['OUTTIME']-group['OUTTIME']).dt.days\n",
    "            icu_pat.at[index, 'NUM_RECENT_ADMISSIONS'] = len(group[(days_diff > 0) & (days_diff <=365)])\n",
    "    \n",
    "    # Create age variable and exclude patients < 18 y.o.\n",
    "    icu_pat['AGE'] = (icu_pat['OUTTIME'] - icu_pat['DOB']).dt.days/365.\n",
    "    icu_pat = icu_pat[icu_pat['AGE'] >= 18]\n",
    "    assert len(icu_pat['SUBJECT_ID'].unique()) == 35233\n",
    "    assert len(icu_pat['ICUSTAY_ID'].unique()) == 48616\n",
    "    \n",
    "    # Time to next admission (discharge to admission!)\n",
    "    icu_pat['DAYS_TO_NEXT'] = (icu_pat.groupby(['SUBJECT_ID']).shift(1)['INTIME'] - icu_pat['OUTTIME']).dt.days\n",
    "    \n",
    "    # Add early readmission flag (less than 30 days after discharge)\n",
    "    icu_pat['POSITIVE'] = (icu_pat['DAYS_TO_NEXT'] <= 30)\n",
    "    assert icu_pat['POSITIVE'].sum() == 5495\n",
    "    \n",
    "    # Add early death flag (less than 30 days after discharge)\n",
    "    early_death = ((icu_pat['DOD'] - icu_pat['OUTTIME']).dt.days <= 30)\n",
    "    assert early_death.sum() == 3795\n",
    "    \n",
    "    # Censor negative patients who died within less than 30 days after discharge (no chance of readmission)\n",
    "    icu_pat = icu_pat[icu_pat['POSITIVE'] | ~early_death]\n",
    "    assert len(icu_pat['SUBJECT_ID'].unique()) == 33150\n",
    "    assert len(icu_pat['ICUSTAY_ID'].unique()) == 45298\n",
    "    \n",
    "    # Clean up\n",
    "    icu_pat.drop(columns=['DOB', 'DOD', 'DAYS_TO_NEXT'], inplace=True)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    # Link icu_pat and admissions tables\n",
    "    print('Link icu_pat and admissions tables...')\n",
    "    icu_pat_admit = pd.merge(icu_pat, admissions, how='left', on=['SUBJECT_ID', 'HADM_ID'])\n",
    "    print(icu_pat_admit.isnull().sum())\n",
    "    \n",
    "    print('Some data cleaning on admissions...')\n",
    "    icu_pat_admit.loc[icu_pat_admit['ETHNICITY'].str.contains('WHITE'), 'ETHNICITY']    = 'WHITE'\n",
    "    icu_pat_admit.loc[icu_pat_admit['ETHNICITY'].str.contains('BLACK'), 'ETHNICITY']    = 'BLACK/AFRICAN AMERICAN'\n",
    "    icu_pat_admit.loc[icu_pat_admit['ETHNICITY'].str.contains('ASIAN'), 'ETHNICITY']    = 'ASIAN'\n",
    "    icu_pat_admit.loc[icu_pat_admit['ETHNICITY'].str.contains('HISPANIC'), 'ETHNICITY'] = 'HISPANIC/LATINO'\n",
    "    icu_pat_admit.loc[icu_pat_admit['ETHNICITY'].str.contains('DECLINED'), 'ETHNICITY'] = 'OTHER/UNKNOWN'\n",
    "    icu_pat_admit.loc[icu_pat_admit['ETHNICITY'].str.contains('MULTI'), 'ETHNICITY']    = 'OTHER/UNKNOWN'\n",
    "    icu_pat_admit.loc[icu_pat_admit['ETHNICITY'].str.contains('UNKNOWN'), 'ETHNICITY']  = 'OTHER/UNKNOWN'\n",
    "    icu_pat_admit.loc[icu_pat_admit['ETHNICITY'].str.contains('OTHER'), 'ETHNICITY']  = 'OTHER/UNKNOWN'\n",
    "    \n",
    "    icu_pat_admit['MARITAL_STATUS'].fillna('UNKNOWN', inplace=True)\n",
    "    icu_pat_admit.loc[icu_pat_admit['MARITAL_STATUS'].str.contains('MARRIED'), 'MARITAL_STATUS']      = 'MARRIED/LIFE PARTNER'\n",
    "    icu_pat_admit.loc[icu_pat_admit['MARITAL_STATUS'].str.contains('LIFE PARTNER'), 'MARITAL_STATUS'] = 'MARRIED/LIFE PARTNER'\n",
    "    icu_pat_admit.loc[icu_pat_admit['MARITAL_STATUS'].str.contains('WIDOWED'), 'MARITAL_STATUS']      = 'WIDOWED/DIVORCED/SEPARATED'\n",
    "    icu_pat_admit.loc[icu_pat_admit['MARITAL_STATUS'].str.contains('DIVORCED'), 'MARITAL_STATUS']     = 'WIDOWED/DIVORCED/SEPARATED'\n",
    "    icu_pat_admit.loc[icu_pat_admit['MARITAL_STATUS'].str.contains('SEPARATED'), 'MARITAL_STATUS']    = 'WIDOWED/DIVORCED/SEPARATED'\n",
    "    icu_pat_admit.loc[icu_pat_admit['MARITAL_STATUS'].str.contains('UNKNOWN'), 'MARITAL_STATUS']      = 'OTHER/UNKNOWN'\n",
    "    \n",
    "    columns_to_mask = ['ADMISSION_LOCATION',\n",
    "                     'INSURANCE',\n",
    "                     'MARITAL_STATUS',\n",
    "                     'ETHNICITY']\n",
    "    icu_pat_admit = icu_pat_admit.apply(lambda x: x.mask(x.map(x.value_counts()) < min_count, 'OTHER/UNKNOWN') if x.name in columns_to_mask else x)                   \n",
    "    icu_pat_admit = icu_pat_admit.apply(lambda x: x.str.title() if x.name in columns_to_mask else x)\n",
    "    \n",
    "    # Compute pre-ICU length of stay in fractional days\n",
    "    icu_pat_admit['PRE_ICU_LOS'] = (icu_pat_admit['INTIME'] - icu_pat_admit['ADMITTIME']) / np.timedelta64(1, 'D')\n",
    "    icu_pat_admit.loc[icu_pat_admit['PRE_ICU_LOS']<0, 'PRE_ICU_LOS'] = 0\n",
    "    \n",
    "    # Clean up\n",
    "    icu_pat_admit.drop(columns=['ADMITTIME'], inplace=True)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    # Link services table\n",
    "    # Keep first service only\n",
    "    services.sort_values(by=['HADM_ID', 'TRANSFERTIME'], ascending=True, inplace=True)\n",
    "    services = services.groupby(['HADM_ID']).nth(0).reset_index()\n",
    "    \n",
    "    # Check if first service is a surgery\n",
    "    services['SURGERY'] = services['CURR_SERVICE'].str.contains('SURG') | (services['CURR_SERVICE'] == 'ORTHO')\n",
    "    \n",
    "    print('Link services table...')  \n",
    "    icu_pat_admit = pd.merge(icu_pat_admit, services, how='left', on=['SUBJECT_ID', 'HADM_ID'])\n",
    "    \n",
    "    # Get elective surgery admissions\n",
    "    icu_pat_admit['ELECTIVE_SURGERY'] = ((icu_pat_admit['ADMISSION_TYPE'] == 'ELECTIVE') & icu_pat_admit['SURGERY']).astype(int)\n",
    "    \n",
    "    # Clean up\n",
    "    icu_pat_admit.drop(columns=['TRANSFERTIME', 'CURR_SERVICE', 'ADMISSION_TYPE', 'SURGERY'], inplace=True)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    # Baseline characteristics table\n",
    "    pos = icu_pat_admit[icu_pat_admit['POSITIVE']==1]\n",
    "    neg = icu_pat_admit[icu_pat_admit['POSITIVE']==0]\n",
    "    print('Total pos {}'.format(len(pos)))\n",
    "    print('Total neg {}'.format(len(neg)))\n",
    "    print(pos['LOS'].describe())\n",
    "    print(neg['LOS'].describe())\n",
    "    print((pos['PRE_ICU_LOS']).describe())\n",
    "    print((neg['PRE_ICU_LOS']).describe())\n",
    "    pd.set_option('display.precision', 1)\n",
    "    print(pos['AGE'].describe())\n",
    "    print(neg['AGE'].describe())\n",
    "    print(pos['NUM_RECENT_ADMISSIONS'].describe())\n",
    "    print(neg['NUM_RECENT_ADMISSIONS'].describe())\n",
    "    print(pd.DataFrame({'COUNTS': pos['GENDER_M'].value_counts(), 'PERC': pos['GENDER_M'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': neg['GENDER_M'].value_counts(), 'PERC': neg['GENDER_M'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': pos['ADMISSION_LOCATION'].value_counts(), 'PERC': pos['ADMISSION_LOCATION'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': neg['ADMISSION_LOCATION'].value_counts(), 'PERC': neg['ADMISSION_LOCATION'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': pos['INSURANCE'].value_counts(), 'PERC': pos['INSURANCE'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': neg['INSURANCE'].value_counts(), 'PERC': neg['INSURANCE'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': pos['MARITAL_STATUS'].value_counts(), 'PERC': pos['MARITAL_STATUS'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': neg['MARITAL_STATUS'].value_counts(), 'PERC': neg['MARITAL_STATUS'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': pos['ETHNICITY'].value_counts(), 'PERC': pos['ETHNICITY'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': neg['ETHNICITY'].value_counts(), 'PERC': neg['ETHNICITY'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': pos['ELECTIVE_SURGERY'].value_counts(), 'PERC': pos['ELECTIVE_SURGERY'].value_counts(normalize=True)*100}))\n",
    "    print(pd.DataFrame({'COUNTS': neg['ELECTIVE_SURGERY'].value_counts(), 'PERC': neg['ELECTIVE_SURGERY'].value_counts(normalize=True)*100}))  \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Save...')\n",
    "    assert len(icu_pat_admit) == 45298\n",
    "    icu_pat_admit.sort_values(by='ICUSTAY_ID', ascending=True, inplace=True)\n",
    "    icu_pat_admit.to_pickle(data_dir + 'icu_pat_admit.pkl')\n",
    "    icu_pat_admit.to_csv(data_dir + 'icu_pat_admit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: reduce chartevents size by removing implausible measurements, but also add human readable labels to indicate what the chart data represents\n",
    "# for each entry using data in D_ITEMS\n",
    "def reduce_chart_events():\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    \n",
    "    # Relevant ITEMIDs\n",
    "    gcs_eye_opening          = [184, 220739, 226756, 227011]\n",
    "    gcs_verbal_response      = [723, 223900, 226758, 227014]\n",
    "    gcs_motor_response       = [454, 223901, 226757, 227012]\n",
    "    gcs_total                = [198, 226755]\n",
    "    diastolic_blood_pressure = [8364, 8368, 8440, 8441, 8502, 8503, 8506, 8555, 220051, 220180, 224643, 225310, 227242]\n",
    "    systolic_blood_pressure  = [   6,   51,  442,  455, 3313, 3315, 3321, 6701, 220050, 220179, 224167, 225309, 227243]\n",
    "    mean_blood_pressure      = [52, 443, 456, 2293, 2294, 2647, 3312, 3314, 3320, 6590, 6702, 6927, 7620, 220052, 220181, 225312]\n",
    "    heart_rate               = [211, 220045, 227018]\n",
    "    fraction_inspired_oxygen = [189, 190, 727, 1040, 1206, 1863, 2518, 2981, 3420, 3422, 7018, 7041, 7570, 223835, 226754, 227009, 227010]\n",
    "    respiratory_rate         = [614, 615, 618, 619, 651, 653, 1884, 3603, 6749, 7884, 8113, 220210, 224422, 224688, 224689, 224690, 226774, 227050]\n",
    "    body_temperature         = [676, 677, 678, 679, 3652, 3654, 6643, 223761, 223762, 226778, 227054]\n",
    "    weight                   = [763, 3580, 3581, 3582, 3693, 224639, 226512, 226531]\n",
    "    height                   = [1394, 226707, 226730]\n",
    "    \n",
    "    def inch_to_cm(value):\n",
    "        return value*2.54\n",
    "    \n",
    "    def lb_to_kg(value):\n",
    "        return value/2.205\n",
    "    \n",
    "    def oz_to_kg(value):\n",
    "        return value/35.274\n",
    "    \n",
    "    def f_to_c(value):\n",
    "        return (value-32)*5/9\n",
    "    \n",
    "    def frac_to_perc(value):\n",
    "        return value*100\n",
    "    \n",
    "    # Relevant ITEMIDs\n",
    "    body_temperature_F       = [678, 679, 3652, 3654, 6643, 223761, 226778, 227054]\n",
    "    weight_lb                = [3581, 226531]\n",
    "    weight_oz                = [3582]\n",
    "    height_inch              = [1394, 226707]\n",
    "    \n",
    "    relevant_ids = (gcs_eye_opening + gcs_verbal_response + gcs_motor_response + gcs_total + mean_blood_pressure + \n",
    "                  heart_rate + fraction_inspired_oxygen + respiratory_rate + body_temperature + weight + height)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    print('Load item definitions')\n",
    "    dtype = {'ITEMID': 'int32',\n",
    "           'LABEL': 'str',\n",
    "           'UNITNAME': 'str'}\n",
    "    defs = pd.read_csv(mimic_dir + 'D_ITEMS.csv', usecols=dtype.keys(), dtype=dtype)\n",
    "    print('GCS_EYE_OPENING')\n",
    "    print(defs[defs['ITEMID'].isin(gcs_eye_opening)])\n",
    "    print('GCS_VERBAL_RESPONSE')\n",
    "    print(defs[defs['ITEMID'].isin(gcs_verbal_response)])\n",
    "    print('GCS_MOTOR_RESPONSE')\n",
    "    print(defs[defs['ITEMID'].isin(gcs_motor_response)])\n",
    "    print('GCS_TOTAL')\n",
    "    print(defs[defs['ITEMID'].isin(gcs_total)])\n",
    "    print('DIASTOLIC_BP')\n",
    "    print(defs[defs['ITEMID'].isin(diastolic_blood_pressure)])\n",
    "    print('SYSTOLIC_BP')\n",
    "    print(defs[defs['ITEMID'].isin(systolic_blood_pressure)])\n",
    "    print('MEAN_BP')\n",
    "    print(defs[defs['ITEMID'].isin(mean_blood_pressure)])\n",
    "    print('HEART_RATE')\n",
    "    print(defs[defs['ITEMID'].isin(heart_rate)])\n",
    "    print('FRACTION_INSPIRED_OXYGEN')\n",
    "    print(defs[defs['ITEMID'].isin(fraction_inspired_oxygen)])\n",
    "    print('RESPIRATORY_RATE')\n",
    "    print(defs[defs['ITEMID'].isin(respiratory_rate)])\n",
    "    print('BODY_TEMPERATURE')\n",
    "    print(defs[defs['ITEMID'].isin(body_temperature)])\n",
    "    print('WEIGHT')\n",
    "    print(defs[defs['ITEMID'].isin(weight)])\n",
    "    print('HEIGHT')\n",
    "    print(defs[defs['ITEMID'].isin(height)])\n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Loading Chart Events')\n",
    "    dtype = {'SUBJECT_ID': 'int32',\n",
    "           'HADM_ID': 'int32',\n",
    "           'ICUSTAY_ID': 'str',\n",
    "           'ITEMID': 'int32',\n",
    "           'CHARTTIME': 'str',\n",
    "           'VALUENUM': 'float32'}\n",
    "    parse_dates = ['CHARTTIME']\n",
    "    # Load chartevents table\n",
    "    # Table purpose: Contains all charted data for all patients.\n",
    "    chunksize = 1000000\n",
    "    i = 0\n",
    "    # Not parsing dates\n",
    "    for df in tqdm(pd.read_csv(mimic_dir + 'CHARTEVENTS.csv', usecols=dtype.keys(), dtype=dtype, chunksize=chunksize)):\n",
    "        df = df[df['ICUSTAY_ID'].notna() & df['VALUENUM'].notna() & (df['ITEMID'].isin(relevant_ids)) & (df['VALUENUM'] > 0)]\n",
    "        # convert units\n",
    "        df.loc[df['ITEMID'].isin(body_temperature_F), 'VALUENUM'] = f_to_c(df[df['ITEMID'].isin(body_temperature_F)].VALUENUM)\n",
    "        df.loc[df['ITEMID'].isin(weight_lb), 'VALUENUM'] = lb_to_kg(df[df['ITEMID'].isin(weight_lb)].VALUENUM)\n",
    "        df.loc[df['ITEMID'].isin(weight_oz), 'VALUENUM'] = oz_to_kg(df[df['ITEMID'].isin(weight_oz)].VALUENUM)\n",
    "        df.loc[df['ITEMID'].isin(height_inch), 'VALUENUM'] = inch_to_cm(df[df['ITEMID'].isin(height_inch)].VALUENUM)\n",
    "        df.loc[(df['ITEMID'].isin(fraction_inspired_oxygen)) & (df['VALUENUM']<=1), 'VALUENUM'] = frac_to_perc(df[(df['ITEMID'].isin(fraction_inspired_oxygen)) & (df['VALUENUM']<=1)].VALUENUM)\n",
    "        # remove implausible measurements\n",
    "        df = df[~(df['ITEMID'].isin(gcs_total) & (df.VALUENUM < 3))]\n",
    "        df = df[~(df['ITEMID'].isin(diastolic_blood_pressure + systolic_blood_pressure + mean_blood_pressure) & (df.VALUENUM > 250))]\n",
    "        df = df[~(df['ITEMID'].isin(heart_rate) & ((df.VALUENUM < 1) | (df.VALUENUM > 250)))]\n",
    "        df = df[~(df['ITEMID'].isin(fraction_inspired_oxygen) & (df.VALUENUM > 100))]\n",
    "        df = df[~(df['ITEMID'].isin(respiratory_rate) & ((df.VALUENUM < 1) | (df.VALUENUM > 100)))]\n",
    "        df = df[~(df['ITEMID'].isin(body_temperature) & (df.VALUENUM > 50))]\n",
    "        df = df[~(df['ITEMID'].isin(weight) & (df.VALUENUM > 700))]\n",
    "        df = df[~(df['ITEMID'].isin(height) & (df.VALUENUM > 300))]\n",
    "        df = df[df['VALUENUM'] > 0]\n",
    "        # label\n",
    "        df['CE_TYPE'] = ''\n",
    "        df.loc[df['ITEMID'].isin(gcs_eye_opening), 'CE_TYPE'] = 'GCS_EYE_OPENING'\n",
    "        df.loc[df['ITEMID'].isin(gcs_verbal_response), 'CE_TYPE'] = 'GCS_VERBAL_RESPONSE'\n",
    "        df.loc[df['ITEMID'].isin(gcs_motor_response), 'CE_TYPE'] = 'GCS_MOTOR_RESPONSE'\n",
    "        df.loc[df['ITEMID'].isin(gcs_total), 'CE_TYPE'] = 'GCS_TOTAL'\n",
    "        df.loc[df['ITEMID'].isin(diastolic_blood_pressure), 'CE_TYPE'] = 'DIASTOLIC_BP'\n",
    "        df.loc[df['ITEMID'].isin(systolic_blood_pressure), 'CE_TYPE'] = 'SYSTOLIC_BP'\n",
    "        df.loc[df['ITEMID'].isin(mean_blood_pressure), 'CE_TYPE'] = 'MEAN_BP'\n",
    "        df.loc[df['ITEMID'].isin(heart_rate), 'CE_TYPE'] = 'HEART_RATE'\n",
    "        df.loc[df['ITEMID'].isin(fraction_inspired_oxygen), 'CE_TYPE'] = 'FRACTION_INSPIRED_OXYGEN'\n",
    "        df.loc[df['ITEMID'].isin(respiratory_rate), 'CE_TYPE'] = 'RESPIRATORY_RATE'\n",
    "        df.loc[df['ITEMID'].isin(body_temperature), 'CE_TYPE'] = 'BODY_TEMPERATURE'\n",
    "        df.loc[df['ITEMID'].isin(weight), 'CE_TYPE'] = 'WEIGHT'\n",
    "        df.loc[df['ITEMID'].isin(height), 'CE_TYPE'] = 'HEIGHT'    \n",
    "        df.drop(columns=['ITEMID'], inplace=True)\n",
    "        \n",
    "        # save\n",
    "        if i == 0:\n",
    "            df.to_csv(data_dir + 'chartevents_reduced.csv', index=False)\n",
    "        else:\n",
    "            df.to_csv(data_dir + 'chartevents_reduced.csv', mode='a', header=False, index=False)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: reduce outputs events step by filtering on events specific to urine output, determined by data in D_ITEMS\n",
    "def reduce_output_events():\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    \n",
    "    # Relevant ITEMIDs, from https://github.com/vincentmajor/mimicfilters/blob/master/lists/OASIS_components/preprocess_urine_awk_str.txt\n",
    "    urine_output = [42810, 43171, 43173, 43175, 43348, 43355, 43365, 43372, 43373, 43374, 43379, 43380, 43431, 43462, 43522, 40405, 40428, 40534, \n",
    "    40288, 42042, 42068, 42111, 42119, 42209, 41857, 40715, 40056, 40061, 40085, 40094, 40096, 42001, 42676, 42556, 43093, 44325, 44706,\n",
    "    44506, 42859, 44237, 44313, 44752, 44824, 44837, 43576, 43589, 43633, 44911, 44925, 42362, 42463, 42507, 42510, 40055, 40057, 40065,\n",
    "    40069, 45804, 45841, 43811, 43812, 43856, 43897, 43931, 43966, 44080, 44103, 44132, 45304, 46177, 46532, 46578, 46658, 46748, 40651,\n",
    "    43053, 43057, 40473, 42130, 41922, 44253, 44278, 46180, 44684, 43333, 43347, 42592, 42666, 42765, 42892, 45927, 44834, 43638, 43654,\n",
    "    43519, 43537, 42366, 45991, 46727, 46804, 43987, 44051, 227489, 226566, 226627, 226631, 45415, 42111, 41510, 40055, 226559, 40428,\n",
    "    40580, 40612, 40094, 40848, 43685, 42362, 42463, 42510, 46748, 40972, 40973, 46456, 226561, 226567, 226632, 40096, 40651, 226557,\n",
    "    226558, 40715, 226563]\n",
    "    \n",
    "    # Relevant ITEMIDs\n",
    "    print('-----------------------------------------')\n",
    "    print('Load item definitions')\n",
    "    dtype = {'ITEMID': 'int32',\n",
    "           'LABEL': 'str',\n",
    "           'UNITNAME': 'str',\n",
    "           'LINKSTO': 'str'}\n",
    "    defs = pd.read_csv(mimic_dir + 'D_ITEMS.csv', usecols=dtype.keys(), dtype=dtype)\n",
    "    print('URINE_OUTPUT')\n",
    "    defs = defs[defs['ITEMID'].isin(urine_output)]\n",
    "    defs['LABEL'] = defs['LABEL'].str.lower()\n",
    "    # Remove measurements in /kg/hr\n",
    "    defs = defs[~(defs['LABEL'].str.contains('hr') | defs['LABEL'].str.contains('kg')) | defs['LABEL'].str.contains('nephro')]\n",
    "    print(defs['LABEL'])\n",
    "    urine_output = defs['ITEMID'].tolist()\n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Loading Output Events')\n",
    "    dtype = {'ICUSTAY_ID': 'str',\n",
    "           'ITEMID': 'int32',\n",
    "           'CHARTTIME': 'str',\n",
    "           'VALUE': 'float32'}\n",
    "    parse_dates = ['CHARTTIME']\n",
    "    \n",
    "    # Load outputevents table\n",
    "    # Table purpose: Output data for patients.\n",
    "    df = pd.read_csv(mimic_dir + 'OUTPUTEVENTS.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)\n",
    "    df = df.rename(columns={'VALUE': 'VALUENUM'})\n",
    "    df = df[df['ICUSTAY_ID'].notna() & df['VALUENUM'].notna() & (df['ITEMID'].isin(urine_output)) & (df['VALUENUM'] > 0)]\n",
    "    df['ICUSTAY_ID'] = df['ICUSTAY_ID'].astype('int32')\n",
    "    \n",
    "    # remove implausible measurements\n",
    "    df = df[~(df.VALUENUM > 10000)]\n",
    "    \n",
    "    # sum all outputs in one day\n",
    "    df.drop(columns=['ITEMID'], inplace=True)\n",
    "    df['CHARTTIME'] = df['CHARTTIME'].dt.date\n",
    "    df = df.groupby(['ICUSTAY_ID', 'CHARTTIME']).sum()\n",
    "    df['CE_TYPE'] = 'URINE_OUTPUT'\n",
    "    df = df[~(df.VALUENUM > 10000)]\n",
    "    \n",
    "    print('Remove admission and discharge days (since data on urine output is incomplete)')\n",
    "    # Load icustays table\n",
    "    # Table purpose: Defines each ICUSTAY_ID in the database, i.e. defines a single ICU stay\n",
    "    print('Load ICU stays...')\n",
    "    dtype = {'ICUSTAY_ID': 'int32',\n",
    "           'INTIME': 'str',\n",
    "           'OUTTIME': 'str'}\n",
    "    parse_dates = ['INTIME', 'OUTTIME']\n",
    "    icustays = pd.read_csv(mimic_dir + 'ICUSTAYS.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)\n",
    "    icustays['INTIME'] = icustays['INTIME'].dt.date\n",
    "    icustays['OUTTIME'] = icustays['OUTTIME'].dt.date\n",
    "    \n",
    "    # Merge\n",
    "    tmp = icustays[['ICUSTAY_ID', 'INTIME']].drop_duplicates()\n",
    "    tmp = tmp.rename(columns={'INTIME': 'CHARTTIME'})\n",
    "    tmp['ID_IN'] = 1\n",
    "    df = pd.merge(df, tmp, how='left', on=['ICUSTAY_ID', 'CHARTTIME'])\n",
    "    tmp = icustays[['ICUSTAY_ID', 'OUTTIME']].drop_duplicates()\n",
    "    tmp = tmp.rename(columns={'OUTTIME': 'CHARTTIME'})\n",
    "    tmp['ID_OUT'] = 1\n",
    "    df = pd.merge(df, tmp, how='left', on=['ICUSTAY_ID', 'CHARTTIME'])\n",
    "    \n",
    "    # Remove admission and discharge days\n",
    "    df = df[df['ID_IN'].isnull() & df['ID_OUT'].isnull()]\n",
    "    df.drop(columns=['ID_IN', 'ID_OUT'], inplace=True)\n",
    "    \n",
    "    # Add SUBJECT_ID and HADM_ID\n",
    "    icustays.drop(columns=['INTIME', 'OUTTIME'], inplace=True)  \n",
    "    df['CHARTTIME'] = pd.to_datetime(df['CHARTTIME']) + pd.DateOffset(hours=12)\n",
    "    \n",
    "    # Save\n",
    "    df.to_pickle(data_dir + 'outputevents_reduced.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Create a merged intermediate output file which contains content from the reduced chartevents and reduced output events generated\n",
    "# in the previous two steps\n",
    "def merge_chart_outputs():\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    # Load (reduced) chartevents table\n",
    "    print('Loading chart events...')\n",
    "    dtype = {'SUBJECT_ID': 'int32',\n",
    "           'ICUSTAY_ID': 'int32',\n",
    "           'CE_TYPE': 'str',\n",
    "           'CHARTTIME': 'str',\n",
    "           'VALUENUM': 'float32'}\n",
    "    parse_dates = ['CHARTTIME']\n",
    "    charts = pd.read_csv(data_dir + 'chartevents_reduced.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Compute BMI and GCS total...')\n",
    "    charts.sort_values(by=['SUBJECT_ID', 'ICUSTAY_ID', 'CHARTTIME'], ascending=[True, True, False], inplace=True)\n",
    "    \n",
    "    # Compute BMI\n",
    "    rows_bmi = (charts['CE_TYPE']=='WEIGHT') | (charts['CE_TYPE']=='HEIGHT')\n",
    "    charts_bmi = charts[rows_bmi]\n",
    "    charts_bmi = charts_bmi.pivot_table(index=['SUBJECT_ID', 'ICUSTAY_ID', 'CHARTTIME'], columns='CE_TYPE', values='VALUENUM')\n",
    "    charts_bmi = charts_bmi.rename_axis(None, axis=1).reset_index()\n",
    "    charts_bmi['HEIGHT'] = charts_bmi.groupby('SUBJECT_ID')['HEIGHT'].ffill()\n",
    "    charts_bmi['HEIGHT'] = charts_bmi.groupby('SUBJECT_ID')['HEIGHT'].bfill()\n",
    "    charts_bmi =  charts_bmi[~pd.isnull(charts_bmi).any(axis=1)]\n",
    "    charts_bmi['VALUENUM'] = charts_bmi['WEIGHT']/charts_bmi['HEIGHT']/charts_bmi['HEIGHT']*10000\n",
    "    charts_bmi['CE_TYPE'] = 'BMI'\n",
    "    charts_bmi.drop(columns=['HEIGHT', 'WEIGHT'], inplace=True)\n",
    "    \n",
    "    # Compute GCS total if not available\n",
    "    rows_gcs = (charts['CE_TYPE']=='GCS_EYE_OPENING') | (charts['CE_TYPE']=='GCS_VERBAL_RESPONSE') | (charts['CE_TYPE']=='GCS_MOTOR_RESPONSE') | (charts['CE_TYPE']=='GCS_TOTAL')\n",
    "    charts_gcs = charts[rows_gcs]\n",
    "    charts_gcs = charts_gcs.pivot_table(index=['SUBJECT_ID', 'ICUSTAY_ID', 'CHARTTIME'], columns='CE_TYPE', values='VALUENUM')\n",
    "    charts_gcs = charts_gcs.rename_axis(None, axis=1).reset_index()\n",
    "    null_gcs_total = charts_gcs['GCS_TOTAL'].isnull()\n",
    "    charts_gcs.loc[null_gcs_total, 'GCS_TOTAL'] = charts_gcs[null_gcs_total].GCS_EYE_OPENING + charts_gcs[null_gcs_total].GCS_VERBAL_RESPONSE + charts_gcs[null_gcs_total].GCS_MOTOR_RESPONSE\n",
    "    charts_gcs =  charts_gcs[~charts_gcs['GCS_TOTAL'].isnull()]\n",
    "    charts_gcs = charts_gcs.rename(columns={'GCS_TOTAL': 'VALUENUM'})\n",
    "    charts_gcs['CE_TYPE'] = 'GCS_TOTAL'\n",
    "    charts_gcs.drop(columns=['GCS_EYE_OPENING', 'GCS_VERBAL_RESPONSE', 'GCS_MOTOR_RESPONSE'], inplace=True)\n",
    "    \n",
    "    # Merge back with rest of the table\n",
    "    rows_others = ~rows_bmi & ~rows_gcs\n",
    "    charts = pd.concat([charts_bmi, charts_gcs, charts[rows_others]], ignore_index=True, sort=False)\n",
    "    charts.drop(columns=['SUBJECT_ID'], inplace=True)\n",
    "    charts.sort_values(by=['ICUSTAY_ID', 'CHARTTIME'], ascending=[True, False], inplace=True)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    # Load (reduced) outputevents table\n",
    "    print('Loading output events...')\n",
    "    outputs = pd.read_pickle(data_dir + 'outputevents_reduced.pkl')\n",
    "    df = pd.concat([charts, outputs], ignore_index=True, sort=False)\n",
    "    df.sort_values(by=['ICUSTAY_ID', 'CHARTTIME'], ascending=[True, False], inplace=True)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Create categorical variable...')\n",
    "    # Bin according to OASIS severity score\n",
    "    heart_rate_bins               = np.array([-1, 32.99, 88.5, 106.5, 125.5, np.Inf])\n",
    "    respiratory_rate_bins         = np.array([-1, 5.99, 12.5, 22.5, 30.5, 44.5, np.Inf])\n",
    "    body_temperature_bins         = np.array([-1, 33.21, 35.93, 36.39, 36.88, 39.88, np.Inf])\n",
    "    mean_bp_bins                  = np.array([-1, 20.64, 50.99, 61.32, 143.44, np.Inf])\n",
    "    fraction_inspired_oxygen_bins = np.array([-1, np.Inf])\n",
    "    gcs_total_bins                = np.array([-1, 7, 13, 14, 15])\n",
    "    bmi_bins                      = np.array([-1, 15, 16, 18.5, 25, 30, 35, 40, 45, 50, 60, np.Inf])\n",
    "    urine_output_bins             = np.array([-1, 670.99, 1426.99, 2543.99, 6896, np.Inf])\n",
    "    bins = [heart_rate_bins, respiratory_rate_bins, body_temperature_bins, mean_bp_bins, fraction_inspired_oxygen_bins, gcs_total_bins, urine_output_bins]\n",
    "\n",
    "    # Labels \n",
    "    heart_rate_labels               = ['CHART_HR_m1', 'CHART_HR_n', 'CHART_HR_p1', 'CHART_HR_p2', 'CHART_HR_p3']\n",
    "    respiratory_rate_labels         = ['CHART_RR_m2', 'CHART_RR_m1', 'CHART_RR_n', 'CHART_RR_p1', 'CHART_RR_p2', 'CHART_RR_p3']\n",
    "    body_temperature_labels         = ['CHART_BT_m3', 'CHART_BT_m2', 'CHART_BT_m1', 'CHART_BT_n', 'CHART_BT_p1', 'CHART_BT_p2']\n",
    "    mean_bp_labels                  = ['CHART_BP_m3', 'CHART_BP_m2', 'CHART_BP_m1', 'CHART_BP_n', 'CHART_BP_p1']\n",
    "    fraction_inspired_oxygen_labels = ['CHART_VENT']\n",
    "    gcs_total_labels                = ['CHART_GC_m3', 'CHART_GC_m2', 'CHART_GC_m1', 'CHART_GC_n']\n",
    "    bmi_labels                      = ['CHART_BM_m3', 'CHART_BM_m2', 'CHART_BM_m1', 'CHART_BM_n', 'CHART_BM_p1', 'CHART_BM_p2', 'CHART_BM_p3', 'CHART_BM_p4', 'CHART_BM_p5', 'CHART_BM_p6', 'CHART_BM_p7']\n",
    "    urine_output_labels             = ['CHART_UO_m3', 'CHART_UO_m2', 'CHART_UO_m1', 'CHART_UO_n', 'CHART_UO_p1']\n",
    "    labels = [heart_rate_labels, respiratory_rate_labels, body_temperature_labels, mean_bp_labels, fraction_inspired_oxygen_labels, gcs_total_labels, urine_output_labels]\n",
    "\n",
    "    # Chart event types\n",
    "    ce_types = ['HEART_RATE', 'RESPIRATORY_RATE', 'BODY_TEMPERATURE', 'MEAN_BP', 'FRACTION_INSPIRED_OXYGEN', 'GCS_TOTAL', 'URINE_OUTPUT']\n",
    "    \n",
    "    df_list = []\n",
    "    df_list_last_only = [] # for logistic regression\n",
    "    for type, label, bin in zip(ce_types, labels, bins):\n",
    "        # get chart events of a specific type\n",
    "        tmp = df[df['CE_TYPE'] == type]\n",
    "\n",
    "        # bin them and sort\n",
    "        tmp['VALUECAT'] = pd.cut(tmp['VALUENUM'], bins=bin, labels=label)\n",
    "        tmp.drop(columns=['CE_TYPE', 'VALUENUM'], inplace=True)\n",
    "        tmp.sort_values(by=['ICUSTAY_ID', 'CHARTTIME'], ascending=[True, False], inplace=True)\n",
    "\n",
    "        # remove consecutive duplicates\n",
    "        tmp = tmp[(tmp[['ICUSTAY_ID', 'VALUECAT']] != tmp[['ICUSTAY_ID', 'VALUECAT']].shift()).any(axis=1)]\n",
    "        df_list.append(tmp)\n",
    "\n",
    "        # for logistic regression, keep only the last measurement\n",
    "        tmp = tmp.drop_duplicates(subset='ICUSTAY_ID')\n",
    "        df_list_last_only.append(tmp)\n",
    "    \n",
    "    df = pd.concat(df_list, ignore_index=True, sort=False)\n",
    "    df.sort_values(by=['ICUSTAY_ID', 'CHARTTIME'], ascending=[True, False], inplace=True)\n",
    "    \n",
    "    # drop duplicates to keep size manageable\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Save...')\n",
    "    df.to_pickle(data_dir + 'charts_outputs_reduced.pkl')\n",
    "    df.to_csv(data_dir + 'charts_outputs_reduced.csv', index=False)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Save data for logistic regression...')\n",
    "    \n",
    "    # for logistic regression\n",
    "    df_last_only = pd.concat(df_list_last_only, ignore_index=True, sort=False)\n",
    "    df_last_only.sort_values(by=['ICUSTAY_ID', 'CHARTTIME'], ascending=[True, False], inplace=True)\n",
    "    df_last_only.to_pickle(data_dir + 'charts_outputs_last_only.pkl')\n",
    "    df_last_only.to_csv(data_dir + 'charts_outputs_last_only.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Links diagnoses with their associated procedures\n",
    "def link_diagnoses_procedures():\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    # Load icu_pat table\n",
    "    print('Loading icu_pat...')\n",
    "    icu_pat = pd.read_pickle(data_dir + 'icu_pat_admit.pkl')\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    print('Load admissions...')\n",
    "    # Load admissions table\n",
    "    # Table purpose: Define a patients hospital admission, HADM_ID.\n",
    "    dtype = {'HADM_ID': 'int32',\n",
    "           'ADMITTIME': 'str',\n",
    "           'DISCHTIME': 'str'}\n",
    "    parse_dates = ['ADMITTIME', 'DISCHTIME']\n",
    "    admissions = pd.read_csv(mimic_dir + 'ADMISSIONS.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    print('Load diagnoses and procedures...')\n",
    "    dtype = {'SUBJECT_ID': 'int32',\n",
    "           'HADM_ID': 'int32',\n",
    "           'ICD9_CODE': 'str'}\n",
    "\n",
    "    # Load diagnosis_icd table\n",
    "    # Table purpose: Contains ICD diagnoses for patients, most notably ICD-9 diagnoses.\n",
    "    diagnoses = pd.read_csv(mimic_dir + 'DIAGNOSES_ICD.csv', usecols=dtype.keys(), dtype=dtype)\n",
    "    diagnoses = diagnoses.dropna()\n",
    "\n",
    "    # Load procedures_icd table\n",
    "    # Table purpose: Contains ICD procedures for patients, most notably ICD-9 procedures.\n",
    "    procedures = pd.read_csv(mimic_dir + 'PROCEDURES_ICD.csv', usecols=dtype.keys(), dtype=dtype)\n",
    "    procedures = procedures.dropna()\n",
    "    \n",
    "    # Merge diagnoses and procedures\n",
    "    diagnoses['ICD9_CODE'] = 'DIAGN_' + diagnoses['ICD9_CODE'].str.lower().str.strip()\n",
    "    procedures['ICD9_CODE'] = 'PROCE_' + procedures['ICD9_CODE'].str.lower().str.strip()\n",
    "    diag_proc = pd.concat([diagnoses, procedures], ignore_index=True, sort=False)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    # Link diagnoses/procedures and admissions tables\n",
    "    print('Link diagnoses/procedures and admissions tables...')\n",
    "    diag_proc = pd.merge(diag_proc, admissions, how='inner', on='HADM_ID').drop(columns=['HADM_ID'])\n",
    "    \n",
    "    # Link diagnoses/procedures and icu_pat tables\n",
    "    print('Link diagnoses/procedures and icu_pat tables...')\n",
    "    diag_proc = pd.merge(icu_pat[['SUBJECT_ID', 'ICUSTAY_ID', 'OUTTIME']], diag_proc, how='left', on=['SUBJECT_ID'])\n",
    "    \n",
    "    # Remove codes related to future admissions using time difference to ADMITTIME\n",
    "    diag_proc['DAYS_TO_OUT'] = (diag_proc['OUTTIME']-diag_proc['ADMITTIME']) / np.timedelta64(1, 'D')\n",
    "    diag_proc = diag_proc[(diag_proc['DAYS_TO_OUT'] >= 0) | diag_proc['DAYS_TO_OUT'].isna()]\n",
    "\n",
    "    # Reset time value using time difference to DISCHTIME (0 if negative)\n",
    "    diag_proc['DAYS_TO_OUT'] = (diag_proc['OUTTIME']-diag_proc['DISCHTIME']) / np.timedelta64(1, 'D')\n",
    "    diag_proc.loc[diag_proc['DAYS_TO_OUT'] < 0, 'DAYS_TO_OUT'] = 0\n",
    "    diag_proc = diag_proc.drop(columns=['SUBJECT_ID', 'OUTTIME', 'ADMITTIME', 'DISCHTIME'])\n",
    "\n",
    "    # Lost some ICUSTAY_IDs with only negative DAYS_TO_OUT, merge back\n",
    "    diag_proc = pd.merge(icu_pat[['ICUSTAY_ID']], diag_proc, how='left', on=['ICUSTAY_ID'])\n",
    "    \n",
    "    print('Drop duplicates...')\n",
    "    diag_proc = diag_proc.drop_duplicates()\n",
    "    \n",
    "    print('Map rare codes to OTHER...')\n",
    "    diag_proc = diag_proc.apply(lambda x: x.mask(x.map(x.value_counts()) < min_count, 'other') if x.name in ['ICD9_CODE'] else x)                   \n",
    "    \n",
    "    print('-----------------------------------------')  \n",
    "    print('Save...')\n",
    "    assert len(diag_proc['ICUSTAY_ID'].unique()) == 45298\n",
    "    diag_proc.sort_values(by=['ICUSTAY_ID', 'DAYS_TO_OUT'], ascending=[True, True], inplace=True)\n",
    "    diag_proc.to_pickle(data_dir + 'diag_proc.pkl')\n",
    "    diag_proc.to_csv(data_dir + 'diag_proc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Links chartevents and outputevents data with relevant prescriptions\n",
    "def link_charts_prescriptions():\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    # Load icu_pat table\n",
    "    print('Loading icu_pat...')\n",
    "    icu_pat = pd.read_pickle(data_dir + 'icu_pat_admit.pkl')\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    print('Load charts and outputs...')\n",
    "    charts_outputs = pd.read_pickle(data_dir + 'charts_outputs_reduced.pkl')\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    print('Load prescriptions...')\n",
    "    dtype = {'ICUSTAY_ID': 'str',\n",
    "           'DRUG': 'str',\n",
    "           'STARTDATE': 'str'}\n",
    "    parse_dates = ['STARTDATE']\n",
    "    # Load prescriptions table\n",
    "    # Table purpose: Contains medication related order entries, i.e. prescriptions\n",
    "    prescriptions = pd.read_csv(mimic_dir + 'PRESCRIPTIONS.csv', usecols=dtype.keys(), dtype=dtype, parse_dates=parse_dates)\n",
    "    prescriptions = prescriptions.dropna()\n",
    "    prescriptions['ICUSTAY_ID'] = prescriptions['ICUSTAY_ID'].astype('int32')\n",
    "    prescriptions['DRUG'] = 'PRESC_' + prescriptions['DRUG'].str.lower().replace('\\s+', '', regex=True)\n",
    "    prescriptions = prescriptions.rename(columns={'DRUG': 'VALUECAT', 'STARTDATE': 'CHARTTIME'})\n",
    "    df = pd.concat([charts_outputs, prescriptions], ignore_index=True, sort=False)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    # Link charts/outputs and icu_pat tables\n",
    "    print('Link charts/outputs and icu_pat tables...')\n",
    "    df = pd.merge(icu_pat[['ICUSTAY_ID', 'OUTTIME']], df, how='left', on=['ICUSTAY_ID'])\n",
    "    \n",
    "    # Reset time value using time difference to DISCHTIME (0 if negative)\n",
    "    df['HOURS_TO_OUT'] = (df['OUTTIME']-df['CHARTTIME']) / np.timedelta64(1, 'h')\n",
    "    df.loc[df['HOURS_TO_OUT'] < 0, 'HOURS_TO_OUT'] = 0\n",
    "    df = df.drop(columns=['OUTTIME', 'CHARTTIME'])\n",
    "    \n",
    "    print('Drop duplicates...')\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    print('Map rare codes to OTHER...')\n",
    "    df = df.apply(lambda x: x.mask(x.map(x.value_counts()) < min_count, 'other') if x.name in ['VALUECAT'] else x)                   \n",
    "    \n",
    "    print('-----------------------------------------')  \n",
    "    print('Save...')\n",
    "    assert len(df['ICUSTAY_ID'].unique()) == 45298\n",
    "    df.sort_values(by=['ICUSTAY_ID', 'HOURS_TO_OUT'], ascending=[True, True], inplace=True)\n",
    "    df.to_pickle(data_dir + 'charts_prescriptions.pkl')\n",
    "    df.to_csv(data_dir + 'charts_prescriptions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: creates data arrays step used for training/validating/testing all models\n",
    "def create_data_arrays():\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    def get_arrays(df, code_column, time_column, quantile=1):\n",
    "      df['COUNT'] = df.groupby(['ICUSTAY_ID']).cumcount()\n",
    "      df = df[df['COUNT'] < df.groupby(['ICUSTAY_ID']).size().quantile(q=quantile)]\n",
    "      max_count_df = df['COUNT'].max()+1\n",
    "      print('max_count {}'.format(max_count_df))\n",
    "      multiindex_df = pd.MultiIndex.from_product([icu_pat['ICUSTAY_ID'], range(max_count_df)], names = ['ICUSTAY_ID', 'COUNT'])\n",
    "      df = df.set_index(['ICUSTAY_ID', 'COUNT'])\n",
    "    \n",
    "      print('Reindex df...')\n",
    "      df = df.reindex(multiindex_df).fillna(0)\n",
    "      print('done')\n",
    "      df_times = df[time_column].values.reshape((num_icu_stays, max_count_df))\n",
    "      df[code_column] = df[code_column].astype('category')\n",
    "      dict_df = dict(enumerate(df[code_column].cat.categories))\n",
    "      df[code_column] = df[code_column].cat.codes\n",
    "      df = df[code_column].values.reshape((num_icu_stays, max_count_df))\n",
    "    \n",
    "      return df, df_times, dict_df\n",
    "  \n",
    "    # Load icu_pat table\n",
    "    print('Loading icu_pat...')\n",
    "    icu_pat = pd.read_pickle(data_dir + 'icu_pat_admit.pkl')\n",
    "    \n",
    "    print('Loading diagnoses/procedures...')\n",
    "    dp = pd.read_pickle(data_dir + 'diag_proc.pkl')\n",
    "    \n",
    "    print('Loading charts/prescriptions...')\n",
    "    cp = pd.read_pickle(data_dir + 'charts_prescriptions.pkl')\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    num_icu_stays = len(icu_pat['ICUSTAY_ID'])\n",
    "    \n",
    "    # static variables\n",
    "    print('Create static array...')\n",
    "    icu_pat = pd.get_dummies(icu_pat, columns = ['ADMISSION_LOCATION', 'INSURANCE', 'MARITAL_STATUS', 'ETHNICITY'])\n",
    "    icu_pat.drop(columns=['ADMISSION_LOCATION_Emergency Room Admit', 'INSURANCE_Medicare', 'MARITAL_STATUS_Married/Life Partner', 'ETHNICITY_White'], inplace=True) # drop reference columns\n",
    "    static_columns = icu_pat.columns.str.contains('AGE|GENDER_M|LOS|NUM_RECENT_ADMISSIONS|ADMISSION_LOCATION|INSURANCE|MARITAL_STATUS|ETHNICITY|PRE_ICU_LOS|ELECTIVE_SURGERY')\n",
    "    static = icu_pat.loc[:, static_columns].values\n",
    "    static_vars = icu_pat.loc[:, static_columns].columns.values.tolist()\n",
    "    \n",
    "    # classification label\n",
    "    print('Create label array...')\n",
    "    label = icu_pat.loc[:, 'POSITIVE'].values\n",
    "    \n",
    "    # diagnoses/procedures and charts/prescriptions\n",
    "    print('Create diagnoses/procedures and charts/prescriptions array...')\n",
    "    dp, dp_times, dict_dp = get_arrays(dp, 'ICD9_CODE', 'DAYS_TO_OUT', 1)\n",
    "    cp, cp_times, dict_cp = get_arrays(cp, 'VALUECAT', 'HOURS_TO_OUT', 0.95)\n",
    "    \n",
    "    # Normalize times\n",
    "    dp_times = dp_times/dp_times.max()\n",
    "    cp_times = cp_times/cp_times.max()\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Split data into train/validate/test...')\n",
    "    # Split patients to avoid data leaks\n",
    "    patients = icu_pat['SUBJECT_ID'].drop_duplicates()\n",
    "    train, validate, test = np.split(patients.sample(frac=1, random_state=123), [int(.9*len(patients)), int(.9*len(patients))])\n",
    "    train_ids = icu_pat['SUBJECT_ID'].isin(train).values\n",
    "    validate_ids = icu_pat['SUBJECT_ID'].isin(validate).values\n",
    "    test_ids = icu_pat['SUBJECT_ID'].isin(test).values\n",
    "    \n",
    "    print('Get patients corresponding to test ids')\n",
    "    test_ids_patients = icu_pat['SUBJECT_ID'].iloc[test_ids].reset_index(drop=True)\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    print('Save...')\n",
    "    np.savez(data_dir + 'data_arrays.npz', static=static, static_vars=static_vars, label=label,\n",
    "           dp=dp, cp=cp, dp_times=dp_times, cp_times=cp_times, dict_dp=dict_dp, dict_cp=dict_cp,\n",
    "           train_ids=train_ids, validate_ids=validate_ids, test_ids=test_ids)\n",
    "    # np.savez(data_dir + 'data_dictionaries.npz', dict_dp=dict_dp, dict_cp=dict_cp)\n",
    "    test_ids_patients.to_pickle(data_dir + 'test_ids_patients.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZScZNbROw-N"
   },
   "outputs": [],
   "source": [
    "# Order in which the preprocessing steps must be run in order to produce the final data_arrays.npz and test_ids_patients.pkl files needed for training/validating/testing\n",
    "\n",
    "# create_icu_pat_admit()\n",
    "# reduce_chart_events()\n",
    "# reduce_output_events()\n",
    "# merge_chart_outputs()\n",
    "# link_diagnoses_procedures()\n",
    "# link_charts_prescriptions()\n",
    "# create_data_arrays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  batch_size = 128\n",
    "  num_epochs = 80\n",
    "  dropout_rate = 0.5\n",
    "  patience = 10 # early stopping\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data, type):\n",
    "  # Data\n",
    "  static       = data['static'].astype('float32')\n",
    "  label        = data['label'].astype('float32')\n",
    "  dp           = data['dp'].astype('int64') # diagnoses/procedures\n",
    "  cp           = data['cp'].astype('int64') # charts/prescriptions\n",
    "  dp_times     = data['dp_times'].astype('float32')\n",
    "  cp_times     = data['cp_times'].astype('float32')\n",
    "  train_ids    = data['train_ids']\n",
    "  validate_ids = data['validate_ids']\n",
    "  test_ids     = data['test_ids']  \n",
    "\n",
    "  if (type == 'TRAIN'):\n",
    "    ids = train_ids\n",
    "  elif (type == 'VALIDATE'):\n",
    "    ids = validate_ids\n",
    "  elif (type == 'TEST'):\n",
    "    ids = test_ids\n",
    "  elif (type == 'ALL'):\n",
    "    ids = np.full_like(label, True, dtype=bool)\n",
    "\n",
    "  static   = static[ids, :]\n",
    "  label    = label[ids]\n",
    "  dp       = dp[ids, :]\n",
    "  cp       = cp[ids, :]\n",
    "  dp_times = dp_times[ids, :]\n",
    "  cp_times = cp_times[ids, :]\n",
    "  \n",
    "  return static, dp, cp, dp_times, cp_times, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_trainloader(data, type, shuffle=True, idx=None):\n",
    "  # Data\n",
    "  static, dp, cp, dp_times, cp_times, label = get_data(data, type)\n",
    "\n",
    "  # Bootstrap\n",
    "  if idx is not None:\n",
    "    static, dp, cp, dp_times, cp_times, label = static[idx], dp[idx], cp[idx], dp_times[idx], cp_times[idx], label[idx]\n",
    "\n",
    "  # Compute total batch count\n",
    "  num_batches = len(label) // batch_size\n",
    "  \n",
    "  # Create dataset\n",
    "  dataset = utils.TensorDataset(torch.from_numpy(static), \n",
    "                                torch.from_numpy(dp),\n",
    "                                torch.from_numpy(cp),\n",
    "                                torch.from_numpy(dp_times),\n",
    "                                torch.from_numpy(cp_times),\n",
    "                                torch.from_numpy(label))\n",
    "\n",
    "  # # Create batch queues\n",
    "  trainloader = utils.DataLoader(dataset,\n",
    "                                 batch_size = batch_size, \n",
    "                                 shuffle = shuffle,\n",
    "                                 sampler = None,\n",
    "                                 num_workers = 2,\n",
    "                                 drop_last = True)\n",
    "                                 \n",
    "  # # Weight of positive samples for training\n",
    "  pos_weight = torch.tensor((len(label) - np.sum(label))/np.sum(label))\n",
    "  \n",
    "  return trainloader, num_batches, pos_weight\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3muyDPFPbozY"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Model\n",
    "\n",
    "The combination of the different approaches that we plan to experiment and evaluate the outcomes are listed below:\n",
    "  * RNN (concatenated Î”time)\n",
    "  * RNN (concatenated Î”time)â€‰+â€‰Attention\n",
    "  * RNN (exp time decay)â€‰+â€‰Attention\n",
    "  * RNN (exp time decay)\n",
    "  * RNN (ODE time decay)\n",
    "  * ODEâ€‰+â€‰RNNâ€‰+â€‰Attention\n",
    "  * ODEâ€‰+â€‰Attention.\n",
    "  * RNN (concatenated Î”time)\n",
    "  * RNN (ODE time decay)â€‰+â€‰Attention\n",
    "  * Attention (concatenated time)\n",
    "  * MCEâ€‰+â€‰RNNâ€‰+â€‰Attention\n",
    "  * MCEâ€‰+â€‰RNN\n",
    "  * MCEâ€‰+â€‰Attention\n",
    "  * ODE + RNN\n",
    "\n",
    "Three of the above models (RNN (concatenated Î”time), RNN (concatenated Î”time)â€‰+â€‰Attention and RNN (ODE time decay)) have been implemented.\n",
    "Each model is defined in its own class, with the comments in the class outlining each step. \n",
    "\n",
    "Training is done on the output of the preprocessing step - data_arrays.npz.\n",
    "Loss function : BCEWithLogitsLoss\n",
    "Optimizer : Adam for stochastic gradient descent\n",
    "\n",
    "The trained model definitions are stored under \"logdir\". A separate folder by model captures the \"final_model.pt\" file.\n",
    "Model need not be trained every time, it does take on an average of 6-8 hours for training due to the data volumes and the devidce being \"cpu\".\n",
    "The pretrained models that are uploaded can be used for evaluation of results. However, if desired the trauining can also be done, the only prerequisities would be ensuring the required data files are available in data folder and the preprocessing steps to generate the \"data_arrays.npz\" have successfully completed.\n",
    "\n",
    "Subsequent functions for testing the model and capturing the results in the \"results\" folder are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class birnn_concat_time_delta(nn.Module):\n",
    "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
    "      super(birnn_concat_time_delta, self).__init__()\n",
    "      \n",
    "      # Embedding dimensions\n",
    "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))\n",
    "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))\n",
    "\n",
    "      # Embedding layers\n",
    "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
    "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
    "\n",
    "      # GRU layers\n",
    "      self.gru_dp_fw = nn.GRU(input_size=self.embed_dp_dim+1, hidden_size=self.embed_dp_dim+1, num_layers=1, batch_first=True)\n",
    "      self.gru_cp_fw = nn.GRU(input_size=self.embed_cp_dim+1, hidden_size=self.embed_cp_dim+1, num_layers=1, batch_first=True)\n",
    "      self.gru_dp_bw = nn.GRU(input_size=self.embed_dp_dim+1, hidden_size=self.embed_dp_dim+1, num_layers=1, batch_first=True)\n",
    "      self.gru_cp_bw = nn.GRU(input_size=self.embed_cp_dim+1, hidden_size=self.embed_cp_dim+1, num_layers=1, batch_first=True)\n",
    "      \n",
    "      # Fully connected output\n",
    "      self.fc_dp  = nn.Linear(2*(self.embed_dp_dim+1), 1)\n",
    "      self.fc_cp  = nn.Linear(2*(self.embed_cp_dim+1), 1)\n",
    "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
    "      \n",
    "      # Others\n",
    "      self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
    "      # Compute time delta\n",
    "      ## output dim: batch_size x seq_len\n",
    "      dp_t_delta_fw = abs_time_to_delta(dp_t)\n",
    "      cp_t_delta_fw = abs_time_to_delta(cp_t)\n",
    "      dp_t_delta_bw = abs_time_to_delta(torch.flip(dp_t, [1]))\n",
    "      cp_t_delta_bw = abs_time_to_delta(torch.flip(cp_t, [1]))    \n",
    "    \n",
    "      # Embedding\n",
    "      ## output dim: batch_size x seq_len x embedding_dim\n",
    "      embedded_dp_fw = self.embed_dp(dp)\n",
    "      embedded_cp_fw = self.embed_cp(cp)\n",
    "      embedded_dp_bw = torch.flip(embedded_dp_fw, [1])\n",
    "      embedded_cp_bw = torch.flip(embedded_cp_fw, [1])\n",
    "      \n",
    "      # Concatate with time\n",
    "      ## output dim: batch_size x seq_len x (embedding_dim+1)\n",
    "      concat_dp_fw = torch.cat((embedded_dp_fw, torch.unsqueeze(dp_t_delta_fw, dim=-1)), dim=-1)\n",
    "      concat_cp_fw = torch.cat((embedded_cp_fw, torch.unsqueeze(cp_t_delta_fw, dim=-1)), dim=-1)\n",
    "      concat_dp_bw = torch.cat((embedded_dp_bw, torch.unsqueeze(dp_t_delta_bw, dim=-1)), dim=-1)\n",
    "      concat_cp_bw = torch.cat((embedded_cp_bw, torch.unsqueeze(cp_t_delta_bw, dim=-1)), dim=-1)\n",
    "      ## Dropout\n",
    "      concat_dp_fw = self.dropout(concat_dp_fw)\n",
    "      concat_cp_fw = self.dropout(concat_cp_fw)\n",
    "      concat_dp_bw = self.dropout(concat_dp_bw)\n",
    "      concat_cp_bw = self.dropout(concat_cp_bw)\n",
    "      \n",
    "      # GRU\n",
    "      ## output dim rnn:        batch_size x seq_len x (embedding_dim+1)\n",
    "      ## output dim rnn_hidden: batch_size x 1 x (embedding_dim+1)\n",
    "      rnn_dp_fw, rnn_hidden_dp_fw = self.gru_dp_fw(concat_dp_fw)\n",
    "      rnn_cp_fw, rnn_hidden_cp_fw = self.gru_cp_fw(concat_cp_fw)\n",
    "      rnn_dp_bw, rnn_hidden_dp_bw = self.gru_dp_bw(concat_dp_bw)\n",
    "      rnn_cp_bw, rnn_hidden_cp_bw = self.gru_cp_bw(concat_cp_bw)      \n",
    "      ## output dim rnn_hidden: batch_size x (embedding_dim+1)\n",
    "      rnn_hidden_dp_fw = rnn_hidden_dp_fw.view(-1, self.embed_dp_dim+1)\n",
    "      rnn_hidden_cp_fw = rnn_hidden_cp_fw.view(-1, self.embed_cp_dim+1)\n",
    "      rnn_hidden_dp_bw = rnn_hidden_dp_bw.view(-1, self.embed_dp_dim+1)\n",
    "      rnn_hidden_cp_bw = rnn_hidden_cp_bw.view(-1, self.embed_cp_dim+1)\n",
    "      ## concatenate forward and backward: batch_size x 2*(embedding_dim+1)\n",
    "      rnn_hidden_dp = torch.cat((rnn_hidden_dp_fw, rnn_hidden_dp_bw), dim=-1)\n",
    "      rnn_hidden_cp = torch.cat((rnn_hidden_cp_fw, rnn_hidden_cp_bw), dim=-1)\n",
    "      \n",
    "      # Scores\n",
    "      score_dp = self.fc_dp(self.dropout(rnn_hidden_dp))\n",
    "      score_cp = self.fc_cp(self.dropout(rnn_hidden_cp))\n",
    "\n",
    "      # Concatenate to variable collection\n",
    "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
    "      \n",
    "      # Final linear projection\n",
    "      out = self.fc_all(self.dropout(all)).squeeze()\n",
    "\n",
    "      return out, []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUOdeDecay(nn.Module):\n",
    "  \"\"\"\n",
    "  GRU RNN module where the hidden state decays according to an ODE.\n",
    "  (see Rubanova et al. 2019, Latent ODEs for Irregularly-Sampled Time Series)\n",
    "  \n",
    "  Args:\n",
    "    inputs: A `Tensor` with embeddings in the last dimension.\n",
    "    times: A `Tensor` with the same shape as inputs containing the recorded times (but no embedding dimension).\n",
    "\n",
    "  Returns:\n",
    "    outs: Hidden states of the RNN.\n",
    "  \"\"\"\n",
    "  def __init__(self, input_size, hidden_size, bias=True):\n",
    "    super(GRUOdeDecay, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.gru_cell = nn.GRUCell(input_size, hidden_size)\n",
    "    self.decays = nn.Parameter(torch.Tensor(hidden_size)) # exponential decays vector\n",
    "    \n",
    "    # ODE\n",
    "    self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    self.ode_net = ODENet(self.device, self.input_size, self.input_size, output_dim=self.input_size, augment_dim=0, time_dependent=False, non_linearity='softplus', tol=1e-3, adjoint=True)\n",
    "  \n",
    "  def forward(self, inputs, times):\n",
    "    # initializing and then calling cuda() later isn't working for some reason\n",
    "    if torch.cuda.is_available():\n",
    "      hn = torch.zeros(inputs.size(0), self.hidden_size).cuda() # batch_size x hidden_size\n",
    "      outs = torch.zeros(inputs.size(0), inputs.size(1), self.hidden_size).cuda() # batch_size x seq_len x hidden_size\n",
    "    else:\n",
    "      hn = torch.zeros(inputs.size(0), self.hidden_size) # batch_size x hidden_size\n",
    "      outs = torch.zeros(inputs.size(0), inputs.size(1), self.hidden_size) # batch_size x seq_len x hidden_size\n",
    "\n",
    "    # this is slow\n",
    "    for seq in range(inputs.size(1)):\n",
    "      hn = self.gru_cell(inputs[:,seq,:], hn)\n",
    "      outs[:,seq,:] = hn\n",
    "      \n",
    "      times_unique, inverse_indices = torch.unique(times[:,seq], sorted=True, return_inverse=True)\n",
    "      if times_unique.size(0) > 1:\n",
    "        hn = self.ode_net(hn, times_unique)\n",
    "        hn = hn[inverse_indices, torch.arange(0, inverse_indices.size(0)), :]\n",
    "    return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(torch.nn.Module):\n",
    "  \"\"\"\n",
    "  Dot-product attention module.\n",
    "  \n",
    "  Args:\n",
    "    inputs: A `Tensor` with embeddings in the last dimension.\n",
    "    mask: A `Tensor`. Dimensions are the same as inputs but without the embedding dimension.\n",
    "      Values are 0 for 0-padding in the input and 1 elsewhere.\n",
    "\n",
    "  Returns:\n",
    "    outputs: The input `Tensor` whose embeddings in the last dimension have undergone a weighted average.\n",
    "      The second-last dimension of the `Tensor` is removed.\n",
    "    attention_weights: weights given to each embedding.\n",
    "  \"\"\"\n",
    "  def __init__(self, embedding_dim):\n",
    "    super(Attention, self).__init__()\n",
    "    self.context = nn.Parameter(torch.Tensor(embedding_dim)) # context vector\n",
    "    self.linear_hidden = nn.Linear(embedding_dim, embedding_dim)\n",
    "    self.reset_parameters()\n",
    "    \n",
    "  def reset_parameters(self):\n",
    "    nn.init.normal_(self.context)\n",
    "\n",
    "  def forward(self, inputs, mask):\n",
    "    # Hidden representation of embeddings (no change in dimensions)\n",
    "    hidden = torch.tanh(self.linear_hidden(inputs))\n",
    "    # Compute weight of each embedding\n",
    "    importance = torch.sum(hidden * self.context, dim=-1)\n",
    "    importance = importance.masked_fill(mask == 0, -1e9)\n",
    "    # Softmax so that weights sum up to one\n",
    "    attention_weights = F.softmax(importance, dim=-1)\n",
    "    # Weighted sum of embeddings\n",
    "    weighted_projection = inputs * torch.unsqueeze(attention_weights, dim=-1)\n",
    "    # Output\n",
    "    outputs = torch.sum(weighted_projection, dim=-2)\n",
    "    return outputs, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_STEPS = 1000 \n",
    "\n",
    "class ODEFunc(nn.Module):\n",
    "    \"\"\"MLP modeling the derivative of ODE system.\n",
    "    Parameters\n",
    "    ----------\n",
    "    device : torch.device\n",
    "    data_dim : int\n",
    "        Dimension of data.\n",
    "    hidden_dim : int\n",
    "        Dimension of hidden layers.\n",
    "    augment_dim: int\n",
    "        Dimension of augmentation. If 0 does not augment ODE, otherwise augments\n",
    "        it with augment_dim dimensions.\n",
    "    time_dependent : bool\n",
    "        If True adds time as input, making ODE time dependent.\n",
    "    non_linearity : string\n",
    "        One of 'relu' and 'softplus'\n",
    "    \"\"\"\n",
    "    def __init__(self, device, data_dim, hidden_dim, augment_dim=0,\n",
    "                 time_dependent=False, non_linearity='relu'):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        self.device = device\n",
    "        self.augment_dim = augment_dim\n",
    "        self.data_dim = data_dim\n",
    "        self.input_dim = data_dim + augment_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.nfe = 0  # Number of function evaluations\n",
    "        self.time_dependent = time_dependent\n",
    "\n",
    "        if time_dependent:\n",
    "            self.fc1 = nn.Linear(self.input_dim + 1, hidden_dim)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(self.input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, self.input_dim)\n",
    "\n",
    "        if non_linearity == 'relu':\n",
    "            self.non_linearity = nn.ReLU(inplace=True)\n",
    "        elif non_linearity == 'softplus':\n",
    "            self.non_linearity = nn.Softplus()\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        t : torch.Tensor\n",
    "            Current time. Shape (1,).\n",
    "        x : torch.Tensor\n",
    "            Shape (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        # Forward pass of model corresponds to one function evaluation, so\n",
    "        # increment counter\n",
    "        self.nfe += 1\n",
    "        if self.time_dependent:\n",
    "            # Shape (batch_size, 1)\n",
    "            t_vec = torch.ones(x.shape[0], 1).to(self.device) * t\n",
    "            # Shape (batch_size, data_dim + 1)\n",
    "            t_and_x = torch.cat([t_vec, x], 1)\n",
    "            # Shape (batch_size, hidden_dim)\n",
    "            out = self.fc1(t_and_x)\n",
    "        else:\n",
    "            out = self.fc1(x)\n",
    "        out = self.non_linearity(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.non_linearity(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ODEBlock(nn.Module):\n",
    "    \"\"\"Solves ODE defined by odefunc.\n",
    "    Parameters\n",
    "    ----------\n",
    "    device : torch.device\n",
    "    odefunc : ODEFunc instance or anode.conv_models.ConvODEFunc instance\n",
    "        Function defining dynamics of system.\n",
    "    is_conv : bool\n",
    "        If True, treats odefunc as a convolutional model.\n",
    "    tol : float\n",
    "        Error tolerance.\n",
    "    adjoint : bool\n",
    "        If True calculates gradient with adjoint method, otherwise\n",
    "        backpropagates directly through operations of ODE solver.\n",
    "    \"\"\"\n",
    "    def __init__(self, device, odefunc, is_conv=False, tol=1e-3, adjoint=False):\n",
    "        super(ODEBlock, self).__init__()\n",
    "        self.adjoint = adjoint\n",
    "        self.device = device\n",
    "        self.is_conv = is_conv\n",
    "        self.odefunc = odefunc\n",
    "        self.tol = tol\n",
    "\n",
    "    def forward(self, x, eval_times=None):\n",
    "        \"\"\"Solves ODE starting from x.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape (batch_size, self.odefunc.data_dim)\n",
    "        eval_times : None or torch.Tensor\n",
    "            If None, returns solution of ODE at final time t=1. If torch.Tensor\n",
    "            then returns full ODE trajectory evaluated at points in eval_times.\n",
    "        \"\"\"\n",
    "        # Forward pass corresponds to solving ODE, so reset number of function\n",
    "        # evaluations counter\n",
    "        self.odefunc.nfe = 0\n",
    "        \n",
    "        if eval_times is None:\n",
    "            integration_time = torch.tensor([0, 1]).float().type_as(x)\n",
    "        else:\n",
    "            integration_time = eval_times.type_as(x)\n",
    "\n",
    "\n",
    "        if self.odefunc.augment_dim > 0:\n",
    "            if self.is_conv:\n",
    "                # Add augmentation\n",
    "                batch_size, channels, height, width = x.shape\n",
    "                aug = torch.zeros(batch_size, self.odefunc.augment_dim,\n",
    "                                  height, width).to(self.device)\n",
    "                # Shape (batch_size, channels + augment_dim, height, width)\n",
    "                x_aug = torch.cat([x, aug], 1)\n",
    "            else:\n",
    "                # Add augmentation\n",
    "                aug = torch.zeros(x.shape[0], self.odefunc.augment_dim).to(self.device)\n",
    "                # Shape (batch_size, data_dim + augment_dim)\n",
    "                x_aug = torch.cat([x, aug], 1)\n",
    "        else:\n",
    "            x_aug = x\n",
    "\n",
    "        if self.adjoint:\n",
    "            out = odeint_adjoint(self.odefunc, x_aug, integration_time,\n",
    "                                 rtol=self.tol, atol=self.tol, method='euler',\n",
    "                                 options={'max_num_steps': MAX_NUM_STEPS})\n",
    "        else:\n",
    "            out = odeint(self.odefunc, x_aug, integration_time,\n",
    "                         rtol=self.tol, atol=self.tol, method='euler',\n",
    "                         options={'max_num_steps': MAX_NUM_STEPS})\n",
    "\n",
    "        if eval_times is None:\n",
    "            return out[1]  # Return only final time\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class ODENet(nn.Module):\n",
    "    \"\"\"An ODEBlock followed by a Linear layer.\n",
    "    Parameters\n",
    "    ----------\n",
    "    device : torch.device\n",
    "    data_dim : int\n",
    "        Dimension of data.\n",
    "    hidden_dim : int\n",
    "        Dimension of hidden layers.\n",
    "    output_dim : int\n",
    "        Dimension of output after hidden layer. Should be 1 for regression or\n",
    "        num_classes for classification.\n",
    "    augment_dim: int\n",
    "        Dimension of augmentation. If 0 does not augment ODE, otherwise augments\n",
    "        it with augment_dim dimensions.\n",
    "    time_dependent : bool\n",
    "        If True adds time as input, making ODE time dependent.\n",
    "    non_linearity : string\n",
    "        One of 'relu' and 'softplus'\n",
    "    tol : float\n",
    "        Error tolerance.\n",
    "    adjoint : bool\n",
    "        If True calculates gradient with adjoint method, otherwise\n",
    "        backpropagates directly through operations of ODE solver.\n",
    "    \"\"\"\n",
    "    def __init__(self, device, data_dim, hidden_dim, output_dim=1,\n",
    "                 augment_dim=0, time_dependent=False, non_linearity='relu',\n",
    "                 tol=1e-3, adjoint=False):\n",
    "        super(ODENet, self).__init__()\n",
    "        self.device = device\n",
    "        self.data_dim = data_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.augment_dim = augment_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.time_dependent = time_dependent\n",
    "        self.tol = tol\n",
    "\n",
    "        odefunc = ODEFunc(device, data_dim, hidden_dim, augment_dim,\n",
    "                          time_dependent, non_linearity)\n",
    "\n",
    "        self.odeblock = ODEBlock(device, odefunc, tol=tol, adjoint=adjoint)\n",
    "\n",
    "    def forward(self, x, eval_times=None):\n",
    "        features = self.odeblock(x, eval_times)\n",
    "        return features\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class birnn_concat_time_delta_attention(nn.Module):\n",
    "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
    "      super(birnn_concat_time_delta_attention, self).__init__()\n",
    "      \n",
    "      # Embedding dimensions\n",
    "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))\n",
    "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))\n",
    "\n",
    "      # Embedding layers\n",
    "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
    "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
    "\n",
    "      # GRU layers\n",
    "      self.gru_dp_fw = nn.GRU(input_size=self.embed_dp_dim+1, hidden_size=self.embed_dp_dim+1, num_layers=1, batch_first=True)\n",
    "      self.gru_cp_fw = nn.GRU(input_size=self.embed_cp_dim+1, hidden_size=self.embed_cp_dim+1, num_layers=1, batch_first=True)\n",
    "      self.gru_dp_bw = nn.GRU(input_size=self.embed_dp_dim+1, hidden_size=self.embed_dp_dim+1, num_layers=1, batch_first=True)\n",
    "      self.gru_cp_bw = nn.GRU(input_size=self.embed_cp_dim+1, hidden_size=self.embed_cp_dim+1, num_layers=1, batch_first=True)\n",
    "\n",
    "      # Attention layers\n",
    "      self.attention_dp = Attention(embedding_dim=2*(self.embed_dp_dim+1)) #+1 for the concatenated time\n",
    "      self.attention_cp = Attention(embedding_dim=2*(self.embed_cp_dim+1))\n",
    "            \n",
    "      # Fully connected output\n",
    "      self.fc_dp  = nn.Linear(2*(self.embed_dp_dim+1), 1)\n",
    "      self.fc_cp  = nn.Linear(2*(self.embed_cp_dim+1), 1)\n",
    "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
    "      \n",
    "      # Others\n",
    "      self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
    "      # Compute time delta\n",
    "      ## output dim: batch_size x seq_len\n",
    "      dp_t_delta_fw = abs_time_to_delta(dp_t)\n",
    "      cp_t_delta_fw = abs_time_to_delta(cp_t)\n",
    "      dp_t_delta_bw = abs_time_to_delta(torch.flip(dp_t, [1]))\n",
    "      cp_t_delta_bw = abs_time_to_delta(torch.flip(cp_t, [1]))    \n",
    "    \n",
    "      # Embedding\n",
    "      ## output dim: batch_size x seq_len x embedding_dim\n",
    "      embedded_dp_fw = self.embed_dp(dp)\n",
    "      embedded_cp_fw = self.embed_cp(cp)\n",
    "      embedded_dp_bw = torch.flip(embedded_dp_fw, [1])\n",
    "      embedded_cp_bw = torch.flip(embedded_cp_fw, [1])\n",
    "      \n",
    "      # Concatate with time\n",
    "      ## output dim: batch_size x seq_len x (embedding_dim+1)\n",
    "      concat_dp_fw = torch.cat((embedded_dp_fw, torch.unsqueeze(dp_t_delta_fw, dim=-1)), dim=-1)\n",
    "      concat_cp_fw = torch.cat((embedded_cp_fw, torch.unsqueeze(cp_t_delta_fw, dim=-1)), dim=-1)\n",
    "      concat_dp_bw = torch.cat((embedded_dp_bw, torch.unsqueeze(dp_t_delta_bw, dim=-1)), dim=-1)\n",
    "      concat_cp_bw = torch.cat((embedded_cp_bw, torch.unsqueeze(cp_t_delta_bw, dim=-1)), dim=-1)\n",
    "      ## Dropout\n",
    "      concat_dp_fw = self.dropout(concat_dp_fw)\n",
    "      concat_cp_fw = self.dropout(concat_cp_fw)\n",
    "      concat_dp_bw = self.dropout(concat_dp_bw)\n",
    "      concat_cp_bw = self.dropout(concat_cp_bw)\n",
    "      \n",
    "      # GRU\n",
    "      ## output dim rnn:        batch_size x seq_len x (embedding_dim+1)\n",
    "      ## output dim rnn_hidden: batch_size x 1 x (embedding_dim+1)\n",
    "      rnn_dp_fw, rnn_hidden_dp_fw = self.gru_dp_fw(concat_dp_fw)\n",
    "      rnn_cp_fw, rnn_hidden_cp_fw = self.gru_cp_fw(concat_cp_fw)\n",
    "      rnn_dp_bw, rnn_hidden_dp_bw = self.gru_dp_bw(concat_dp_bw)\n",
    "      rnn_cp_bw, rnn_hidden_cp_bw = self.gru_cp_bw(concat_cp_bw)      \n",
    "      # concatenate forward and backward\n",
    "      ## output dim: batch_size x seq_len x 2*(embedding_dim+1)\n",
    "      rnn_dp = torch.cat((rnn_dp_fw, torch.flip(rnn_dp_bw, [1])), dim=-1)\n",
    "      rnn_cp = torch.cat((rnn_cp_fw, torch.flip(rnn_cp_bw, [1])), dim=-1)\n",
    "\n",
    "      # Attention\n",
    "      ## output dim: batch_size x 2*(embedding_dim+1)\n",
    "      attended_dp, weights_dp = self.attention_dp(rnn_dp, (dp > 0).float())\n",
    "      attended_cp, weights_cp = self.attention_cp(rnn_cp, (cp > 0).float())\n",
    "      \n",
    "      # Scores\n",
    "      score_dp = self.fc_dp(self.dropout(attended_dp))\n",
    "      score_cp = self.fc_cp(self.dropout(attended_cp))\n",
    "\n",
    "      # Concatenate to variable collection\n",
    "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
    "      \n",
    "      # Final linear projection\n",
    "      out = self.fc_all(self.dropout(all)).squeeze()\n",
    "\n",
    "      return out, []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class birnn_ode_decay(nn.Module):\n",
    "    def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
    "      super(birnn_ode_decay, self).__init__()\n",
    "      \n",
    "      # Embedding dimensions\n",
    "      self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))+1\n",
    "      self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))+1\n",
    "\n",
    "      # Embedding layers\n",
    "      self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
    "      self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
    "\n",
    "      # GRU layers\n",
    "      self.gru_dp_fw = GRUOdeDecay(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim)\n",
    "      self.gru_cp_fw = GRUOdeDecay(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim)\n",
    "      self.gru_dp_bw = GRUOdeDecay(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim)\n",
    "      self.gru_cp_bw = GRUOdeDecay(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim)\n",
    "      \n",
    "      # Fully connected output\n",
    "      self.fc_dp  = nn.Linear(2*self.embed_dp_dim, 1)\n",
    "      self.fc_cp  = nn.Linear(2*self.embed_cp_dim, 1)\n",
    "      self.fc_all = nn.Linear(num_static + 2, 1)\n",
    "      \n",
    "      # Others\n",
    "      self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, stat, dp, cp, dp_t, cp_t):\n",
    "      # Compute time delta\n",
    "      ## output dim: batch_size x seq_len\n",
    "      dp_t_delta_fw = abs_time_to_delta(dp_t)\n",
    "      cp_t_delta_fw = abs_time_to_delta(cp_t)\n",
    "      ## Round\n",
    "      dp_t_delta_fw = torch.round(100*dp_t_delta_fw)/100\n",
    "      cp_t_delta_fw = torch.round(100*cp_t_delta_fw)/100            \n",
    "      dp_t_delta_bw = abs_time_to_delta(torch.flip(dp_t, [1]))\n",
    "      cp_t_delta_bw = abs_time_to_delta(torch.flip(cp_t, [1]))    \n",
    "    \n",
    "      # Embedding\n",
    "      ## output dim: batch_size x seq_len x embedding_dim\n",
    "      embedded_dp_fw = self.embed_dp(dp)\n",
    "      embedded_cp_fw = self.embed_cp(cp)\n",
    "      embedded_dp_bw = torch.flip(embedded_dp_fw, [1])\n",
    "      embedded_cp_bw = torch.flip(embedded_cp_fw, [1])\n",
    "      ## Dropout\n",
    "      embedded_dp_fw = self.dropout(embedded_dp_fw)\n",
    "      embedded_cp_fw = self.dropout(embedded_cp_fw)\n",
    "      embedded_dp_bw = self.dropout(embedded_dp_bw)\n",
    "      embedded_cp_bw = self.dropout(embedded_cp_bw)\n",
    "      \n",
    "      # GRU\n",
    "      ## output dim rnn:        batch_size x seq_len x embedding_dim\n",
    "      rnn_dp_fw = self.gru_dp_fw(embedded_dp_fw, dp_t_delta_fw)\n",
    "      rnn_cp_fw = self.gru_cp_fw(embedded_cp_fw, cp_t_delta_fw)\n",
    "      rnn_dp_bw = self.gru_dp_bw(embedded_dp_bw, dp_t_delta_bw)\n",
    "      rnn_cp_bw = self.gru_cp_bw(embedded_cp_bw, cp_t_delta_bw)      \n",
    "      ## output dim rnn_hidden: batch_size x embedding_dim\n",
    "      rnn_dp_fw = rnn_dp_fw[:,-1,:]\n",
    "      rnn_cp_fw = rnn_cp_fw[:,-1,:]\n",
    "      rnn_dp_bw = rnn_dp_bw[:,-1,:]\n",
    "      rnn_cp_bw = rnn_cp_bw[:,-1,:]\n",
    "      ## concatenate forward and backward: batch_size x 2*embedding_dim\n",
    "      rnn_dp = torch.cat((rnn_dp_fw, rnn_dp_bw), dim=-1)\n",
    "      rnn_cp = torch.cat((rnn_cp_fw, rnn_cp_bw), dim=-1)\n",
    "      \n",
    "      # Scores\n",
    "      score_dp = self.fc_dp(self.dropout(rnn_dp))\n",
    "      score_cp = self.fc_cp(self.dropout(rnn_cp))\n",
    "\n",
    "      # Concatenate to variable collection\n",
    "      all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
    "      \n",
    "      # Final linear projection\n",
    "      out = self.fc_all(self.dropout(all)).squeeze()\n",
    "\n",
    "      return out, []      \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzVUQS0CHry0"
   },
   "source": [
    "### Training Code\n",
    "\n",
    "Training each model uses the data_arrays.npz file generated from the preprocessing steps discussed earlier in this notebook.  This file is approximately 430MB, so it was too large to upload to GitHub.  Training each model for 80 epochs (the number specified in the GitHub repo associated with the original paper) takes approximately 6-8 hours running from a terminal in a M2 16GB Macbook Pro.  The implementation in the section below largely adheres to the implementation from the orignal paper's repo as well.\n",
    "\n",
    "train - function that trains the model passed and captures the results in the \"logdir\" folder\n",
    "test  - function that tests the trained model and captures the performance metrics used for evaluation in the \"results\" folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n"
     ]
    }
   ],
   "source": [
    "def num_static(data):\n",
    "  return data['static_vars'].shape[0]\n",
    "\n",
    "def vocab_sizes(data):\n",
    "  return data['dp'].max()+1, data['cp'].max()+1\n",
    "\n",
    "def abs_time_to_delta(times):\n",
    "  delta = torch.cat((torch.unsqueeze(times[:, 0], dim=-1), times[:, 1:] - times[:, :-1]), dim=1)\n",
    "  delta = torch.clamp(delta, min=0)\n",
    "  return delta\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "print('Load data...')\n",
    "current_dir = os.getcwd()\n",
    "data = np.load(current_dir + '\\data\\data_arrays.npz')\n",
    "# Vocabulary sizes\n",
    "num_static = num_static(data)\n",
    "num_dp_codes, num_cp_codes = vocab_sizes(data)\n",
    "\n",
    "def train(net,model_name, num_epochs):\n",
    "    trainloader, num_batches, pos_weight = get_trainloader(data, 'TRAIN')\n",
    "\n",
    "     \n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    print('Start Train...' + model_name)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = optim.Adam(net.parameters(), lr = 0.001)  \n",
    "    \n",
    "    # Create log dir\n",
    "    logdir = current_dir + '\\\\logdir\\\\' + model_name + '/'\n",
    "    # logdir = hp.logdir + hp.net_variant + '/'\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "    \n",
    "    # Store times\n",
    "    epoch_times = []\n",
    "    \n",
    "    # Train\n",
    "    for epoch in tqdm(range(num_epochs)): \n",
    "    # print('-----------------------------------------')\n",
    "    # print('Epoch: {}'.format(epoch))\n",
    "        net.train()\n",
    "        time_start = time()\n",
    "        for i, (stat, dp, cp, dp_t, cp_t, label) in enumerate(tqdm(trainloader), 0):\n",
    "          # move to GPU if available\n",
    "          stat  = stat.to(device)\n",
    "          dp    = dp.to(device)\n",
    "          cp    = cp.to(device)\n",
    "          dp_t  = dp_t.to(device)\n",
    "          cp_t  = cp_t.to(device)\n",
    "          label = label.to(device)\n",
    "        \n",
    "          # zero the parameter gradients\n",
    "          optimizer.zero_grad()\n",
    "        \n",
    "          # forward + backward + optimize\n",
    "          label_pred, _ = net(stat, dp, cp, dp_t, cp_t)\n",
    "          loss = criterion(label_pred, label)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "    \n",
    "    # timing\n",
    "    time_end = time()\n",
    "    epoch_times.append(time_end-time_start)\n",
    "    \n",
    "    # Save\n",
    "    print('Saving...')\n",
    "    torch.save(net.state_dict(), logdir + 'final_model.pt')\n",
    "    np.savez(logdir + 'epoch_times', epoch_times=epoch_times)\n",
    "    print('Done Train for ' + model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_seed = 1234\n",
    "bootstrap_samples = 100\n",
    "\n",
    "def num_static1(data):\n",
    "  return data['static_vars'].shape[0]\n",
    "\n",
    "def vocab_sizes(data):\n",
    "  return data['dp'].max()+1, data['cp'].max()+1\n",
    "\n",
    "def roundval(val):\n",
    "    if math.isnan(val):\n",
    "        return\"nan\"\n",
    "    else:\n",
    "        return round(val)\n",
    "        \n",
    "def round(num):\n",
    "  return np.round(num*1000)/1000\n",
    "    \n",
    "\n",
    "def test(net, model_name):\n",
    "    # Load data\n",
    "    # print('Load data...')\n",
    "    # # data = np.load(data_dir + 'data_arrays.npz')\n",
    "    # data = np.load(data_dir + 'data_arrays.npz', allow_pickle=True)\n",
    "    # # \n",
    "    # test_ids_patients = pd.read_pickle(data_dir + 'test_ids_patients.pkl')\n",
    "\n",
    "    print(\"called\")\n",
    "    data = np.load(current_dir + '\\\\data\\data_arrays.npz')\n",
    "    test_ids_patients = pd.read_pickle(current_dir + '/data/test_ids_patients.pkl')\n",
    "    \n",
    "    # Patients in test data\n",
    "    patients = test_ids_patients.drop_duplicates()\n",
    "    num_patients = patients.shape[0]\n",
    "    row_ids = pd.DataFrame({'ROW_IDX': test_ids_patients.index}, index=test_ids_patients)\n",
    "    \n",
    "    # Vocabulary sizes\n",
    "    num_static = num_static1(data)\n",
    "    num_dp_codes, num_cp_codes = vocab_sizes(data)\n",
    "    \n",
    "    # CUDA for PyTorch\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # Network\n",
    "    # net = Net(num_static, num_dp_codes, num_cp_codes).to(device)\n",
    "    \n",
    "    print('Evaluate...')\n",
    "    # Set log dir to read trained model from\n",
    "    # logdir = logdir + model_name + '/'\n",
    "    logdir = current_dir + '\\\\logdir\\\\' + model_name + '\\\\'\n",
    "    print(logdir)\n",
    "    \n",
    "    # Restore variables from disk\n",
    "    net.load_state_dict(torch.load(logdir + 'final_model.pt', map_location=device))\n",
    "    \n",
    "    # Bootstrapping\n",
    "    np.random.seed(np_seed)\n",
    "    avpre_vec = np.zeros(bootstrap_samples)\n",
    "    auroc_vec = np.zeros(bootstrap_samples)\n",
    "    f1_vec    = np.zeros(bootstrap_samples)\n",
    "    sensitivity_vec = np.zeros(bootstrap_samples)\n",
    "    specificity_vec = np.zeros(bootstrap_samples)\n",
    "    ppv_vec = np.zeros(bootstrap_samples)\n",
    "    npv_vec = np.zeros(bootstrap_samples)\n",
    "\n",
    "    for sample in range(bootstrap_samples):\n",
    "        print('Bootstrap sample {}'.format(sample))\n",
    "        \n",
    "        # Test data\n",
    "        sample_patients = patients.sample(n=num_patients, replace=True)\n",
    "        idx = np.squeeze(row_ids.loc[sample_patients].values)\n",
    "        testloader, _, _ = get_trainloader(data, 'TEST', shuffle=False, idx=idx)\n",
    "        \n",
    "        # evaluate on test data\n",
    "        net.eval()\n",
    "        label_pred = torch.Tensor([])\n",
    "        label_test = torch.Tensor([])\n",
    "        with torch.no_grad():\n",
    "            for i, (stat, dp, cp, dp_t, cp_t, label_batch) in enumerate(tqdm(testloader), 0):\n",
    "                # move to GPU if available\n",
    "                stat  = stat.to(device)\n",
    "                dp    = dp.to(device)\n",
    "                cp    = cp.to(device)\n",
    "                dp_t  = dp_t.to(device)\n",
    "                cp_t  = cp_t.to(device)\n",
    "        \n",
    "                label_pred_batch, _ = net(stat, dp, cp, dp_t, cp_t)\n",
    "                label_pred = torch.cat((label_pred, label_pred_batch.cpu()))\n",
    "                label_test = torch.cat((label_test, label_batch))\n",
    "\n",
    "        label_sigmoids = torch.sigmoid(label_pred).cpu().numpy()\n",
    "        \n",
    "        # Average precision\n",
    "        avpre = average_precision_score(label_test, label_sigmoids)\n",
    "        \n",
    "        # Determine AUROC score\n",
    "        auroc = roc_auc_score(label_test, label_sigmoids)\n",
    "        \n",
    "        # Sensitivity, specificity\n",
    "        fpr, tpr, thresholds = roc_curve(label_test, label_sigmoids)\n",
    "        youden_idx = np.argmax(tpr - fpr)\n",
    "        sensitivity = tpr[youden_idx]\n",
    "        specificity = 1-fpr[youden_idx]\n",
    "        \n",
    "        # F1, PPV, NPV score\n",
    "        f1 = 0\n",
    "        ppv = 0\n",
    "        npv = 0\n",
    "        for t in thresholds:\n",
    "            label_pred = (np.array(label_sigmoids) >= t).astype(int)\n",
    "            f1_temp = f1_score(label_test, label_pred)\n",
    "            ppv_temp = precision_score(label_test, label_pred, pos_label=1)\n",
    "            npv_temp = precision_score(label_test, label_pred, pos_label=0)\n",
    "            if f1_temp > f1:\n",
    "                f1 = f1_temp\n",
    "            if (ppv_temp+npv_temp) > (ppv+npv):\n",
    "                ppv = ppv_temp\n",
    "                npv = npv_temp\n",
    "\n",
    "        # print(f1)\n",
    "        # Store in vectors\n",
    "        avpre_vec[sample] = avpre\n",
    "        auroc_vec[sample] = auroc\n",
    "        f1_vec[sample]    = f1\n",
    "        sensitivity_vec[sample]  = sensitivity\n",
    "        specificity_vec[sample]  = specificity\n",
    "        ppv_vec[sample]  = ppv\n",
    "        npv_vec[sample]  = npv\n",
    "\n",
    "    #     print(avpre_vec)\n",
    "    # print('==')\n",
    "    # print(avpre_vec)\n",
    "    avpre_mean = np.mean(avpre_vec)\n",
    "    # print('avpre_mean')\n",
    "    # print(avpre_mean)\n",
    "    \n",
    "    avpre_lci, avpre_uci = st.t.interval(0.95, bootstrap_samples-1, loc=avpre_mean, scale=st.sem(avpre_vec))\n",
    "    auroc_mean = np.mean(auroc_vec)\n",
    "    auroc_lci, auroc_uci = st.t.interval(0.95, bootstrap_samples-1, loc=auroc_mean, scale=st.sem(auroc_vec))\n",
    "    f1_mean = np.mean(f1_vec)\n",
    "    f1_lci, f1_uci = st.t.interval(0.95, bootstrap_samples-1, loc=f1_mean, scale=st.sem(f1_vec))\n",
    "    ppv_mean = np.mean(ppv_vec)\n",
    "    ppv_lci, ppv_uci = st.t.interval(0.95, bootstrap_samples-1, loc=ppv_mean, scale=st.sem(ppv_vec))\n",
    "    npv_mean = np.mean(npv_vec)\n",
    "    npv_lci, npv_uci = st.t.interval(0.95, bootstrap_samples-1, loc=npv_mean, scale=st.sem(npv_vec))\n",
    "    sensitivity_mean = np.mean(sensitivity_vec)\n",
    "    sensitivity_lci, sensitivity_uci = st.t.interval(0.95, bootstrap_samples-1, loc=sensitivity_mean, scale=st.sem(sensitivity_vec))\n",
    "    specificity_mean = np.mean(specificity_vec)\n",
    "    specificity_lci, specificity_uci = st.t.interval(0.95, bootstrap_samples-1, loc=specificity_mean, scale=st.sem(specificity_vec))\n",
    "    \n",
    "    # epoch_times = np.load(logdir + net_variant + '/epoch_times.npz')['epoch_times']\n",
    "    epoch_times = np.load(logdir +  'epoch_times.npz')['epoch_times']\n",
    "    # net.load_state_dict(torch.load(logdir + 'final_model.pt', map_location=device))\n",
    "    times_mean = np.mean(epoch_times)\n",
    "    times_lci, times_uci = st.t.interval(0.95, len(epoch_times)-1, loc=np.mean(epoch_times), scale=st.sem(epoch_times))\n",
    "    times_std = np.std(epoch_times)\n",
    "\n",
    "    print('------------------------------------------------')\n",
    "    print('Net variant: {}'.format(model_name))\n",
    "    print('Average Precision: {} [{},{}]'.format(round(avpre_mean), round(avpre_lci), round(avpre_uci)))\n",
    "    print('AUROC: {} [{},{}]'.format(round(auroc_mean), round(auroc_lci), round(auroc_uci)))\n",
    "    print('F1: {} [{},{}]'.format(round(f1_mean), round(f1_lci), round(f1_uci)))\n",
    "    print('PPV: {} [{},{}]'.format(round(ppv_mean), round(ppv_lci), round(ppv_uci)))\n",
    "    print('NPV: {} [{},{}]'.format(round(npv_mean), round(npv_lci), round(npv_uci)))\n",
    "    print('Sensitivity: {} [{},{}]'.format(round(sensitivity_mean), round(sensitivity_lci), round(sensitivity_uci)))\n",
    "    print('Specificity: {} [{},{}]'.format(round(specificity_mean), round(specificity_lci), round(specificity_uci)))\n",
    "    print('Time: {} [{},{}] std: {}'.format(round(times_mean), round(times_lci), round(times_uci), round(times_std)))\n",
    "    print('Done')\n",
    "\n",
    "    results_file = current_dir + '\\\\results\\\\' + model_name + '_results.txt'\n",
    "    with open(results_file, 'a') as f:\n",
    "        f.write('\\n')\n",
    "        f.write('\\nNet variant: {}'.format(model_name))\n",
    "        f.write('\\nAverage Precision: {} [{},{}]'.format(round(avpre_mean), round(avpre_lci), round(avpre_uci)))\n",
    "        f.write('\\nAUROC: {} [{},{}]'.format(round(auroc_mean), round(auroc_lci), round(auroc_uci)))\n",
    "        f.write('\\nF1: {} [{},{}]'.format(round(f1_mean), round(f1_lci), round(f1_uci)))\n",
    "        f.write('\\nPPV: {} [{},{}]'.format(round(ppv_mean), round(ppv_lci), round(ppv_uci)))\n",
    "        f.write('\\nNPV: {} [{},{}]'.format(round(npv_mean), round(npv_lci), round(npv_uci)))\n",
    "        f.write('\\nSensitivity: {} [{},{}]'.format(round(sensitivity_mean), round(sensitivity_lci), round(sensitivity_uci)))\n",
    "        f.write('\\nSpecificity: {} [{},{}]'.format(round(specificity_mean), round(specificity_lci), round(specificity_uci)))\n",
    "        f.write('\\nTime: {} [{},{}] std: {}'.format(round(times_mean), round(times_lci), round(times_uci), round(times_std)))\n",
    "        f.write('\\n Test Complete')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 80\n",
    "model=birnn_concat_time_delta(num_static, num_dp_codes, num_cp_codes)\n",
    "# train(model, 'birnn_concat_time_delta',num_epochs)\n",
    "test(model, 'birnn_concat_time_delta')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 80\n",
    "model=birnn_concat_time_delta_attention(num_static, num_dp_codes, num_cp_codes)\n",
    "# train(model, 'birnn_concat_time_delta',num_epochs)\n",
    "test(model, 'birnn_concat_time_delta_attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=birnn_ode_decay(num_static, num_dp_codes, num_cp_codes)\n",
    "# train(model, 'birnn_ode_decay',num_epochs)\n",
    "\n",
    "# test(model, 'birnn_ode_decay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX6bCcZNuxmz"
   },
   "source": [
    "# (TODO) Results\n",
    "\n",
    "According to our paper, the paper's results indicate that the deep learning architectures, particularly those with a recurrent component, outperformed the logistic regression baseline model in predicting ICU readmissions.\n",
    "\n",
    "Baseline Characteristics: The analysis involved 23 static variables, 992 unique ICD-9 diagnosis codes, 298 unique ICD-9 procedure codes, 586 unique medications, and 32 codes related to vital signs. Each patient's electronic medical record (EMR) contained a maximum of 552 ICD-9 diagnosis and procedure codes and 392 medications and vital sign codes associated with the current ICU stay.\n",
    "\n",
    "Performance Trends: Models with a recurrent component generally performed better (average precision range: 0.298â€“0.331) than those based solely on attention layers (average precision range: 0.269â€“0.294). This suggests that incorporating recurrent neural network (RNN) components improved the predictive power for ICU readmissions.\n",
    "\n",
    "Therefore, the deep learning architectures, especially those combining recurrent components, showed significantly improved predictive accuracy compared to traditional logistic regression models when predicting ICU readmissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the table of our result from the each model we tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of our training model\n",
      "Net variant                        Average Precision    AUROC               F1                   PPV                  NPV                  Sensitivity          Specificity          Time                           Time Std\n",
      "---------------------------------  -------------------  ------------------  -------------------  -------------------  -------------------  -------------------  -------------------  ---------------------------  ----------\n",
      "birnn_concat_time_delta            0.315 [0.307,0.322]  0.737 [0.734,0.74]  0.373 [0.369,0.378]  0.999 [0.996,1.001]  0.882 [0.881,0.884]  0.68 [0.666,0.695]   0.694 [0.681,0.708]  236.483 [nan,nan]                 0\n",
      "birnn_concat_time_delta_attention  0.312 [0.303,0.32]   0.737 [0.734,0.74]  0.365 [0.36,0.369]   0.999 [0.955,0.991]  0.882 [0.881,0.884]  0.676 [0.66,0.693]   0.686 [0.669,0.704]  1608.401 [212.281,3004.521]    6234.26\n",
      "birnn_ode_decay                    0.307 [0.299,0.315]  0.74 [0.737,0.742]  0.373 [0.368,0.378]  0.976 [0.956,0.995]  0.883 [0.881,0.884]  0.661 [0.646,0.676]  0.71 [0.694,0.726]   385.219 [365.704,404.734]        87.142\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "results = {\n",
    "    \"birnn_concat_time_delta\": {\n",
    "        \"Net variant\": \"birnn_concat_time_delta\",\n",
    "        \"Average Precision\": \"0.315 [0.307,0.322]\",\n",
    "        \"AUROC\": \"0.737 [0.734,0.74]\",\n",
    "        \"F1\": \"0.373 [0.369,0.378]\",\n",
    "        \"PPV\": \"0.999 [0.996,1.001]\",\n",
    "        \"NPV\": \"0.882 [0.881,0.884]\",\n",
    "        \"Sensitivity\": \"0.68 [0.666,0.695]\",\n",
    "        \"Specificity\": \"0.694 [0.681,0.708]\",\n",
    "        \"Time\": \"236.483 [nan,nan]\",\n",
    "        \"Time Std\": \"0.0\"\n",
    "    },\n",
    "    \"birnn_concat_time_delta_attention\": {\n",
    "        \"Net variant\": \"birnn_concat_time_delta_attention\",\n",
    "        \"Average Precision\": \"0.312 [0.303,0.32]\",\n",
    "        \"AUROC\": \"0.737 [0.734,0.74]\",\n",
    "        \"F1\": \"0.365 [0.36,0.369]\",\n",
    "        \"PPV\": \"0.999 [0.955,0.991]\",\n",
    "        \"NPV\": \"0.882 [0.881,0.884]\",\n",
    "        \"Sensitivity\": \"0.676 [0.66,0.693]\",\n",
    "        \"Specificity\": \"0.686 [0.669,0.704]\",\n",
    "        \"Time\": \"1608.401 [212.281,3004.521]\",\n",
    "        \"Time Std\": \"6234.259\"\n",
    "    },\n",
    "    \"birnn_ode_decay\": {\n",
    "        \"Net variant\": \"birnn_ode_decay\",\n",
    "        \"Average Precision\": \"0.307 [0.299,0.315]\",\n",
    "        \"AUROC\": \"0.74 [0.737,0.742]\",\n",
    "        \"F1\": \"0.373 [0.368,0.378]\",\n",
    "        \"PPV\": \"0.976 [0.956,0.995]\",\n",
    "        \"NPV\": \"0.883 [0.881,0.884]\",\n",
    "        \"Sensitivity\": \"0.661 [0.646,0.676]\",\n",
    "        \"Specificity\": \"0.71 [0.694,0.726]\",\n",
    "        \"Time\": \"385.219 [365.704,404.734]\",\n",
    "        \"Time Std\": \"87.142\"\n",
    "    }\n",
    "}\n",
    "\n",
    "headers = [\"Net variant\", \"Average Precision\", \"AUROC\", \"F1\", \"PPV\", \"NPV\", \"Sensitivity\", \"Specificity\", \"Time\", \"Time Std\"]\n",
    "\n",
    "data = []\n",
    "for result in results.values():\n",
    "    data.append([result[key] for key in headers])\n",
    "\n",
    "print(\"Results of our training model\")\n",
    "print(tabulate(data, headers=headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the table of Summary statistics (mean, [95% confidence interval]) for the different algorithms used to predict readmission within 30 days of discharge from the intensive care unit from the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of model from the paper\n",
      "Net variant                           Average Precision    AUROC                F1                   Sensitivity          Specificity\n",
      "------------------------------------  -------------------  -------------------  -------------------  -------------------  -------------------\n",
      "ODE + RNN + Attention                 0.314 [0.306,0.321]  0.739 [0.736,0.741]  0.376 [0.371,0.381]  0.685 [0.666,0.704]  0.677 [0.658,0.696]\n",
      "ODE + RNN                             0.331 [0.323,0.339]  0.739 [0.737,0.742]  0.372 [0.367,0.377]  0.672 [0.659,0.686]  0.697 [0.683,0.711]\n",
      "RNN (ODE time decay) + Attention      0.316 [0.307,0.324]  0.743 [0.741,0.746]  0.375 [0.370,0.379]  0.648 [0.641,0.656]  0.733 [0.726,0.739]\n",
      "RNN (ODE time decay)                  0.300 [0.293,0.308]  0.741 [0.738,0.744]  0.372 [0.367,0.376]  0.710 [0.698,0.722]  0.667 [0.655,0.679]\n",
      "RNN (exp time decay) + Attention      0.320 [0.312,0.328]  0.748 [0.745,0.751]  0.377 [0.372,0.382]  0.704 [0.692,0.715]  0.680 [0.668,0.692]\n",
      "RNN (exp time decay)                  0.304 [0.297,0.311]  0.735 [0.732,0.738]  0.368 [0.363,0.373]  0.707 [0.700,0.714]  0.670 [0.663,0.676]\n",
      "RNN (concatenated Î”time) + Attention  0.312 [0.303,0.320]  0.741 [0.739,0.744]  0.368 [0.363,0.372]  0.687 [0.680,0.695]  0.688 [0.681,0.696]\n",
      "RNN (concatenated Î”time)              0.311 [0.303,0.320]  0.739 [0.737,0.742]  0.364 [0.359,0.369]  0.698 [0.692,0.704]  0.688 [0.684,0.693]\n",
      "ODE + Attention                       0.294 [0.285,0.302]  0.717 [0.714,0.720]  0.333 [0.328,0.339]  0.776 [0.768,0.784]  0.554 [0.548,0.560]\n",
      "Attention (concatenated time)         0.286 [0.277,0.295]  0.711 [0.709,0.714]  0.330 [0.325,0.334]  0.700 [0.686,0.714]  0.614 [0.601,0.628]\n",
      "MCE + RNN + Attention                 0.317 [0.308,0.325]  0.736 [0.734,0.739]  0.373 [0.369,0.378]  0.630 [0.622,0.638]  0.744 [0.738,0.749]\n",
      "MCE + RNN                             0.298 [0.291,0.306]  0.727 [0.724,0.730]  0.361 [0.357,0.366]  0.654 [0.645,0.663]  0.706 [0.697,0.715]\n",
      "MCE + Attention                       0.269 [0.261,0.278]  0.689 [0.686,0.692]  0.312 [0.308,0.316]  0.686 [0.676,0.695]  0.616 [0.607,0.625]\n",
      "Logistic Regression                   0.257 [0.248,0.266]  0.659 [0.656,0.663]  0.296 [0.291,0.300]  0.606 [0.597,0.615]  0.647 [0.639,0.655]\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "results = {\n",
    "    \"ODE + RNN + Attention\": {\n",
    "        \"Net variant\": \"ODE + RNN + Attention\",\n",
    "        \"Average Precision\": \"0.314 [0.306,0.321]\",\n",
    "        \"AUROC\": \"0.739 [0.736,0.741]\",\n",
    "        \"F1\": \"0.376 [0.371,0.381]\",\n",
    "        \"Sensitivity\": \"0.685 [0.666,0.704]\",\n",
    "        \"Specificity\": \"0.677 [0.658,0.696]\",\n",
    "    },\n",
    "\n",
    "    \"ODE + RNN\": {\n",
    "        \"Net variant\": \"ODE + RNN\",\n",
    "        \"Average Precision\": \"0.331 [0.323,0.339]\",\n",
    "        \"AUROC\": \"0.739 [0.737,0.742]\",\n",
    "        \"F1\": \"0.372 [0.367,0.377]\",\n",
    "        \"Sensitivity\": \"0.672 [0.659,0.686]\",\n",
    "        \"Specificity\": \"0.697 [0.683,0.711]\",\n",
    "    },\n",
    "    \"RNN (ODE time decay) + Attention\": {\n",
    "        \"Net variant\": \"RNN (ODE time decay) + Attention\",\n",
    "        \"Average Precision\": \"0.316 [0.307,0.324]\",\n",
    "        \"AUROC\": \"0.743 [0.741,0.746]\",\n",
    "        \"F1\": \"0.375 [0.370,0.379]\",\n",
    "        \"Sensitivity\": \"0.648 [0.641,0.656]\",\n",
    "        \"Specificity\": \"0.733 [0.726,0.739]\",\n",
    "    },\n",
    "    \"RNN (ODE time decay)\": {\n",
    "        \"Net variant\": \"RNN (ODE time decay)\",\n",
    "        \"Average Precision\": \"0.300 [0.293,0.308]\",\n",
    "        \"AUROC\": \"0.741 [0.738,0.744]\",\n",
    "        \"F1\": \"0.372 [0.367,0.376]\",\n",
    "        \"Sensitivity\": \"0.710 [0.698,0.722]\",\n",
    "        \"Specificity\": \"0.667 [0.655,0.679]\",\n",
    "    },\n",
    "    \"RNN (exp time decay) + Attention\": {\n",
    "        \"Net variant\": \"RNN (exp time decay) + Attention\",\n",
    "        \"Average Precision\": \"0.320 [0.312,0.328]\",\n",
    "        \"AUROC\": \"0.748 [0.745,0.751]\",\n",
    "        \"F1\": \"0.377 [0.372,0.382] \",\n",
    "        \"Sensitivity\": \"0.704 [0.692,0.715]\",\n",
    "        \"Specificity\": \"0.680 [0.668,0.692]\",\n",
    "    },\n",
    "    \"RNN (exp time decay)\": {\n",
    "        \"Net variant\": \"RNN (exp time decay)\",\n",
    "        \"Average Precision\": \"0.304 [0.297,0.311]\",\n",
    "        \"AUROC\": \"0.735 [0.732,0.738]\",\n",
    "        \"F1\": \"0.368 [0.363,0.373]\",\n",
    "        \"Sensitivity\": \"0.707 [0.700,0.714]\",\n",
    "        \"Specificity\": \"0.670 [0.663,0.676]\",\n",
    "    },\n",
    "    \"RNN (concatenated Î”time) + Attention\": {\n",
    "        \"Net variant\": \"RNN (concatenated Î”time) + Attention\",\n",
    "        \"Average Precision\": \"0.312 [0.303,0.320]\",\n",
    "        \"AUROC\": \"0.741 [0.739,0.744]\",\n",
    "        \"F1\": \"0.368 [0.363,0.372]\",\n",
    "        \"Sensitivity\": \"0.687 [0.680,0.695]\",\n",
    "        \"Specificity\": \"0.688 [0.681,0.696]\",\n",
    "    },\n",
    "    \"RNN (concatenated Î”time)\": {\n",
    "        \"Net variant\": \"RNN (concatenated Î”time)\",\n",
    "        \"Average Precision\": \"0.311 [0.303,0.320]\",\n",
    "        \"AUROC\": \"0.739 [0.737,0.742]\",\n",
    "        \"F1\": \"0.364 [0.359,0.369]\",\n",
    "        \"Sensitivity\": \"0.698 [0.692,0.704]\",\n",
    "        \"Specificity\": \"0.688 [0.684,0.693]\",\n",
    "    },\n",
    "    \"ODE + Attention\": {\n",
    "        \"Net variant\": \"ODE + Attention\",\n",
    "        \"Average Precision\": \"0.294 [0.285,0.302]\",\n",
    "        \"AUROC\": \"0.717 [0.714,0.720]\",\n",
    "        \"F1\": \"0.333 [0.328,0.339]\",\n",
    "        \"Sensitivity\": \"0.776 [0.768,0.784]\",\n",
    "        \"Specificity\": \"0.554 [0.548,0.560]\",\n",
    "    },\n",
    "    \"Attention (concatenated time)\": {\n",
    "        \"Net variant\": \"Attention (concatenated time)\",\n",
    "        \"Average Precision\": \"0.286 [0.277,0.295]\",\n",
    "        \"AUROC\": \"0.711 [0.709,0.714]\",\n",
    "        \"F1\": \"0.330 [0.325,0.334]\",\n",
    "        \"Sensitivity\": \"0.700 [0.686,0.714]\",\n",
    "        \"Specificity\": \"0.614 [0.601,0.628]\",\n",
    "    },\n",
    "    \"MCE + RNN + Attention\": {\n",
    "        \"Net variant\": \"MCE + RNN + Attention\",\n",
    "        \"Average Precision\": \" 0.317 [0.308,0.325]\",\n",
    "        \"AUROC\": \"0.736 [0.734,0.739]\",\n",
    "        \"F1\": \"0.373 [0.369,0.378]\",\n",
    "        \"Sensitivity\": \"0.630 [0.622,0.638]\",\n",
    "        \"Specificity\": \"0.744 [0.738,0.749]\",\n",
    "    },\n",
    "    \"MCE + RNN\": {\n",
    "        \"Net variant\": \"MCE + RNN\",\n",
    "        \"Average Precision\": \"0.298 [0.291,0.306]\",\n",
    "        \"AUROC\": \"0.727 [0.724,0.730]\",\n",
    "        \"F1\": \"0.361 [0.357,0.366]\",\n",
    "        \"Sensitivity\": \"0.654 [0.645,0.663]\",\n",
    "        \"Specificity\": \"0.706 [0.697,0.715]\",\n",
    "    },\n",
    "    \"MCE + Attention\": {\n",
    "        \"Net variant\": \"MCE + Attention\",\n",
    "        \"Average Precision\": \" 0.269 [0.261,0.278]\",\n",
    "        \"AUROC\": \"0.689 [0.686,0.692]\",\n",
    "        \"F1\": \"0.312 [0.308,0.316]\",\n",
    "        \"Sensitivity\": \"0.686 [0.676,0.695]\",\n",
    "        \"Specificity\": \"0.616 [0.607,0.625]\",\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"Net variant\": \"Logistic Regression\",\n",
    "        \"Average Precision\": \"0.257 [0.248,0.266]\",\n",
    "        \"AUROC\": \"0.659 [0.656,0.663]\",\n",
    "        \"F1\": \"0.296 [0.291,0.300]\",\n",
    "        \"Sensitivity\": \"0.606 [0.597,0.615]\",\n",
    "        \"Specificity\": \"0.647 [0.639,0.655]\",\n",
    "    },\n",
    "}\n",
    "\n",
    "headers = [\"Net variant\", \"Average Precision\", \"AUROC\", \"F1\", \"Sensitivity\", \"Specificity\"]\n",
    "\n",
    "data = []\n",
    "for result in results.values():\n",
    "    data.append([result[key] for key in headers])\n",
    "\n",
    "print(\"Results of model from the paper\")\n",
    "print(tabulate(data, headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjW9bCkouv8O"
   },
   "outputs": [],
   "source": [
    "# metrics to evaluate my model\n",
    "\n",
    "# plot figures to better show the results\n",
    "\n",
    "# it is better to save the numbers and figures for your presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EAWAy_LwHlV"
   },
   "source": [
    "## (TODO) Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOdhGrbwwG71"
   },
   "outputs": [],
   "source": [
    "# compare you model with others\n",
    "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AFter running the 3 models above, we were able to get average precision from each model. The highest number of precison we were able to get was 0.315 from the model birnn_concat_time_delta.\n",
    "\n",
    "According to the paper, the deep learning architectures were evaluated based on average precision, AUROC, F1-score, sensitivity, and specificity. The ODE + RNN model achieved the highest average precision of 0.331, indicating better predictive accuracy than other models, including those with attention layers or based solely on logistic regression which had an average precision of 0.257."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH75TNU71eRH"
   },
   "source": [
    "# (TODO) Discussion\n",
    "\n",
    " * # Is the paper reproducible? Are there any negatives?\n",
    " Comparing our trained models with the ones from our paper, most metrics were close. Although Sensitiviy and explicity is slightly off, this could possibly be that the version of the python is different betweem the one we are using and the one from the paper. Overall, we can conclude that the paper is reproducible. <br>\n",
    "\n",
    "*  # â€œWhat was easyâ€ and â€œWhat was difficultâ€ during the reproduction.\n",
    " Although the reproduction was generally well structured, it would better if the author could mention details of the code such as which library to use and the version of the python. The instruction of how to run the code was unfriendly as it didn't provide the steps of execution. <br>\n",
    "\n",
    "* # suggestions to the author or other reproducers on how to improve the reproducibility.\n",
    " In terms of reproduction, mimi-iii dataset was very much accessable with wide understoodable ann comprehenisble of the dataset. The given code was resueable. However, we faced some challenge in training such as the training consumed massive of time(around 6~8 hours) in a personal computer.\n",
    "\n",
    " * # What will you do in next phase.\n",
    " Our next task is to run the reamining models and continue comparting the results with the paper. After this, we would prepare our presentation to conclude which model outperforms.\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2VDXo5F4Frm"
   },
   "outputs": [],
   "source": [
    "# no code is required for this section\n",
    "'''\n",
    "if you want to use an image outside this notebook for explanaition,\n",
    "you can read and plot it here like the Scope of Reproducibility\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHMI2chl9omn"
   },
   "source": [
    "# References\n",
    "1.  â€œBenchmarking Deep Learning Architectures for Predicting Readmission to the ICU and Describing Patients-at-Riskâ€, Sebastiano Barbieri, James Kemp, 2020\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmVuzQ724HbO"
   },
   "source": [
    "# Feel free to add new sections"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
